{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 15 - Natural Language Processing: Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.1. Word Embedding (word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In natural language processing, words are the basic unit of the meaning. The *word vectors* are vectors used to represent words, and can also be considered as feature vectors or representations of words.\n",
    "\n",
    "The technique of mapping words to real vectors is called *word embedding*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.1.1. One-Hot Vectors Are a Bad Choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that the number of different words in the dictionary (the dictionary size) is $N$, and each word corresponds to a different integer (index) from 0 to $N-1$. To obtain the one-hot vector representation for any word with index $i$, we create a length-$N$ vector with all 0s and set the element at position $i$ to 1.\n",
    "\n",
    "However, one-hot word vectors cannot accurately express the similarity between different words, such as the *cosine similarity*. For vectors $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^d$, their cosine similarity is the cosine of the angle between them:\n",
    "\\begin{split}\n",
    "\\frac{\\mathbf{x}^\\top \\mathbf{y}}{\\|\\mathbf{x}\\| \\|\\mathbf{y}\\|} \\in [-1, 1].\n",
    "\\end{split}\n",
    "Since the cosine similarity between one-hot vectors of any two different words is 0, one-hot vectors cannot encode similarities among words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.1.2. Self-Supervised word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word2vec algorithm maps each word to a fixed-length vector, and thes vectors can better express the similarity and analogy relationships among different words.\n",
    "\n",
    "The word2vec algorithm contains two models:\n",
    "* *skip-gram*, and\n",
    "* *continuous bag of words (CBOW)*.\n",
    "\n",
    "Since supervision comes from the data without labels, both skip-gram and continuous bag of words are self-supervised learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.1.3. The Skip-Gram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *skip-gram* model assumes that *a word can be used to generate its surrounding words in a text squence*.\n",
    "\n",
    "For example, suppose a text squence, \"the\", \"man\", \"loves\", \"his\", \"son\". If \"loves\" is chosen as the *center word* and we set the context window size to 2. As shown in the figure below, given the center word \"loves\", the skip-gram model considers the conditional probability for generating the *context words*: \"the\", \"man\", \"his\", \"son\", which are no more than 2 words away from the center word:\n",
    "\\begin{split}\n",
    "P(\\textrm{\"the\"},\\textrm{\"man\"},\\textrm{\"his\"},\\textrm{\"son\"}\\mid\\textrm{\"loves\"}).\n",
    "\\end{split}\n",
    "\n",
    "![](../imgs/ch15/skip-gram.svg)\n",
    "\n",
    "Assume that the context words are independently generated given the center words (i.e., conditional independence). Then, the above conditional probability can be rewritten as\n",
    "\\begin{split}\n",
    "P(\\textrm{\"the\"},\\textrm{\"man\"},\\textrm{\"his\"},\\textrm{\"son\"}\\mid\\textrm{\"loves\"}) = P(\\textrm{\"the\"}\\mid\\textrm{\"loves\"})\\cdot P(\\textrm{\"man\"}\\mid\\textrm{\"loves\"})\\cdot P(\\textrm{\"his\"}\\mid\\textrm{\"loves\"})\\cdot P(\\textrm{\"son\"}\\mid\\textrm{\"loves\"}).\n",
    "\\end{split}\n",
    "\n",
    "Each word in this model has two $d$-dimensional vector representations to calculate conditional probabilities.\n",
    "\n",
    "For any word with index $i$ in the dictionary, denote by $\\mathbf{v}_i \\in \\mathbb{R}^d$ and $\\mathbf{u}_i \\in \\mathbb{R}^d$ its two vectors when used as a *center word* and a *context word*, respectively. The conditional probability of generating any context word $w_o$ (with index $o$ in the dictionary) given the center word $w_c$ (with index $c$ in the dictionary) can be modeled by a softmax operation on vector dot products:\n",
    "\\begin{split}\n",
    "P(w_o \\mid w_c) = \\frac{\\exp(\\mathbf{u}_o^\\top \\mathbf{v}_c)}{ \\sum_{i \\in \\mathcal{V}} \\exp(\\mathbf{u}_i^\\top \\mathbf{v}_c)},\n",
    "\\end{split}\n",
    "where the vocabulary index set is $\\mathcal{V} = \\{0, 1, \\ldots, |\\mathcal{V}|-1\\}$.\n",
    "\n",
    "Given a text sequence of length $T$, where the word at time step $t$ is denoted by $w^{(t)}$. Assume that context words are independently generated given any center word. For context window size $m$, the likelihood function of the skip-gram model is the probability of generating all context words given any center word:\n",
    "\\begin{split}\n",
    "\\prod_{t=1}^{T} \\prod_{-m \\leq j \\leq m,\\ j \\neq 0} P(w^{(t+j)} \\mid w^{(t)}),\n",
    "\\end{split}\n",
    "where any time step that is less than 1 or greater than $T$ can be ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15.1.3.1. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model parameters in the skip-gram model are the center word vectors and the context word vectors for each word in the vocabulary.\n",
    "\n",
    "During training, we learn the model parameters by maximizing the likelihood function (i.e., maximum likelihood estimation), which is equivalent to minimizing the following loss function:\n",
    "\\begin{split}\n",
    "- \\sum_{t=1}^{T} \\sum_{-m \\leq j \\leq m,\\ j \\neq 0} \\textrm{log}\\, P(w^{(t+j)} \\mid w^{(t)}).\n",
    "\\end{split}\n",
    "\n",
    "Using SGD to minimize this loss function, in each iteration, we can randomly sample a shorter subsequence to calculate the (stochastic) gradient for this subsequence to update the model parameters.\n",
    "\n",
    "The (stochastic) gradients are the gradients of the log conditional probability with respect to the center word vector and the context word vector. Recall in the previous section that the log conditional probability involving any pair of the center word $w_c$ and the context word $w_o$ is\n",
    "\\begin{split}\n",
    "\\log P(w_o \\mid w_c) =\\mathbf{u}_o^\\top \\mathbf{v}_c - \\log\\left(\\sum_{i \\in \\mathcal{V}} \\exp(\\mathbf{u}_i^\\top \\mathbf{v}_c)\\right).\n",
    "\\end{split}\n",
    "\n",
    "Its gradient with respect to the center word vector $\\mathbf{v}_c$ is\n",
    "\\begin{split}\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial \\textrm{log}\\, P(w_o \\mid w_c)}{\\partial \\mathbf{v}_c}&= \\mathbf{u}_o - \\frac{\\sum_{j \\in \\mathcal{V}} \\exp(\\mathbf{u}_j^\\top \\mathbf{v}_c)\\mathbf{u}_j}{\\sum_{i \\in \\mathcal{V}} \\exp(\\mathbf{u}_i^\\top \\mathbf{v}_c)}\\\\&= \\mathbf{u}_o - \\sum_{j \\in \\mathcal{V}} \\left(\\frac{\\exp(\\mathbf{u}_j^\\top \\mathbf{v}_c)}{ \\sum_{i \\in \\mathcal{V}} \\exp(\\mathbf{u}_i^\\top \\mathbf{v}_c)}\\right) \\mathbf{u}_j\\\\&= \\mathbf{u}_o - \\sum_{j \\in \\mathcal{V}} P(w_j \\mid w_c) \\mathbf{u}_j.\n",
    "\\end{aligned}\n",
    "\\end{split}\n",
    "which requries the conditional probabilities of all words in the dictionary with $w_c$ as the center word.\n",
    "\n",
    "After training, for any word with index $i$ in the dictionary, we obtain both word vectors $\\mathbf{v}_i$ (as the center word) and $\\mathbf{u}_i$ (as the context word).\n",
    "\n",
    "In NLP, the center word vectors of the skip-gram model are typically used as the word representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.1.4. The Continuous Bag of Words (CBOW) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *continuous bag of words (CBOW)* model is similar to the skip-gram model. The main difference is that the CBOW model assumes that *a center word is generated based on its surrounding context words in the text sequence*.\n",
    "\n",
    "Using the same text sequence, \"the\", \"man\", \"loves\", \"his\", \"son\", with \"loves\" as the center word and the context window size being 2, the continuous bag of words model consideres the conditional probability of generating the center word \"loves\" based on the context words \"the\", \"man\", \"his\", \"son\":\n",
    "\\begin{split}\n",
    "P(\\textrm{\"loves\"}\\mid\\textrm{\"the\"},\\textrm{\"man\"},\\textrm{\"his\"},\\textrm{\"son\"}).\n",
    "\\end{split}\n",
    "as shown in the figure below.\n",
    "\n",
    "![](../imgs/ch15/cbow.svg)\n",
    "\n",
    "Since there are multiple context words in the CBOW model, these context word vectors are averaged in the calculation of the conditional probability.\n",
    "\n",
    "For any word with index $i$ in the dictionary, denote by $\\mathbf{v}_i \\in \\mathbb{R}^d$ and $\\mathbf{u}_i \\in \\mathbb{R}^d$ its two vectors when used as a *context word* and a *center word*, respectively, (the reverse of the skip-gram model). The conditional probability of generating any center word $w_c$ (with index $c$ in the dictionary) given the context words $w_{o_1}, \\ldots, w_{o_{2m}}$ (with indices $o_1, \\ldots, o_{2m}$ in the dictionary) can be modeled by\n",
    "\\begin{split}\n",
    "P(w_c \\mid w_{o_1}, \\ldots, w_{o_{2m}}) = \\frac{\\exp\\left(\\frac{1}{2m}\\mathbf{u}_c^\\top (\\mathbf{v}_{o_1} + \\ldots + \\mathbf{v}_{o_{2m}}) \\right)}{ \\sum_{i \\in \\mathcal{V}} \\exp\\left(\\frac{1}{2m}\\mathbf{u}_i^\\top (\\mathbf{v}_{o_1} + \\ldots + \\mathbf{v}_{o_{2m}}) \\right)}.\n",
    "\\end{split}\n",
    "\n",
    "For brevity, let $\\mathcal{W}_o = \\{w_{o_1}, \\ldots, w_{o_{2m}}\\}$ denote the set of context words and $\\bar{\\mathbf{v}}_o = \\left(\\mathbf{v}_{o_1} + \\ldots + \\mathbf{v}_{o_{2m}}\\right)/(2m)$ denote their average. Then, the above conditional probability can be rewritten as\n",
    "\\begin{split}\n",
    "P(w_c \\mid \\mathcal{W}_o) = \\frac{\\exp\\left(\\mathbf{u}_c^\\top \\bar{\\mathbf{v}}_o\\right)}{\\sum_{i \\in \\mathcal{V}} \\exp\\left(\\mathbf{u}_i^\\top \\bar{\\mathbf{v}}_o\\right)}.\n",
    "\\end{split}\n",
    "\n",
    "Given a text sequence of length $T$, where the word at time step $t$ is denoted by $w^{(t)}$. For context window size $m$, the likelihood function of the CBOW model is the probability of generating all center words given any context words:\n",
    "\\begin{split}\n",
    "\\prod_{t=1}^{T}  P(w^{(t)} \\mid  w^{(t-m)}, \\ldots, w^{(t-1)}, w^{(t+1)}, \\ldots, w^{(t+m)}).\n",
    "\\end{split}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15.1.4.1. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the CBOW model is similar to training the skip-gram model.\n",
    "\n",
    "The maximum likelihood estimation of the CBOW model is equivalent to minimizing the following loss function:\n",
    "\\begin{split}\n",
    "-\\sum_{t=1}^T  \\textrm{log}\\, P(w^{(t)} \\mid  w^{(t-m)}, \\ldots, w^{(t-1)}, w^{(t+1)}, \\ldots, w^{(t+m)}).\n",
    "\\end{split}\n",
    "\n",
    "Note that\n",
    "\\begin{split}\n",
    "\\log\\,P(w_c \\mid \\mathcal{W}_o) = \\mathbf{u}_c^\\top \\bar{\\mathbf{v}}_o - \\log\\,\\left(\\sum_{i \\in \\mathcal{V}} \\exp\\left(\\mathbf{u}_i^\\top \\bar{\\mathbf{v}}_o\\right)\\right).\n",
    "\\end{split}\n",
    "\n",
    "Hence, its gradient with respect to the context word vector $\\mathbf{v}_{o_i}$ ($i = 1, \\ldots, 2m$) is\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial \\log\\, P(w_c \\mid \\mathcal{W}_o)}{\\partial \\mathbf{v}_{o_i}} &= \\frac{1}{2m} \\left(\\mathbf{u}_c - \\sum_{j \\in \\mathcal{V}} \\frac{\\exp(\\mathbf{u}_j^\\top \\bar{\\mathbf{v}}_o)\\mathbf{u}_j}{ \\sum_{i \\in \\mathcal{V}} \\exp(\\mathbf{u}_i^\\top \\bar{\\mathbf{v}}_o)} \\right) \\\\\n",
    "&= \\frac{1}{2m}\\left(\\mathbf{u}_c - \\sum_{j \\in \\mathcal{V}} P(w_j \\mid \\mathcal{W}_o) \\mathbf{u}_j \\right).\n",
    "\\end{aligned}\n",
    "\n",
    "Unlike the skip-gram model, the continuous bag of words model typically uses context word vectors as the word representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.2. Approximate Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient calculation of the skip-gram and CBOW models involves the sum of all conditional probabilities of words in the dictionary. When the dictionary is large, the sum of all conditional probabilities is computationally expensive.\n",
    "\n",
    "To reduce the aforementioned computational cost, we can use *negative sampling* and *hierarchical softmax* to approximate the maximum likelihood estimation of the skip-gram and CBOW models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.2.1. Negative Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Negative sampling* modifies the original objective function.\n",
    "\n",
    "Given the context window of a center word $w_c$, the fact that any (context) word $w_o$ comes from this context window is considered as an event with the probability modeled by\n",
    "\\begin{split}\n",
    "P(D=1\\mid w_c, w_o) = \\sigma(\\mathbf{u}_o^\\top \\mathbf{v}_c),\n",
    "\\end{split}\n",
    "where $\\sigma$ is the sigmoid function:\n",
    "\\begin{split}\n",
    "\\sigma(x) = \\frac{1}{1+\\exp(-x)}.\n",
    "\\end{split}\n",
    "\n",
    "Given a text sequence of lenght $T$, denote by $w^{(t)}$ the word at time step $t$ and let the context window size be $m$, the joint probability of generating all context words in the text sequence is\n",
    "\\begin{split}\n",
    "\\prod_{t=1}^{T} \\prod_{-m \\leq j \\leq m,\\ j \\neq 0} P(D=1\\mid w^{(t)}, w^{(t+j)}).\n",
    "\\end{split}\n",
    "\n",
    "This joint probability is maximized to 1 only if all the word vectors are equal to infinity. To make the objective function more meaningful, *negative sampling* adds negative examples sampled from a predefined distribution.\n",
    "\n",
    "Denote by $S$ the event that a context word $w_o$ comes from the context window of a center word $w_c$. \n",
    "\n",
    "For this event involving $w_o$, from a predefined distribution $P(w)$, we can sample $K$ *noise words* that are not from this context window. Denote by $N_k$ the event that a noise word $w_k$ ($k = 1, \\ldots, K$) does not come from the context window of $w_c$.\n",
    "\n",
    "Assume that these events involving both the positive example $S$ and the negative examples $N_1, \\ldots, N_K$ are mutually independent. Then, the negative sampling rewrites the joint probability (involving only positive examples $S$) as\n",
    "\\begin{split}\n",
    "\\prod_{t=1}^{T} \\prod_{-m \\leq j \\leq m,\\ j \\neq 0} P(D=1\\mid w^{(t)}, w^{(t+j)}) = \\prod_{t=1}^{T} \\prod_{-m \\leq j \\leq m,\\ j \\neq 0} P(w^{(t+j)} \\mid w^{(t)}),\n",
    "\\end{split}\n",
    "where the conditional probability is approximated through events $S, N_1, \\ldots, N_K$:\n",
    "\\begin{split}\n",
    "P(w^{(t+j)} \\mid w^{(t)}) =P(D=1\\mid w^{(t)}, w^{(t+j)})\\prod_{k=1,\\ w_k \\sim P(w)}^K P(D=0\\mid w^{(t)}, w_k).\n",
    "\\end{split}\n",
    "\n",
    "Denote by $i_t$ and $h_k$ the indices of a word $w^{(t)}$ at time step $t$ of a text sequence and a noise word $w_k$, respectively. The logarithmic loss with respect to the conditional probabilities of the positive example and the negative examples is\n",
    "\\begin{split}\\begin{aligned}\n",
    "-\\log P(w^{(t+j)} \\mid w^{(t)})\n",
    "=& -\\log P(D=1\\mid w^{(t)}, w^{(t+j)}) - \\sum_{k=1,\\ w_k \\sim P(w)}^K \\log P(D=0\\mid w^{(t)}, w_k)\\\\\n",
    "=&-  \\log\\, \\sigma\\left(\\mathbf{u}_{i_{t+j}}^\\top \\mathbf{v}_{i_t}\\right) - \\sum_{k=1,\\ w_k \\sim P(w)}^K \\log\\left(1-\\sigma\\left(\\mathbf{u}_{h_k}^\\top \\mathbf{v}_{i_t}\\right)\\right)\\\\\n",
    "=&-  \\log\\, \\sigma\\left(\\mathbf{u}_{i_{t+j}}^\\top \\mathbf{v}_{i_t}\\right) - \\sum_{k=1,\\ w_k \\sim P(w)}^K \\log\\sigma\\left(-\\mathbf{u}_{h_k}^\\top \\mathbf{v}_{i_t}\\right).\n",
    "\\end{aligned}\\end{split}\n",
    "\n",
    "The computational cost for gradients at each training step does not depend on the dictionary size, but linearly depends on $K$. When setting the hyperparameter $K$ to a smaller value, the computational cost for gradients at each training with negative sampling is smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.2.2. Hierarchical Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *hierarchical softmax* uses a binary where each leaf node of the tree represents a word in the dictionary $\\mathcal{V}$, as shown in the figure below.\n",
    "\n",
    "![](../imgs/ch15/hi-softmax.svg)\n",
    "\n",
    "Denote by $L(w)$ the number of nodes (including both ends) on the path from the root node to the leaf node representing word $w$ in the binary tree.\n",
    "\n",
    "Let $n(w,j)$ be the $j$-th node on this path, with its context word vector being $\\mathbf{u}_{n(w,j)}$. For instance, in the figure above, $L(w_3) = 4$ since the path from the root node to the leaf node representing $w_3$ is $n(w_3, 1)$, $n(w_3, 2)$, $n(w_3, 3)$ and $w_3$.\n",
    "\n",
    "The *hierarchical softmax* approximates the conditional probability \n",
    "\\begin{split}\n",
    "P(w_o \\mid w_c) = \\frac{\\exp(\\mathbf{u}_o^\\top \\mathbf{v}_c)}{ \\sum_{i \\in \\mathcal{V}} \\exp(\\mathbf{u}_i^\\top \\mathbf{v}_c)},\n",
    "\\end{split}\n",
    "as\n",
    "\\begin{split}\n",
    "P(w_o \\mid w_c) = \\prod_{j=1}^{L(w_o)-1} \\sigma\\left( [\\![  n(w_o, j+1) = \\textrm{leftChild}(n(w_o, j)) ]\\!] \\cdot \\mathbf{u}_{n(w_o, j)}^\\top \\mathbf{v}_c\\right),\n",
    "\\end{split}\n",
    "where $\\sigma$ is the sigmoid function and $\\textrm{leftChild}(n)$ is the left child node of $n$: if $x$ is true, then $[\\![x]\\!] = 1$; otherwise, $[\\![x]\\!] = -1$.\n",
    "\n",
    "In the figure above, if we want to calculate the conditional probaility of generating word $w_3$ given word $w_c$, we need to calculate the dot products between the word vector $\\mathbf{v}_c$ of $w_c$ and non-leaf node vectors on the path (the bold line segments in the figure above) from the root to $w_3$, which is traversed left, right, and then left:\n",
    "\\begin{split}\n",
    "P(w_3 \\mid w_c) = \\sigma(\\mathbf{u}_{n(w_3, 1)}^\\top \\mathbf{v}_c) \\cdot \\sigma(-\\mathbf{u}_{n(w_3, 2)}^\\top \\mathbf{v}_c) \\cdot \\sigma(\\mathbf{u}_{n(w_3, 3)}^\\top \\mathbf{v}_c).\n",
    "\\end{split}\n",
    "\n",
    "Since $\\sigma(x) + \\sigma(-x) = 1$, it holds that the conditional probabilities of generating all the words in dictionary $\\mathcal{V}$ based on any word $w_c$ sum up to one:\n",
    "\\begin{split}\n",
    "\\sum_{w \\in \\mathcal{V}} P(w \\mid w_c) = 1.\n",
    "\\end{split}\n",
    "\n",
    "Since $L(w_o) - 1$ is the order of $\\mathcal{O}(\\log_2 |\\mathcal{V}|)$ due to the binary tree structure, when the dictionary size $|\\mathcal{V}|$ is large, the computational cost for each training step of the hierarchical softmax is significantly reduced compared with that without approximate training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.3. The Dataset for Pretraining Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import torch\n",
    "\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.3.1. Reading the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Penn Tree Bank (PTB) is a widely used dataset for language modeling. This corpus is sampled from Wall Street Journal articles. In the original format, each line of the text file represents a sentence of words that are separated by spaces. We will treat each word as a token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2l.DATA_HUB['ptb'] = (d2l.DATA_URL + 'ptb.zip',\n",
    "                       '319d85e578af0cdc590547f26231e4e31cdf1e42')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ptb():\n",
    "    '''Load the PTB dataset into a list of text lines'''\n",
    "    data_dir = d2l.download_extract('ptb')\n",
    "    # Read the training set\n",
    "    with open(os.path.join(data_dir, 'ptb.train.txt')) as f:\n",
    "        raw_text = f.read()\n",
    "\n",
    "    return [line.split() for line in raw_text.split('\\n')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# sentences: 42069\n"
     ]
    }
   ],
   "source": [
    "sentences = read_ptb()\n",
    "print(f'# sentences: {len(sentences)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to build a vocabulary for the corpus after reading the dataset. Any word that appears less than 10 times is replaced by the `<unk>` token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size: 6719\n"
     ]
    }
   ],
   "source": [
    "vocab = d2l.Vocab(sentences, min_freq=10)\n",
    "print(f'vocabulary size: {len(vocab)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.3.2. Subsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text data typically have high-frequency words such as “the”, “a”, and “in”: they may even occur billions of times in very large corpora. However, these words often co-occur with many different words in context windows, providing little useful signals.\n",
    "\n",
    "When training word embedding models, high-frequency words can be *subsampled*. Each indexed word $w_i$ in the dataset will be discarded with probability\n",
    "\\begin{split}\n",
    "P(w_i) = \\max\\left(1 - \\sqrt{\\frac{t}{f(w_i)}}, 0\\right),\n",
    "\\end{split}\n",
    "where $f(w_i)$ is the ratio of the number of words $w_i$ to the total number of words in the dataset, and the constant $t$ is a hyperparameter.\n",
    "\n",
    "When the relative frequency $f(w_i)>t$, the word $w_i$ can be discarded. The higher the relative frequency of the word, the greater the probability of being discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsample(sentences, vocab):\n",
    "    '''Subsample high-frequency words'''\n",
    "    # exclude unkown tokens '<unk>'\n",
    "    sentences = [\n",
    "        [token for token in line if vocab[token] != vocab.unk]\n",
    "        for line in sentences\n",
    "    ]\n",
    "    counter = collections.Counter(\n",
    "        [token for line in sentences for token in line]\n",
    "    )\n",
    "\n",
    "    num_tokens = sum(counter.values())\n",
    "\n",
    "    # return True if `token` is kept during subsampling\n",
    "    def keep(token):\n",
    "        t = 1e-4 # threshold for subsampling\n",
    "        f_w = counter[token] / num_tokens\n",
    "        prob = math.sqrt(t / f_w)\n",
    "        return random.uniform(0, 1) < prob\n",
    "    \n",
    "    return (\n",
    "        [[token for token in line if keep(token)] for line in sentences],\n",
    "        counter\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsampled, counter = subsample(sentences, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the histogram of the number of tokens per sentence before and after subsampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"262.190625pt\" height=\"185.484929pt\" viewBox=\"0 0 262.190625 185.484929\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2024-01-03T11:55:35.174131</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.7.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 185.484929 \n",
       "L 262.190625 185.484929 \n",
       "L 262.190625 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 59.690625 147.928679 \n",
       "L 254.990625 147.928679 \n",
       "L 254.990625 9.328679 \n",
       "L 59.690625 9.328679 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 68.567898 147.928679 \n",
       "L 75.814651 147.928679 \n",
       "L 75.814651 126.211667 \n",
       "L 68.567898 126.211667 \n",
       "z\n",
       "\" clip-path=\"url(#p8cecdc083d)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 86.684781 147.928679 \n",
       "L 93.931534 147.928679 \n",
       "L 93.931534 87.718057 \n",
       "L 86.684781 87.718057 \n",
       "z\n",
       "\" clip-path=\"url(#p8cecdc083d)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 104.801664 147.928679 \n",
       "L 112.048417 147.928679 \n",
       "L 112.048417 78.368513 \n",
       "L 104.801664 78.368513 \n",
       "z\n",
       "\" clip-path=\"url(#p8cecdc083d)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 122.918547 147.928679 \n",
       "L 130.1653 147.928679 \n",
       "L 130.1653 98.688845 \n",
       "L 122.918547 98.688845 \n",
       "z\n",
       "\" clip-path=\"url(#p8cecdc083d)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_7\">\n",
       "    <path d=\"M 141.03543 147.928679 \n",
       "L 148.282183 147.928679 \n",
       "L 148.282183 127.400215 \n",
       "L 141.03543 127.400215 \n",
       "z\n",
       "\" clip-path=\"url(#p8cecdc083d)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_8\">\n",
       "    <path d=\"M 159.152313 147.928679 \n",
       "L 166.399067 147.928679 \n",
       "L 166.399067 140.956231 \n",
       "L 159.152313 140.956231 \n",
       "z\n",
       "\" clip-path=\"url(#p8cecdc083d)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_9\">\n",
       "    <path d=\"M 177.269196 147.928679 \n",
       "L 184.51595 147.928679 \n",
       "L 184.51595 146.351252 \n",
       "L 177.269196 146.351252 \n",
       "z\n",
       "\" clip-path=\"url(#p8cecdc083d)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_10\">\n",
       "    <path d=\"M 195.38608 147.928679 \n",
       "L 202.632833 147.928679 \n",
       "L 202.632833 147.528845 \n",
       "L 195.38608 147.528845 \n",
       "z\n",
       "\" clip-path=\"url(#p8cecdc083d)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_11\">\n",
       "    <path d=\"M 213.502963 147.928679 \n",
       "L 220.749716 147.928679 \n",
       "L 220.749716 147.79175 \n",
       "L 213.502963 147.79175 \n",
       "z\n",
       "\" clip-path=\"url(#p8cecdc083d)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_12\">\n",
       "    <path d=\"M 231.619846 147.928679 \n",
       "L 238.866599 147.928679 \n",
       "L 238.866599 147.851999 \n",
       "L 231.619846 147.851999 \n",
       "z\n",
       "\" clip-path=\"url(#p8cecdc083d)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_13\">\n",
       "    <path d=\"M 75.814651 147.928679 \n",
       "L 83.061404 147.928679 \n",
       "L 83.061404 15.928679 \n",
       "L 75.814651 15.928679 \n",
       "z\n",
       "\" clip-path=\"url(#p8cecdc083d)\" style=\"fill: url(#h379c09ba72)\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_14\">\n",
       "    <path d=\"M 93.931534 147.928679 \n",
       "L 101.178287 147.928679 \n",
       "L 101.178287 59.828264 \n",
       "L 93.931534 59.828264 \n",
       "z\n",
       "\" clip-path=\"url(#p8cecdc083d)\" style=\"fill: url(#h379c09ba72)\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_15\">\n",
       "    <path d=\"M 112.048417 147.928679 \n",
       "L 119.29517 147.928679 \n",
       "L 119.29517 138.217642 \n",
       "L 112.048417 138.217642 \n",
       "z\n",
       "\" clip-path=\"url(#p8cecdc083d)\" style=\"fill: url(#h379c09ba72)\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_16\">\n",
       "    <path d=\"M 130.1653 147.928679 \n",
       "L 137.412054 147.928679 \n",
       "L 137.412054 147.359053 \n",
       "L 130.1653 147.359053 \n",
       "z\n",
       "\" clip-path=\"url(#p8cecdc083d)\" style=\"fill: url(#h379c09ba72)\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_17\">\n",
       "    <path d=\"M 148.282183 147.928679 \n",
       "L 155.528937 147.928679 \n",
       "L 155.528937 147.890339 \n",
       "L 148.282183 147.890339 \n",
       "z\n",
       "\" clip-path=\"url(#p8cecdc083d)\" style=\"fill: url(#h379c09ba72)\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_18\">\n",
       "    <path d=\"M 166.399067 147.928679 \n",
       "L 173.64582 147.928679 \n",
       "L 173.64582 147.928679 \n",
       "L 166.399067 147.928679 \n",
       "z\n",
       "\" clip-path=\"url(#p8cecdc083d)\" style=\"fill: url(#h379c09ba72)\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_19\">\n",
       "    <path d=\"M 184.51595 147.928679 \n",
       "L 191.762703 147.928679 \n",
       "L 191.762703 147.928679 \n",
       "L 184.51595 147.928679 \n",
       "z\n",
       "\" clip-path=\"url(#p8cecdc083d)\" style=\"fill: url(#h379c09ba72)\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_20\">\n",
       "    <path d=\"M 202.632833 147.928679 \n",
       "L 209.879586 147.928679 \n",
       "L 209.879586 147.928679 \n",
       "L 202.632833 147.928679 \n",
       "z\n",
       "\" clip-path=\"url(#p8cecdc083d)\" style=\"fill: url(#h379c09ba72)\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_21\">\n",
       "    <path d=\"M 220.749716 147.928679 \n",
       "L 227.996469 147.928679 \n",
       "L 227.996469 147.928679 \n",
       "L 220.749716 147.928679 \n",
       "z\n",
       "\" clip-path=\"url(#p8cecdc083d)\" style=\"fill: url(#h379c09ba72)\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_22\">\n",
       "    <path d=\"M 238.866599 147.928679 \n",
       "L 246.113352 147.928679 \n",
       "L 246.113352 147.928679 \n",
       "L 238.866599 147.928679 \n",
       "z\n",
       "\" clip-path=\"url(#p8cecdc083d)\" style=\"fill: url(#h379c09ba72)\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"m7b53ddae45\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m7b53ddae45\" x=\"66.756209\" y=\"147.928679\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(63.574959 162.527117) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m7b53ddae45\" x=\"110.943729\" y=\"147.928679\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 20 -->\n",
       "      <g transform=\"translate(104.581229 162.527117) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m7b53ddae45\" x=\"155.131249\" y=\"147.928679\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 40 -->\n",
       "      <g transform=\"translate(148.768749 162.527117) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m7b53ddae45\" x=\"199.318769\" y=\"147.928679\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 60 -->\n",
       "      <g transform=\"translate(192.956269 162.527117) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \n",
       "Q 1688 2584 1439 2293 \n",
       "Q 1191 2003 1191 1497 \n",
       "Q 1191 994 1439 701 \n",
       "Q 1688 409 2113 409 \n",
       "Q 2538 409 2786 701 \n",
       "Q 3034 994 3034 1497 \n",
       "Q 3034 2003 2786 2293 \n",
       "Q 2538 2584 2113 2584 \n",
       "z\n",
       "M 3366 4563 \n",
       "L 3366 3988 \n",
       "Q 3128 4100 2886 4159 \n",
       "Q 2644 4219 2406 4219 \n",
       "Q 1781 4219 1451 3797 \n",
       "Q 1122 3375 1075 2522 \n",
       "Q 1259 2794 1537 2939 \n",
       "Q 1816 3084 2150 3084 \n",
       "Q 2853 3084 3261 2657 \n",
       "Q 3669 2231 3669 1497 \n",
       "Q 3669 778 3244 343 \n",
       "Q 2819 -91 2113 -91 \n",
       "Q 1303 -91 875 529 \n",
       "Q 447 1150 447 2328 \n",
       "Q 447 3434 972 4092 \n",
       "Q 1497 4750 2381 4750 \n",
       "Q 2619 4750 2861 4703 \n",
       "Q 3103 4656 3366 4563 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-36\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m7b53ddae45\" x=\"243.506289\" y=\"147.928679\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 80 -->\n",
       "      <g transform=\"translate(237.143789 162.527117) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \n",
       "Q 1584 2216 1326 1975 \n",
       "Q 1069 1734 1069 1313 \n",
       "Q 1069 891 1326 650 \n",
       "Q 1584 409 2034 409 \n",
       "Q 2484 409 2743 651 \n",
       "Q 3003 894 3003 1313 \n",
       "Q 3003 1734 2745 1975 \n",
       "Q 2488 2216 2034 2216 \n",
       "z\n",
       "M 1403 2484 \n",
       "Q 997 2584 770 2862 \n",
       "Q 544 3141 544 3541 \n",
       "Q 544 4100 942 4425 \n",
       "Q 1341 4750 2034 4750 \n",
       "Q 2731 4750 3128 4425 \n",
       "Q 3525 4100 3525 3541 \n",
       "Q 3525 3141 3298 2862 \n",
       "Q 3072 2584 2669 2484 \n",
       "Q 3125 2378 3379 2068 \n",
       "Q 3634 1759 3634 1313 \n",
       "Q 3634 634 3220 271 \n",
       "Q 2806 -91 2034 -91 \n",
       "Q 1263 -91 848 271 \n",
       "Q 434 634 434 1313 \n",
       "Q 434 1759 690 2068 \n",
       "Q 947 2378 1403 2484 \n",
       "z\n",
       "M 1172 3481 \n",
       "Q 1172 3119 1398 2916 \n",
       "Q 1625 2713 2034 2713 \n",
       "Q 2441 2713 2670 2916 \n",
       "Q 2900 3119 2900 3481 \n",
       "Q 2900 3844 2670 4047 \n",
       "Q 2441 4250 2034 4250 \n",
       "Q 1625 4250 1398 4047 \n",
       "Q 1172 3844 1172 3481 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-38\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_6\">\n",
       "     <!-- # tokens per sentence -->\n",
       "     <g transform=\"translate(100.6125 176.205242) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-23\" d=\"M 3272 2816 \n",
       "L 2363 2816 \n",
       "L 2100 1772 \n",
       "L 3016 1772 \n",
       "L 3272 2816 \n",
       "z\n",
       "M 2803 4594 \n",
       "L 2478 3297 \n",
       "L 3391 3297 \n",
       "L 3719 4594 \n",
       "L 4219 4594 \n",
       "L 3897 3297 \n",
       "L 4872 3297 \n",
       "L 4872 2816 \n",
       "L 3775 2816 \n",
       "L 3519 1772 \n",
       "L 4513 1772 \n",
       "L 4513 1294 \n",
       "L 3397 1294 \n",
       "L 3072 0 \n",
       "L 2572 0 \n",
       "L 2894 1294 \n",
       "L 1978 1294 \n",
       "L 1656 0 \n",
       "L 1153 0 \n",
       "L 1478 1294 \n",
       "L 494 1294 \n",
       "L 494 1772 \n",
       "L 1594 1772 \n",
       "L 1856 2816 \n",
       "L 850 2816 \n",
       "L 850 3297 \n",
       "L 1978 3297 \n",
       "L 2297 4594 \n",
       "L 2803 4594 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \n",
       "L 1172 3500 \n",
       "L 2356 3500 \n",
       "L 2356 3053 \n",
       "L 1172 3053 \n",
       "L 1172 1153 \n",
       "Q 1172 725 1289 603 \n",
       "Q 1406 481 1766 481 \n",
       "L 2356 481 \n",
       "L 2356 0 \n",
       "L 1766 0 \n",
       "Q 1100 0 847 248 \n",
       "Q 594 497 594 1153 \n",
       "L 594 3053 \n",
       "L 172 3053 \n",
       "L 172 3500 \n",
       "L 594 3500 \n",
       "L 594 4494 \n",
       "L 1172 4494 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \n",
       "Q 1497 3097 1228 2736 \n",
       "Q 959 2375 959 1747 \n",
       "Q 959 1119 1226 758 \n",
       "Q 1494 397 1959 397 \n",
       "Q 2419 397 2687 759 \n",
       "Q 2956 1122 2956 1747 \n",
       "Q 2956 2369 2687 2733 \n",
       "Q 2419 3097 1959 3097 \n",
       "z\n",
       "M 1959 3584 \n",
       "Q 2709 3584 3137 3096 \n",
       "Q 3566 2609 3566 1747 \n",
       "Q 3566 888 3137 398 \n",
       "Q 2709 -91 1959 -91 \n",
       "Q 1206 -91 779 398 \n",
       "Q 353 888 353 1747 \n",
       "Q 353 2609 779 3096 \n",
       "Q 1206 3584 1959 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6b\" d=\"M 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 1991 \n",
       "L 2875 3500 \n",
       "L 3609 3500 \n",
       "L 1753 1863 \n",
       "L 3688 0 \n",
       "L 2938 0 \n",
       "L 1159 1709 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \n",
       "L 2834 2853 \n",
       "Q 2591 2978 2328 3040 \n",
       "Q 2066 3103 1784 3103 \n",
       "Q 1356 3103 1142 2972 \n",
       "Q 928 2841 928 2578 \n",
       "Q 928 2378 1081 2264 \n",
       "Q 1234 2150 1697 2047 \n",
       "L 1894 2003 \n",
       "Q 2506 1872 2764 1633 \n",
       "Q 3022 1394 3022 966 \n",
       "Q 3022 478 2636 193 \n",
       "Q 2250 -91 1575 -91 \n",
       "Q 1294 -91 989 -36 \n",
       "Q 684 19 347 128 \n",
       "L 347 722 \n",
       "Q 666 556 975 473 \n",
       "Q 1284 391 1588 391 \n",
       "Q 1994 391 2212 530 \n",
       "Q 2431 669 2431 922 \n",
       "Q 2431 1156 2273 1281 \n",
       "Q 2116 1406 1581 1522 \n",
       "L 1381 1569 \n",
       "Q 847 1681 609 1914 \n",
       "Q 372 2147 372 2553 \n",
       "Q 372 3047 722 3315 \n",
       "Q 1072 3584 1716 3584 \n",
       "Q 2034 3584 2315 3537 \n",
       "Q 2597 3491 2834 3397 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \n",
       "L 1159 -1331 \n",
       "L 581 -1331 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "z\n",
       "M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \n",
       "Q 2534 3019 2420 3045 \n",
       "Q 2306 3072 2169 3072 \n",
       "Q 1681 3072 1420 2755 \n",
       "Q 1159 2438 1159 1844 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1341 3275 1631 3429 \n",
       "Q 1922 3584 2338 3584 \n",
       "Q 2397 3584 2469 3576 \n",
       "Q 2541 3569 2628 3553 \n",
       "L 2631 2963 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \n",
       "L 3122 2828 \n",
       "Q 2878 2963 2633 3030 \n",
       "Q 2388 3097 2138 3097 \n",
       "Q 1578 3097 1268 2742 \n",
       "Q 959 2388 959 1747 \n",
       "Q 959 1106 1268 751 \n",
       "Q 1578 397 2138 397 \n",
       "Q 2388 397 2633 464 \n",
       "Q 2878 531 3122 666 \n",
       "L 3122 134 \n",
       "Q 2881 22 2623 -34 \n",
       "Q 2366 -91 2075 -91 \n",
       "Q 1284 -91 818 406 \n",
       "Q 353 903 353 1747 \n",
       "Q 353 2603 823 3093 \n",
       "Q 1294 3584 2113 3584 \n",
       "Q 2378 3584 2631 3529 \n",
       "Q 2884 3475 3122 3366 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-23\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"83.789062\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-74\" x=\"115.576172\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"154.785156\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6b\" x=\"215.966797\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"270.251953\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"331.775391\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"395.154297\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"447.253906\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"479.041016\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"542.517578\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"604.041016\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"645.154297\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"676.941406\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"729.041016\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"790.564453\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-74\" x=\"853.943359\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"893.152344\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"954.675781\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"1018.054688\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"1073.035156\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <defs>\n",
       "       <path id=\"m1dd03d628a\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m1dd03d628a\" x=\"59.690625\" y=\"147.928679\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(46.328125 151.727898) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m1dd03d628a\" x=\"59.690625\" y=\"120.542787\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 5000 -->\n",
       "      <g transform=\"translate(27.240625 124.342006) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-35\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m1dd03d628a\" x=\"59.690625\" y=\"93.156895\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 10000 -->\n",
       "      <g transform=\"translate(20.878125 96.956114) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"254.492188\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m1dd03d628a\" x=\"59.690625\" y=\"65.771003\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 15000 -->\n",
       "      <g transform=\"translate(20.878125 69.570222) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"254.492188\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m1dd03d628a\" x=\"59.690625\" y=\"38.385111\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 20000 -->\n",
       "      <g transform=\"translate(20.878125 42.18433) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"254.492188\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m1dd03d628a\" x=\"59.690625\" y=\"10.999219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 25000 -->\n",
       "      <g transform=\"translate(20.878125 14.798437) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"254.492188\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_13\">\n",
       "     <!-- count -->\n",
       "     <g transform=\"translate(14.798438 92.734929) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-75\" d=\"M 544 1381 \n",
       "L 544 3500 \n",
       "L 1119 3500 \n",
       "L 1119 1403 \n",
       "Q 1119 906 1312 657 \n",
       "Q 1506 409 1894 409 \n",
       "Q 2359 409 2629 706 \n",
       "Q 2900 1003 2900 1516 \n",
       "L 2900 3500 \n",
       "L 3475 3500 \n",
       "L 3475 0 \n",
       "L 2900 0 \n",
       "L 2900 538 \n",
       "Q 2691 219 2414 64 \n",
       "Q 2138 -91 1772 -91 \n",
       "Q 1169 -91 856 284 \n",
       "Q 544 659 544 1381 \n",
       "z\n",
       "M 1991 3584 \n",
       "L 1991 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-63\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"54.980469\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-75\" x=\"116.162109\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"179.541016\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-74\" x=\"242.919922\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"patch_23\">\n",
       "    <path d=\"M 59.690625 147.928679 \n",
       "L 59.690625 9.328679 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_24\">\n",
       "    <path d=\"M 254.990625 147.928679 \n",
       "L 254.990625 9.328679 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_25\">\n",
       "    <path d=\"M 59.690625 147.928679 \n",
       "L 254.990625 147.928679 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_26\">\n",
       "    <path d=\"M 59.690625 9.328679 \n",
       "L 254.990625 9.328679 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"legend_1\">\n",
       "    <g id=\"patch_27\">\n",
       "     <path d=\"M 155.389063 46.684929 \n",
       "L 247.990625 46.684929 \n",
       "Q 249.990625 46.684929 249.990625 44.684929 \n",
       "L 249.990625 16.328679 \n",
       "Q 249.990625 14.328679 247.990625 14.328679 \n",
       "L 155.389063 14.328679 \n",
       "Q 153.389063 14.328679 153.389063 16.328679 \n",
       "L 153.389063 44.684929 \n",
       "Q 153.389063 46.684929 155.389063 46.684929 \n",
       "z\n",
       "\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n",
       "    </g>\n",
       "    <g id=\"patch_28\">\n",
       "     <path d=\"M 157.389063 25.927117 \n",
       "L 177.389063 25.927117 \n",
       "L 177.389063 18.927117 \n",
       "L 157.389063 18.927117 \n",
       "z\n",
       "\" style=\"fill: #1f77b4\"/>\n",
       "    </g>\n",
       "    <g id=\"text_14\">\n",
       "     <!-- origin -->\n",
       "     <g transform=\"translate(185.389063 25.927117) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \n",
       "L 1178 3500 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 3500 \n",
       "z\n",
       "M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 4134 \n",
       "L 603 4134 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-67\" d=\"M 2906 1791 \n",
       "Q 2906 2416 2648 2759 \n",
       "Q 2391 3103 1925 3103 \n",
       "Q 1463 3103 1205 2759 \n",
       "Q 947 2416 947 1791 \n",
       "Q 947 1169 1205 825 \n",
       "Q 1463 481 1925 481 \n",
       "Q 2391 481 2648 825 \n",
       "Q 2906 1169 2906 1791 \n",
       "z\n",
       "M 3481 434 \n",
       "Q 3481 -459 3084 -895 \n",
       "Q 2688 -1331 1869 -1331 \n",
       "Q 1566 -1331 1297 -1286 \n",
       "Q 1028 -1241 775 -1147 \n",
       "L 775 -588 \n",
       "Q 1028 -725 1275 -790 \n",
       "Q 1522 -856 1778 -856 \n",
       "Q 2344 -856 2625 -561 \n",
       "Q 2906 -266 2906 331 \n",
       "L 2906 616 \n",
       "Q 2728 306 2450 153 \n",
       "Q 2172 0 1784 0 \n",
       "Q 1141 0 747 490 \n",
       "Q 353 981 353 1791 \n",
       "Q 353 2603 747 3093 \n",
       "Q 1141 3584 1784 3584 \n",
       "Q 2172 3584 2450 3431 \n",
       "Q 2728 3278 2906 2969 \n",
       "L 2906 3500 \n",
       "L 3481 3500 \n",
       "L 3481 434 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"61.181641\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"102.294922\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-67\" x=\"130.078125\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"193.554688\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"221.337891\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"patch_29\">\n",
       "     <path d=\"M 157.389063 40.605242 \n",
       "L 177.389063 40.605242 \n",
       "L 177.389063 33.605242 \n",
       "L 157.389063 33.605242 \n",
       "z\n",
       "\" style=\"fill: url(#h379c09ba72)\"/>\n",
       "    </g>\n",
       "    <g id=\"text_15\">\n",
       "     <!-- subsampled -->\n",
       "     <g transform=\"translate(185.389063 40.605242) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-62\" d=\"M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "M 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2969 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \n",
       "Q 1497 1759 1228 1600 \n",
       "Q 959 1441 959 1056 \n",
       "Q 959 750 1161 570 \n",
       "Q 1363 391 1709 391 \n",
       "Q 2188 391 2477 730 \n",
       "Q 2766 1069 2766 1631 \n",
       "L 2766 1759 \n",
       "L 2194 1759 \n",
       "z\n",
       "M 3341 1997 \n",
       "L 3341 0 \n",
       "L 2766 0 \n",
       "L 2766 531 \n",
       "Q 2569 213 2275 61 \n",
       "Q 1981 -91 1556 -91 \n",
       "Q 1019 -91 701 211 \n",
       "Q 384 513 384 1019 \n",
       "Q 384 1609 779 1909 \n",
       "Q 1175 2209 1959 2209 \n",
       "L 2766 2209 \n",
       "L 2766 2266 \n",
       "Q 2766 2663 2505 2880 \n",
       "Q 2244 3097 1772 3097 \n",
       "Q 1472 3097 1187 3025 \n",
       "Q 903 2953 641 2809 \n",
       "L 641 3341 \n",
       "Q 956 3463 1253 3523 \n",
       "Q 1550 3584 1831 3584 \n",
       "Q 2591 3584 2966 3190 \n",
       "Q 3341 2797 3341 1997 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6d\" d=\"M 3328 2828 \n",
       "Q 3544 3216 3844 3400 \n",
       "Q 4144 3584 4550 3584 \n",
       "Q 5097 3584 5394 3201 \n",
       "Q 5691 2819 5691 2113 \n",
       "L 5691 0 \n",
       "L 5113 0 \n",
       "L 5113 2094 \n",
       "Q 5113 2597 4934 2840 \n",
       "Q 4756 3084 4391 3084 \n",
       "Q 3944 3084 3684 2787 \n",
       "Q 3425 2491 3425 1978 \n",
       "L 3425 0 \n",
       "L 2847 0 \n",
       "L 2847 2094 \n",
       "Q 2847 2600 2669 2842 \n",
       "Q 2491 3084 2119 3084 \n",
       "Q 1678 3084 1418 2786 \n",
       "Q 1159 2488 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1356 3278 1631 3431 \n",
       "Q 1906 3584 2284 3584 \n",
       "Q 2666 3584 2933 3390 \n",
       "Q 3200 3197 3328 2828 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-64\" d=\"M 2906 2969 \n",
       "L 2906 4863 \n",
       "L 3481 4863 \n",
       "L 3481 0 \n",
       "L 2906 0 \n",
       "L 2906 525 \n",
       "Q 2725 213 2448 61 \n",
       "Q 2172 -91 1784 -91 \n",
       "Q 1150 -91 751 415 \n",
       "Q 353 922 353 1747 \n",
       "Q 353 2572 751 3078 \n",
       "Q 1150 3584 1784 3584 \n",
       "Q 2172 3584 2448 3432 \n",
       "Q 2725 3281 2906 2969 \n",
       "z\n",
       "M 947 1747 \n",
       "Q 947 1113 1208 752 \n",
       "Q 1469 391 1925 391 \n",
       "Q 2381 391 2643 752 \n",
       "Q 2906 1113 2906 1747 \n",
       "Q 2906 2381 2643 2742 \n",
       "Q 2381 3103 1925 3103 \n",
       "Q 1469 3103 1208 2742 \n",
       "Q 947 2381 947 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-73\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-75\" x=\"52.099609\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-62\" x=\"115.478516\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"178.955078\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"231.054688\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6d\" x=\"292.333984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"389.746094\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"453.222656\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"481.005859\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-64\" x=\"542.529297\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p8cecdc083d\">\n",
       "   <rect x=\"59.690625\" y=\"9.328679\" width=\"195.3\" height=\"138.6\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       " <defs>\n",
       "  <pattern id=\"h379c09ba72\" patternUnits=\"userSpaceOnUse\" x=\"0\" y=\"0\" width=\"72\" height=\"72\">\n",
       "   <rect x=\"0\" y=\"0\" width=\"73\" height=\"73\" fill=\"#ff7f0e\"/>\n",
       "   <path d=\"M -36 36 \n",
       "L 36 -36 \n",
       "M -24 48 \n",
       "L 48 -24 \n",
       "M -12 60 \n",
       "L 60 -12 \n",
       "M 0 72 \n",
       "L 72 0 \n",
       "M 12 84 \n",
       "L 84 12 \n",
       "M 24 96 \n",
       "L 96 24 \n",
       "M 36 108 \n",
       "L 108 36 \n",
       "\" style=\"fill: #000000; stroke: #000000; stroke-width: 1.0; stroke-linecap: butt; stroke-linejoin: miter\"/>\n",
       "  </pattern>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 350x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "d2l.show_list_len_pair_hist(\n",
    "    ['origin', 'subsampled'],\n",
    "    '# tokens per sentence',\n",
    "    'count',\n",
    "    sentences,\n",
    "    subsampled\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_counts(token):\n",
    "    return (\n",
    "        f'# of \"{token}\": '\n",
    "        f'before={sum([l.count(token) for l in sentences])}, '\n",
    "        f'after={sum([l.count(token) for l in subsampled])}'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# of \"the\": before=50770, after=1971'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_counts('the')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For individual tokens, the sampling rate of the high-frequency words \"the\" is less than 1/20.\n",
    "\n",
    "On the other hand, low-frequency words \"join\" are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# of \"join\": before=45, after=45'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_counts('join')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After subsampling, we map tokens to their indices for the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], [6697, 3228, 710, 1773], [3922, 1922, 4743]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [vocab[line] for line in subsampled]\n",
    "corpus[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These indices correspond to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], ['years', 'join', 'board', 'director'], ['n.v.', 'dutch', 'publishing']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subsampled[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.3.3. Extracting Center Words and Context Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `get_centers_and_contexts` function extracts all the center words and their context words from `corpus`. It uniformly samples an integer between 1 and `max_window_size` at random as the context window size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_centers_and_contexts(corpus, max_window_size):\n",
    "    '''Return center words and context words in skip-gram'''\n",
    "    centers = []\n",
    "    contexts = []\n",
    "\n",
    "    for line in corpus:\n",
    "        # To form a \"center word--context word\" pair,\n",
    "        # each sentence needs to have at least 2 words\n",
    "        if len(line) < 2:\n",
    "            continue\n",
    "\n",
    "        # add line to centers\n",
    "        centers += line\n",
    "        for i in range(len(line)): # context window centered at `i`\n",
    "            window_size = random.randint(1, max_window_size)\n",
    "            indices = list(range(\n",
    "                max(0, i - window_size), # left limit\n",
    "                min(len(line), i + 1 + window_size), # right limit\n",
    "            ))\n",
    "            # exclude the center word from the context words\n",
    "            indices.remove(i)\n",
    "            # add remaining words to contexts\n",
    "            contexts.append([line[idx] for idx in indices])\n",
    "\n",
    "    return centers, contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create an artificial dataset containing two sentences of 7 and 3 words, respectively. Let the maximum context window size be 2 nad print out all the center words and their context words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset:  [[0, 1, 2, 3, 4, 5, 6], [7, 8, 9]]\n",
      "center 0 has contexts [1, 2]\n",
      "center 1 has contexts [0, 2]\n",
      "center 2 has contexts [1, 3]\n",
      "center 3 has contexts [1, 2, 4, 5]\n",
      "center 4 has contexts [3, 5]\n",
      "center 5 has contexts [4, 6]\n",
      "center 6 has contexts [4, 5]\n",
      "center 7 has contexts [8]\n",
      "center 8 has contexts [7, 9]\n",
      "center 9 has contexts [8]\n"
     ]
    }
   ],
   "source": [
    "tiny_dataset = [\n",
    "    list(range(7)), # a sentence with 7 tokens\n",
    "    list(range(7, 10)) # another sentence with 3 tokens\n",
    "]\n",
    "max_window_size = 2\n",
    "\n",
    "print('dataset: ', tiny_dataset)\n",
    "centers, contexts = get_centers_and_contexts(tiny_dataset, max_window_size)\n",
    "for center, context in zip(centers, contexts):\n",
    "    print('center', center, 'has contexts', context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is randomness in the sampling of context windows. Hence, the output of the following code may vary.\n",
    "\n",
    "When training on the PTB dataset, we set the maximum context window size to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# center-context pairs: 1500707\n"
     ]
    }
   ],
   "source": [
    "max_window_size = 5\n",
    "all_centers, all_contexts = get_centers_and_contexts(corpus, max_window_size)\n",
    "print(f'# center-context pairs: {sum([len(contexts) for contexts in all_contexts])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.3.4. Negative Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use negative sampling for approximate training.\n",
    "\n",
    "To sample noise words according to a predefined distribution, we define the `RandomGenerator` class, where the (possibly unnormalized) sampling distribution is passed via the argument `sampling_weights`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomGenerator:\n",
    "    '''Randomly draw among {1,..., n} according to n sampling weights'''\n",
    "    def __init__(self, sampling_weights):\n",
    "        # exclude 0 for convenience\n",
    "        self.population = list(range(1, len(sampling_weights) + 1))\n",
    "        self.sampling_weights = sampling_weights\n",
    "        self.candidates = []\n",
    "        self.i = 0\n",
    "\n",
    "    def draw(self):\n",
    "        if self.i == len(self.candidates):\n",
    "            # cache `k` random sampling results\n",
    "            self.candidates = random.choices(\n",
    "                self.population,\n",
    "                self.sampling_weights,\n",
    "                k=10000\n",
    "            )\n",
    "            self.i = 0\n",
    "        \n",
    "        self.i += 1\n",
    "        return self.candidates[self.i - 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can draw 10 random variables $X$ among indices 1, 2, and 3 with sampling probability\n",
    "\\begin{split}\n",
    "P(X=1)=2/9, P(X=2)=3/9, \\textrm{and } P(X=3)=4/9.\n",
    "\\end{split}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1, 3, 3, 2, 3, 3, 2, 3, 1]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampling_weights = [2, 3, 4] # 3 tokens with weights 2, 3 and 4\n",
    "generator = RandomGenerator(sampling_weights)\n",
    "[generator.draw() for _ in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a pair of center word and context word, we randomly sample `K` (5 in the experiment) noise words. In the word2vec paper, the sampling probability $P(w)$ of a noise word $w$ is set to its relatie frequency in the dictionary raised to the power of 0.75."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_negatives(all_contexts, vocab, counter, K):\n",
    "    '''Return noise words in negative sampling'''\n",
    "    # Sampling weights for words with indices 1, 2, ...\n",
    "    # (index 0 is the excluded unknown token) in the vocabulary\n",
    "    sampling_weights = [counter[vocab.to_tokens(i)]**0.75 for i in range(1, len(vocab))]\n",
    "\n",
    "    all_negatives = []\n",
    "    generator = RandomGenerator(sampling_weights)\n",
    "\n",
    "    for contexts in all_contexts:\n",
    "        negatives = []\n",
    "        while len(negatives) < len(contexts) * K:\n",
    "            neg = generator.draw()\n",
    "            # noise words cannot be context words\n",
    "            if neg not in contexts:\n",
    "                negatives.append(neg)\n",
    "\n",
    "        all_negatives.append(negatives)\n",
    "\n",
    "    return all_negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_negatives = get_negatives(all_contexts, vocab, counter, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.3.5. Loading Training Examples in Minibatches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all the center words together with their context words and sampled noise words are extracted, they will be transformed into minibatches of examples that can be iteratively loaded during training.\n",
    "\n",
    "In a minibatch, the $i$-th example includes a center word and its $n_i$ context words and $m_i$ noise words. Due to varying context window sizes, $n_i + m_i$ varies for different $i$. Thus, for each example in a minibatch, we concatenate its context words and noise words in the `contexts_negatives` variable, and padd zeros until the concatenation length reaches the maximum context window size $\\max_i n_i + m_i$ (`max_len`) in the minibatch.\n",
    "\n",
    "To exclude paddings in the calculation of the loss, we define a mask variable `masks`. There is a one-to-one correspondence between elements in `masks` and elements in `contexts_negatives`, where zeros (otherwise ones) in `masks` correspond to paddings (otherwise non-paddings) in `contexts_negatives`.\n",
    "\n",
    "To distinguish between positive and negative examples in `contexts_negatives`, we separte context words from noise words via a `labels` variable. Similar to `masks`, there is also a one-to-one correspondence between elements in `labels` and elements in `contexts_negatives`, where ones (otherwise zeros) in `labels` correspond to context words (positive examples) in `contexts_negatives`.\n",
    "\n",
    "In the `batchify` function below, its input `data` is a list with length equal to the batch size, where each element is an example consisting of the center word `center`, its context words `context`, and its noise words `negatives`. This function returns a minibatch that can be loaded for calcualtions during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data):\n",
    "    '''Return a minibatch of examples for skip-gram with negative sampling'''\n",
    "    # data: (batch size, centers, contexts, negatives)\n",
    "    max_len = max(len(c) + len(n) for _, c, n in data)\n",
    "    # minibatch of center words\n",
    "    centers = []\n",
    "    # minibatch of context words and noise words associated with the center word\n",
    "    contexts_negatives = []\n",
    "    # mask for non-padding entries\n",
    "    masks = []\n",
    "    # mask for context words and noise words\n",
    "    labels = []\n",
    "\n",
    "    for center, context, negative in data:\n",
    "        # take one example\n",
    "        cur_len = len(context) + len(negative)\n",
    "        # add center word\n",
    "        centers += [center]\n",
    "        # add context words, noise words, and padding associated with the center word\n",
    "        contexts_negatives += [context + negative + [0] * (max_len - cur_len)]\n",
    "        # add mask for padding\n",
    "        masks += [[1] * cur_len + [0] * (max_len - cur_len)]\n",
    "        # add mask for context words\n",
    "        labels += [[1] * len(context) + [0] * (max_len - len(context))]\n",
    "\n",
    "    return (\n",
    "        torch.tensor(centers).reshape((-1, 1)), # (batch size, 1) for center words\n",
    "        torch.tensor(contexts_negatives), # (batch size, max_len) for context words and noise words\n",
    "        torch.tensor(masks), # (batch size, max_len) for mask\n",
    "        torch.tensor(labels) # (batch size, max_len) for mask\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "centers = tensor([[1],\n",
      "        [4],\n",
      "        [1]])\n",
      "contexts_negatives = tensor([[2, 2, 3, 3, 3, 3, 0, 0],\n",
      "        [2, 2, 2, 3, 3, 0, 0, 0],\n",
      "        [3, 3, 3, 1, 4, 4, 2, 2]])\n",
      "masks = tensor([[1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "labels = tensor([[1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "# test a minibatch of three examples\n",
    "X1 = (1, # center word\n",
    "      [2, 2], # context words\n",
    "      [3, 3, 3, 3]) # noise words\n",
    "X2 = (4, # center word\n",
    "      [2, 2, 2], # context words\n",
    "      [3, 3]) # noise words\n",
    "X3 = (1, # center word\n",
    "      [3, 3, 3, 1], # context words\n",
    "      [4, 4, 2, 2]) # noise words\n",
    "\n",
    "examples = [X1, X2, X3]\n",
    "batch = batchify(examples)\n",
    "\n",
    "names = ['centers', 'contexts_negatives', 'masks', 'labels']\n",
    "for name, data in zip(names, batch):\n",
    "    print(name, '=', data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.3.6. Putting It All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataset class\n",
    "class PTBDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, centers, contexts, negatives):\n",
    "        # centers, contexts, and negatives are of equal length\n",
    "        assert len(centers) == len(contexts) == len(negatives)\n",
    "\n",
    "        self.centers = centers\n",
    "        self.contexts = contexts\n",
    "        self.negatives = negatives\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (\n",
    "            self.centers[index], # center word\n",
    "            self.contexts[index], # positive context words\n",
    "            self.negatives[index], # negative noise words\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.centers)\n",
    "    \n",
    "    \n",
    "def load_data_ptb(batch_size, max_window_size, num_noise_words):\n",
    "    '''Download the PTB dataset and then load it into memory'''\n",
    "    num_workers = d2l.get_dataloader_workers()\n",
    "\n",
    "    sentences = read_ptb()\n",
    "    vocab = d2l.Vocab(sentences, min_freq=10)\n",
    "\n",
    "    subsampled, counter = subsample(sentences, vocab)\n",
    "    corpus = [vocab[line] for line in subsampled]\n",
    "    all_centers, all_contexts = get_centers_and_contexts(corpus, max_window_size)\n",
    "    all_negatives = get_negatives(all_contexts, vocab, counter, num_noise_words)\n",
    "        \n",
    "    # intialize a dataset instance\n",
    "    dataset = PTBDataset(all_centers, all_contexts, all_negatives)\n",
    "    # create a dataloader\n",
    "    data_iter = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=batchify, # use batchify function to collate examples\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return data_iter, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "max_window_size = 5\n",
    "num_noise_words = 5\n",
    "\n",
    "data_iter, vocab = load_data_ptb(batch_size, max_window_size, num_noise_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the first minibatch\n",
    "for batch in data_iter:\n",
    "    for name, data in zip(names, batch):\n",
    "        print(name, 'shape:', data.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.4. Pretraining word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement the skip-gram model and then pretrain word2vec using negative sampling on the PTB dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we need to obtain the data iterator and the vocabulary of the PTB dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "max_window_size = 5\n",
    "num_noise_words = 5\n",
    "\n",
    "data_iter, vocab = load_data_ptb(batch_size, max_window_size, num_noise_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.4.1. The Skip-Gram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15.4.1.1. Embedding Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from Chapter 10 that an embedding layer maps a token's index to its feature vector.\n",
    "\n",
    "The weight of this layer is a matrix whose number of rows equals to the dictionary size (`input_dim`) and the number of columns equals to the vector dimension for each token (`output_dim`). After a word embedding model is trained, the weight of the embedding layer is what we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter embedding_weight (torch.Size([20, 4]), dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "input_dim = 20 # vocabulary size\n",
    "output_dim = 4 # vector dimension\n",
    "\n",
    "embed = nn.Embedding(num_embeddings=input_dim,\n",
    "                     embedding_dim=output_dim)\n",
    "print(f'Parameter embedding_weight ({embed.weight.shape}, '\n",
    "      f'dtype={embed.weight.dtype})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input of an embedding layer is the index of a token (word).\n",
    "\n",
    "For any token index $i$, its vector representation can be obtained from the $i$-th row of the weight matrix in the embedding layer. Since the vector dimension (`output_dim`) is set to 4, the embedding layer returns vectors with shape (2, 3, 4) for a minibatch of 2 examples, each consisting of 3 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.4735, -1.2734,  1.3280,  1.4527],\n",
       "          [ 0.1065, -0.8098, -1.5031,  0.3291],\n",
       "          [ 0.4750, -0.7327, -0.5792,  0.3721]],\n",
       " \n",
       "         [[ 1.2788, -0.3188, -1.5614,  0.7817],\n",
       "          [-1.6512, -1.8211,  1.2905, -0.8847],\n",
       "          [-1.5387,  1.2123, -1.1465, -0.9661]]], grad_fn=<EmbeddingBackward0>),\n",
       " torch.Size([2, 3, 4]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor(\n",
    "    [[1, 2, 3],\n",
    "     [4, 5, 6]],\n",
    ")\n",
    "\n",
    "embed(X), embed(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15.4.1.2. Defining the Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the forward propagation, the input of the skip-gram model includes \n",
    "* the center word indices `center` of shape (batch size, 1), and\n",
    "* the concatenated context and noise word indices `contexts_and_negatives` of shape (batch size, `max_len`).\n",
    "\n",
    "These two variables are transformed from the token indices into vectors via the embedding layer, and then their batch matrix multiplication returns an output of shape (batch size, 1, `max_len`), where each element is the dot product of a center word vector and a context or noise word vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skip_gram(center, contexts_and_negatives, embed_v, embed_u):\n",
    "    '''Compute the skip-gram objective function for one center word'''\n",
    "    v = embed_v(center) # center word vector\n",
    "    u = embed_u(contexts_and_negatives) # context and noise word vectors\n",
    "    pred = torch.bmm(v, u.permute(0, 2, 1)) # bmm: batch matrix multiplication\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 4])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skip_gram(\n",
    "    center=torch.ones((2, 1), dtype=torch.long), # batch size: 2\n",
    "    contexts_and_negatives=torch.ones((2, 4), dtype=torch.long), # 4 combined context and noise words\n",
    "    embed_v=embed, # embedding for center words\n",
    "    embed_u=embed # embedding for context and noise words\n",
    ").shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.4.2. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15.4.2.1. Binary Cross-Entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the binary cross-entropy loss to train the skip-gram model with negative sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigmoidBCELoss(nn.Module):\n",
    "    '''Binary cross-entropy loss with masking'''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, inputs, target, mask=None):\n",
    "        out = nn.functional.binary_cross_entropy_with_logits(\n",
    "            inputs, target, weight=mask, reduction='none'\n",
    "        )\n",
    "        return out.mean(dim=1)\n",
    "    \n",
    "loss = SigmoidBCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that\n",
    "* `mask` is a variable for excluding paddings in the loss calculation, and\n",
    "* `label` is a variable for distinguishing between context words and noise words.\n",
    "\n",
    "We can calculate the binary cross-entorpy loss for the given variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9352, 1.8462])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = torch.tensor(\n",
    "    [[1.1, -2.2, 3.3, -4.4]] * 2\n",
    ")\n",
    "\n",
    "mask = torch.tensor(\n",
    "    [[1, 1, 1, 1],\n",
    "     [1, 1, 0, 0]]\n",
    ")\n",
    "\n",
    "label = torch.tensor(\n",
    "    [[1., 0., 0., 0.],\n",
    "     [0., 1., 0., 0.]]\n",
    ")\n",
    "\n",
    "loss(pred, label, mask) * mask.shape[1] / mask.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the sigmoid function in the binary cross-entropy loss to calculate the above results. We can consider the two outputs as two normalized losses that are average over non-masked predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9352\n",
      "1.8462\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return -math.log(1 / (1 + math.exp(-x)))\n",
    "\n",
    "print(f'{(sigmoid(1.1) + sigmoid(2.2) + sigmoid(-3.3) + sigmoid(4.4)) / 4:.4f}')\n",
    "print(f'{(sigmoid(-1.1) + sigmoid(-2.2)) / 2:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15.4.2.2. Initializing Model Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need two embedding layers for all the words in the vocabulary: one for the center words and the other for the context words and noise words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 100\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Embedding(num_embeddings=len(vocab),\n",
    "                 embedding_dim=embed_size), # embedding for center words\n",
    "    nn.Embedding(num_embeddings=len(vocab),\n",
    "                    embedding_dim=embed_size) # embedding for context words\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15.4.2.3. Defning the Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, data_iter, lr, num_epochs, device=d2l.try_gpu()):\n",
    "    # initialize embedding parameters\n",
    "    def init_weights(module):\n",
    "        if type(module) == nn.Embedding:\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "\n",
    "    net.apply(init_weights)\n",
    "    net = net.to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    # create an animator\n",
    "    animator = d2l.Animator(xlabel='epoch', ylabel='loss',\n",
    "                            xlim=[1, num_epochs])\n",
    "    \n",
    "    # sum of normalized loss, number of normalized losses\n",
    "    metric = d2l.Accumulator(2) # loss_sum, num_examples\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        timer = d2l.Timer()\n",
    "        num_batches = len(data_iter)\n",
    "\n",
    "        for i, batch in enumerate(data_iter):\n",
    "            optimizer.zero_grad()\n",
    "            # center words, context+noise words, masks, labels\n",
    "            center, context_negative, mask, label = [\n",
    "                data.to(device) for data in batch\n",
    "            ]\n",
    "\n",
    "            # forward pass\n",
    "            pred = skip_gram(center,\n",
    "                             context_negative,\n",
    "                             embed_v=net[0],\n",
    "                             embed_u=net[1])\n",
    "            # compute loss\n",
    "            l = (loss(pred.reshape(label.shape).float(),\n",
    "                      label.float(),\n",
    "                      mask)\n",
    "                      / mask.sum(axis=1) * mask.shape[1])\n",
    "            \n",
    "            # backward pass\n",
    "            l.sum().backward()\n",
    "            # update parameters\n",
    "            optimizer.step()\n",
    "            # update metric\n",
    "            metric.add(l.sum(), l.numel())\n",
    "\n",
    "            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n",
    "                animator.add(epoch + (i + 1) / num_batches,\n",
    "                             (metric[0] / metric[1],))\n",
    "    \n",
    "    print(f'loss {metric[0] / metric[1]:.3f}, '\n",
    "          f'{metric[1] / timer.stop():.1f} tokens/sec on {str(device)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a skip-gram model using negative sampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.002\n",
    "num_epochs = 5\n",
    "\n",
    "train(net, data_iter, lr, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.4.3. Applying Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the word2vec model, we can use the cosine similarity of word vectors from the trained model to find words from the dictionary that are most semantically similar to an input word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_tokens(query_token, k, embed):\n",
    "    W = embed.weight.data\n",
    "    x = W[vocab[query_token]]\n",
    "\n",
    "    # compute the cosine similarity. Add 1e-9 for numerical stability\n",
    "    cos = torch.mv(W, x) / torch.sqrt(torch.sum(W * W, dim=1) *\n",
    "                                      torch.sum(x * x) + 1e-9)\n",
    "    topk = torch.topk(cos, k=k+1)[1].cpu().numpy().astype('int32')\n",
    "\n",
    "    for i in topk[1:]: # remove the input words\n",
    "        print(f'cosine sim={float(cos[i]):.3f}: {vocab.to_tokens(i)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_similar_tokens('chip', 3, net[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.5. Word Embedding with Global Vectors (GloVe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word-word co-occurrences within context windows may carry rich semantic information. For example, in a large corpus word “solid” is more likely to co-occur with “ice” than “steam”, but word “gas” probably co-occurs with “steam” more frequently than “ice”.\n",
    "\n",
    "The global corpus statistics of such co-occurrences can be pre-computed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.5.1. Skip-Gram with Global Corpus Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Denoting by $q_{ij}$ the conditional probability $P(w_j \\mid w_i)$ of word $w_j$ given word $w_i$ in the skip-gram model, we have\n",
    "\\begin{split}\n",
    "P(w_j \\mid w_i) = q_{ij}=\\frac{\\exp(\\mathbf{u}_j^\\top \\mathbf{v}_i)}{ \\sum_{k \\in \\mathcal{V}} \\exp(\\mathbf{u}_k^\\top \\mathbf{v}_i)},\n",
    "\\end{split}\n",
    "where for any index $i$, vectors $\\mathbf{v}_i$ and $\\mathbf{u}_i$ represent word $w_i$ as the center word and the context word, respectively, and $\\mathcal{V}={0, 1, \\ldots, |\\mathcal{V}|-1}$ is the index set of the vocabulary.\n",
    "\n",
    "Consider the word $w_i$ that may occur multiple times in the corpus. In the entire corpus, all the context words wherever $w_i$ is taken as their center word form a *multiset* $\\mathcal{C}_i$ of word indcies that *allows for multiple instances of the same element*.\n",
    "\n",
    "For any element, its number of instances is called its *multiplicity*.\n",
    "\n",
    "For example, suppose that the word $w_i$ occurs twice in the corpus and indices of the context words that take $w_i$ as their center word in the two context windows are $k, j, m, k$ and $k, l, k, j$. Therefore, the multiset $\\mathcal{C}_i$ of word indices is $\\mathcal{C}_i = \\{j, j, k, k, k, k, l, m\\}$, where the multiplicity of elements $j, k, l, m$ are 2, 4, 1, and 1, respectively.\n",
    "\n",
    "Suppose that the multiplicity of element $j$ in this multiset $\\mathcal{C}_i$ is denoted as $x_{ij}$, this is the global co-occurrence count of word $w_j$ (as the context word) and word $w_i$ (as the center word) in the same context window in the entire corpus.\n",
    "\n",
    "Using such global corpus statistics, the loss function of the skip-gram model is rewritten as\n",
    "\\begin{split}\n",
    "-\\sum_{i\\in\\mathcal{V}}\\sum_{j\\in\\mathcal{V}} x_{ij} \\log\\,q_{ij}.\n",
    "\\end{split}\n",
    "\n",
    "Suppose that the number of all the context words is denoted as $x_i$ in the context windows where $w_i$ occurs as their center word, which is equivalent to the sum of the multiplicities of all the elements in the multiset $\\mathcal{C}_i$, i.e., $x_i = | \\mathcal{C}_i |$. Let $p_{ij}$ be the conditional probability $x_{ij}/x_i$ for generating context word $w_j$ given center word $w_i$, then the loss function of the skip-gram model can be further rewritten as\n",
    "\\begin{split}\n",
    "-\\sum_{i\\in\\mathcal{V}}\\sum_{j\\in\\mathcal{V}} x_{ij} \\log\\,q_{ij} =-\\sum_{i\\in\\mathcal{V}} x_i \\sum_{j\\in\\mathcal{V}} p_{ij} \\log\\,q_{ij}.\n",
    "\\end{split}\n",
    "\n",
    "We can see that $-\\sum_{j\\in\\mathcal{V}} p_{ij} \\log\\,q_{ij}$ calculates the cross-entorpy of the conditional distribution $p_{ij}$ of global corpus statistics and the conditional distribution $q_{ij}$ of the model predictions. This loss is also weighted by $x_i$. Minimizing this loss function will allow the predicted conditional distribution to get close to the conditional distribution from the global corpus statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.5.2. The GloVe Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *global vectors for word representation (GloVe)* model makes 3 changes to the skip-gram model based on squared loss:\n",
    "1. Use variables $p'_{ij}=x_{ij}$ and $q'_{ij}=\\exp(\\mathbf{u}_j^\\top \\mathbf{v}_i)$ that are not probability distributions and take the logarithm of both, so the squared loss term is\n",
    "\\begin{split}\n",
    "\\left(\\log\\,p'_{ij} - \\log\\,q'_{ij}\\right)^2 = \\left(\\mathbf{u}_j^\\top \\mathbf{v}_i - \\log\\,x_{ij}\\right)^2.\n",
    "\\end{split}\n",
    "2. Add two scalar model parameters for each word $w_i$:\n",
    "    * the center word bias $b_i$, and\n",
    "    * the context word bias $c_i$.\n",
    "3. Replace the weight of each loss term with the weight function $h(x_{ij})$, where $h(x)$ is increasing in the interval of [0, 1].\n",
    "\n",
    "Putting all things together, training GloVe is to minimize the following loss function:\n",
    "\\begin{split}\n",
    "\\sum_{i\\in\\mathcal{V}} \\sum_{j\\in\\mathcal{V}} h(x_{ij}) \\left(\\mathbf{u}_j^\\top \\mathbf{v}_i + b_i + c_j - \\log\\,x_{ij}\\right)^2.\n",
    "\\end{split}\n",
    "\n",
    "For the weight function $h(x)$, a sugguested choice is\n",
    "\\begin{split}\n",
    "h(x) = (x/c) ^\\alpha \\textrm{ if } x < c \\textrm{ else } 1,\n",
    "\\end{split}\n",
    "where $\\alpha = 0.75$ and $c = 100$.\n",
    "\n",
    "In this case, since $h(0)=0$, the squared loss term for any $x_{ij}=0$ can be omitted for computational efficiency. When using minibatch SGD, at each iteration we randomly sample a minibatch of *non-zero* $x_{ij}$ to calculate gradients and update the model parameters. These non-zero $x_{ij}$ are precomputed global corpus statistics; thus, the model is called GloVe for *Global Vectors*.\n",
    "\n",
    "If word $w_i$ appears in the context window of word $w_i$, then *vice versa*. Therefore, $x_{ij}=x_{ji}$. Unlike word2vec that fits the asymmetric condtional probability $p_{ij}$, GloVe fits the symmetric $\\log x_{ij}$. Therefore, the center word vector and the context word vector of any word are mathematically equivalent in the GloVe model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.5.3. Interpreting GloVe from the Ratio of Co-occurrence Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $p_{ij} \\stackrel{\\textrm{def}}{=} P(w_j \\mid w_i)$ be the conditional probability of generating context word $w_j$ given $w_i$ as the center word in the corpus.\n",
    "\n",
    "The table bellow lists several co-occurrence probabilities given words \"ice\" and \"steam\" and their ratios based on statistics from a large corpus.\n",
    "\n",
    "|          | $w_k$ = \"solid\" | $w_k$ = \"gas\" | $w_k$ = \"water\" | $w_k$ = \"fashion\" |\n",
    "|:--------:|:---------------:|:-------------:|:---------------:|:-----------------:|\n",
    "| $p_1=P(w_k \\mid \\textrm{\"ice\"})$ | 0.00019 | 0.000066 | 0.003 | 0.000017         |\n",
    "| $p_2=P(w_k \\mid \\textrm{\"steam\"})$ | 0.000022 | 0.00078 | 0.0022 | 0.000018      |\n",
    "| $p_1/p_2$ | 8.9 | 0.085 | 1.36 | 0.96 |\n",
    "\n",
    "Using this table as an example, we can see that\n",
    "* For a word $w_k$ that is related to \"ice\" but unrelated to \"steam\", such as $w_k$ = \"solid\", we expect a larger ratio of co-occurrence probabilities $p_1/p_2 = 8.9$.\n",
    "* For a word $w_k$ that is lreated to \"steam\" but unrelated to \"ice\", such as $w_k$ = \"gas\", we expect a smaller ratio of co-occurrence probabilities $p_1/p_2 = 0.085$.\n",
    "* For a word $w_k$ that is related to both \"ice\" and \"steam\", such as $w_k$ = \"water\", we expect a ratio of co-occurrence probabilities $p_1/p_2 = 1.36$ that is close to 1.\n",
    "* For a word $w_k$ that is unrelated to both \"ice\" and \"steam\", such as $w_k$ = \"fashion\", we expect a ratio of co-occurrence probabilities $p_1/p_2 = 0.96$ that is close to 1.\n",
    "\n",
    "*The ratio of co-occurrence probabilities can intuitively express the relationship between words.* Thus, for the ratio of co-occurrence probabilities $p_{ij}/p_{ik}$, with $w_i$ being the center word and $w_j$ and $w_k$ being the context words, we want to fit this ratio using some function $f$:\n",
    "\\begin{split}\n",
    "f(\\mathbf{u}_j, \\mathbf{u}_k, {\\mathbf{v}}_i) \\approx \\frac{p_{ij}}{p_{ik}}.\n",
    "\\end{split}\n",
    "\n",
    "Since the ratio of co-occurrence probabilities is a scalar, we require that $f$ be a scalar function, such as $f(\\mathbf{u}_j, \\mathbf{u}_k, {\\mathbf{v}}_i) = f\\left((\\mathbf{u}_j - \\mathbf{u}_k)^\\top {\\mathbf{v}}_i\\right)$. Switching word indices $j$ and $k$, it must hold that $f(x)f(-x)=1$, so one possible solution is $f(x)=\\exp(x)$, i.e.,\n",
    "\\begin{split}\n",
    "f(\\mathbf{u}_j, \\mathbf{u}_k, {\\mathbf{v}}_i) = \\frac{\\exp\\left(\\mathbf{u}_j^\\top {\\mathbf{v}}_i\\right)}{\\exp\\left(\\mathbf{u}_k^\\top {\\mathbf{v}}_i\\right)} \\approx \\frac{p_{ij}}{p_{ik}}.\n",
    "\\end{split}\n",
    "\n",
    "If we pick $\\exp\\left(\\mathbf{u}_j^\\top {\\mathbf{v}}_i\\right) \\approx \\alpha p_{ij}$, where $\\alpha$ is a constant, then since $p_{ij}=x_{ij}/x_i$, after taking the logarithm on both sides, we get\n",
    "\\begin{split}\n",
    "\\mathbf{u}_j^\\top {\\mathbf{v}}_i \\approx \\log\\,\\alpha + \\log\\,x_{ij} - \\log\\,x_i\n",
    "\\end{split}\n",
    "\n",
    "We may use additional bias terms to fit $- \\log\\, \\alpha + \\log\\, x_i$, such as the center word bias $b_i$ and the context word bias $c_j$:\n",
    "\\begin{split}\n",
    "\\mathbf{u}_j^\\top \\mathbf{v}_i + b_i + c_j \\approx \\log\\, x_{ij}.\n",
    "\\end{split}\n",
    "\n",
    "Measuring the squared error of the above approximation with weights, the GloVe loss function is obtained as before:\n",
    "\\begin{split}\n",
    "\\sum_{i\\in\\mathcal{V}} \\sum_{j\\in\\mathcal{V}} h(x_{ij}) \\left(\\mathbf{u}_j^\\top \\mathbf{v}_i + b_i + c_j - \\log\\,x_{ij}\\right)^2.\n",
    "\\end{split}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.6. Subword Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
