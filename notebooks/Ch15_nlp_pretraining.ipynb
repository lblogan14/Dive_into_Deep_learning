{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 15 - Natural Language Processing: Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.1. Word Embedding (word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In natural language processing, words are the basic unit of the meaning. The *word vectors* are vectors used to represent words, and can also be considered as feature vectors or representations of words.\n",
    "\n",
    "The technique of mapping words to real vectors is called *word embedding*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.1.1. One-Hot Vectors Are a Bad Choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that the number of different words in the dictionary (the dictionary size) is $N$, and each word corresponds to a different integer (index) from 0 to $N-1$. To obtain the one-hot vector representation for any word with index $i$, we create a length-$N$ vector with all 0s and set the element at position $i$ to 1.\n",
    "\n",
    "However, one-hot word vectors cannot accurately express the similarity between different words, such as the *cosine similarity*. For vectors $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^d$, their cosine similarity is the cosine of the angle between them:\n",
    "\\begin{split}\n",
    "\\frac{\\mathbf{x}^\\top \\mathbf{y}}{\\|\\mathbf{x}\\| \\|\\mathbf{y}\\|} \\in [-1, 1].\n",
    "\\end{split}\n",
    "Since the cosine similarity between one-hot vectors of any two different words is 0, one-hot vectors cannot encode similarities among words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.1.2. Self-Supervised word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word2vec algorithm maps each word to a fixed-length vector, and thes vectors can better express the similarity and analogy relationships among different words.\n",
    "\n",
    "The word2vec algorithm contains two models:\n",
    "* *skip-gram*, and\n",
    "* *continuous bag of words (CBOW)*.\n",
    "\n",
    "Since supervision comes from the data without labels, both skip-gram and continuous bag of words are self-supervised learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.1.3. The Skip-Gram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *skip-gram* model assumes that *a word can be used to generate its surrounding words in a text squence*.\n",
    "\n",
    "For example, suppose a text squence, \"the\", \"man\", \"loves\", \"his\", \"son\". If \"loves\" is chosen as the *center word* and we set the context window size to 2. As shown in the figure below, given the center word \"loves\", the skip-gram model considers the conditional probability for generating the *context words*: \"the\", \"man\", \"his\", \"son\", which are no more than 2 words away from the center word:\n",
    "\\begin{split}\n",
    "P(\\textrm{\"the\"},\\textrm{\"man\"},\\textrm{\"his\"},\\textrm{\"son\"}\\mid\\textrm{\"loves\"}).\n",
    "\\end{split}\n",
    "\n",
    "![](../imgs/ch15/skip-gram.svg)\n",
    "\n",
    "Assume that the context words are independently generated given the center words (i.e., conditional independence). Then, the above conditional probability can be rewritten as\n",
    "\\begin{split}\n",
    "P(\\textrm{\"the\"},\\textrm{\"man\"},\\textrm{\"his\"},\\textrm{\"son\"}\\mid\\textrm{\"loves\"}) = P(\\textrm{\"the\"}\\mid\\textrm{\"loves\"})\\cdot P(\\textrm{\"man\"}\\mid\\textrm{\"loves\"})\\cdot P(\\textrm{\"his\"}\\mid\\textrm{\"loves\"})\\cdot P(\\textrm{\"son\"}\\mid\\textrm{\"loves\"}).\n",
    "\\end{split}\n",
    "\n",
    "Each word in this model has two $d$-dimensional vector representations to calculate conditional probabilities.\n",
    "\n",
    "For any word with index $i$ in the dictionary, denote by $\\mathbf{v}_i \\in \\mathbb{R}^d$ and $\\mathbf{u}_i \\in \\mathbb{R}^d$ its two vectors when used as a *center word* and a *context word*, respectively. The conditional probability of generating any context word $w_o$ (with index $o$ in the dictionary) given the center word $w_c$ (with index $c$ in the dictionary) can be modeled by a softmax operation on vector dot products:\n",
    "\\begin{split}\n",
    "P(w_o \\mid w_c) = \\frac{\\exp(\\mathbf{u}_o^\\top \\mathbf{v}_c)}{ \\sum_{i \\in \\mathcal{V}} \\exp(\\mathbf{u}_i^\\top \\mathbf{v}_c)},\n",
    "\\end{split}\n",
    "where the vocabulary index set is $\\mathcal{V} = \\{0, 1, \\ldots, |\\mathcal{V}|-1\\}$.\n",
    "\n",
    "Given a text sequence of length $T$, where the word at time step $t$ is denoted by $w^{(t)}$. Assume that context words are independently generated given any center word. For context window size $m$, the likelihood function of the skip-gram model is the probability of generating all context words given any center word:\n",
    "\\begin{split}\n",
    "\\prod_{t=1}^{T} \\prod_{-m \\leq j \\leq m,\\ j \\neq 0} P(w^{(t+j)} \\mid w^{(t)}),\n",
    "\\end{split}\n",
    "where any time step that is less than 1 or greater than $T$ can be ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15.1.3.1. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model parameters in the skip-gram model are the center word vectors and the context word vectors for each word in the vocabulary.\n",
    "\n",
    "During training, we learn the model parameters by maximizing the likelihood function (i.e., maximum likelihood estimation), which is equivalent to minimizing the following loss function:\n",
    "\\begin{split}\n",
    "- \\sum_{t=1}^{T} \\sum_{-m \\leq j \\leq m,\\ j \\neq 0} \\textrm{log}\\, P(w^{(t+j)} \\mid w^{(t)}).\n",
    "\\end{split}\n",
    "\n",
    "Using SGD to minimize this loss function, in each iteration, we can randomly sample a shorter subsequence to calculate the (stochastic) gradient for this subsequence to update the model parameters.\n",
    "\n",
    "The (stochastic) gradients are the gradients of the log conditional probability with respect to the center word vector and the context word vector. Recall in the previous section that the log conditional probability involving any pair of the center word $w_c$ and the context word $w_o$ is\n",
    "\\begin{split}\n",
    "\\log P(w_o \\mid w_c) =\\mathbf{u}_o^\\top \\mathbf{v}_c - \\log\\left(\\sum_{i \\in \\mathcal{V}} \\exp(\\mathbf{u}_i^\\top \\mathbf{v}_c)\\right).\n",
    "\\end{split}\n",
    "\n",
    "Its gradient with respect to the center word vector $\\mathbf{v}_c$ is\n",
    "\\begin{split}\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial \\textrm{log}\\, P(w_o \\mid w_c)}{\\partial \\mathbf{v}_c}&= \\mathbf{u}_o - \\frac{\\sum_{j \\in \\mathcal{V}} \\exp(\\mathbf{u}_j^\\top \\mathbf{v}_c)\\mathbf{u}_j}{\\sum_{i \\in \\mathcal{V}} \\exp(\\mathbf{u}_i^\\top \\mathbf{v}_c)}\\\\&= \\mathbf{u}_o - \\sum_{j \\in \\mathcal{V}} \\left(\\frac{\\exp(\\mathbf{u}_j^\\top \\mathbf{v}_c)}{ \\sum_{i \\in \\mathcal{V}} \\exp(\\mathbf{u}_i^\\top \\mathbf{v}_c)}\\right) \\mathbf{u}_j\\\\&= \\mathbf{u}_o - \\sum_{j \\in \\mathcal{V}} P(w_j \\mid w_c) \\mathbf{u}_j.\n",
    "\\end{aligned}\n",
    "\\end{split}\n",
    "which requries the conditional probabilities of all words in the dictionary with $w_c$ as the center word.\n",
    "\n",
    "After training, for any word with index $i$ in the dictionary, we obtain both word vectors $\\mathbf{v}_i$ (as the center word) and $\\mathbf{u}_i$ (as the context word).\n",
    "\n",
    "In NLP, the center word vectors of the skip-gram model are typically used as the word representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.1.4. The Continuous Bag of Words (CBOW) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *continuous bag of words (CBOW)* model is similar to the skip-gram model. The main difference is that the CBOW model assumes that *a center word is generated based on its surrounding context words in the text sequence*.\n",
    "\n",
    "Using the same text sequence, \"the\", \"man\", \"loves\", \"his\", \"son\", with \"loves\" as the center word and the context window size being 2, the continuous bag of words model consideres the conditional probability of generating the center word \"loves\" based on the context words \"the\", \"man\", \"his\", \"son\":\n",
    "\\begin{split}\n",
    "P(\\textrm{\"loves\"}\\mid\\textrm{\"the\"},\\textrm{\"man\"},\\textrm{\"his\"},\\textrm{\"son\"}).\n",
    "\\end{split}\n",
    "as shown in the figure below.\n",
    "\n",
    "![](../imgs/ch15/cbow.svg)\n",
    "\n",
    "Since there are multiple context words in the CBOW model, these context word vectors are averaged in the calculation of the conditional probability.\n",
    "\n",
    "For any word with index $i$ in the dictionary, denote by $\\mathbf{v}_i \\in \\mathbb{R}^d$ and $\\mathbf{u}_i \\in \\mathbb{R}^d$ its two vectors when used as a *context word* and a *center word*, respectively, (the reverse of the skip-gram model). The conditional probability of generating any center word $w_c$ (with index $c$ in the dictionary) given the context words $w_{o_1}, \\ldots, w_{o_{2m}}$ (with indices $o_1, \\ldots, o_{2m}$ in the dictionary) can be modeled by\n",
    "\\begin{split}\n",
    "P(w_c \\mid w_{o_1}, \\ldots, w_{o_{2m}}) = \\frac{\\exp\\left(\\frac{1}{2m}\\mathbf{u}_c^\\top (\\mathbf{v}_{o_1} + \\ldots + \\mathbf{v}_{o_{2m}}) \\right)}{ \\sum_{i \\in \\mathcal{V}} \\exp\\left(\\frac{1}{2m}\\mathbf{u}_i^\\top (\\mathbf{v}_{o_1} + \\ldots + \\mathbf{v}_{o_{2m}}) \\right)}.\n",
    "\\end{split}\n",
    "\n",
    "For brevity, let $\\mathcal{W}_o = \\{w_{o_1}, \\ldots, w_{o_{2m}}\\}$ denote the set of context words and $\\bar{\\mathbf{v}}_o = \\left(\\mathbf{v}_{o_1} + \\ldots + \\mathbf{v}_{o_{2m}}\\right)/(2m)$ denote their average. Then, the above conditional probability can be rewritten as\n",
    "\\begin{split}\n",
    "P(w_c \\mid \\mathcal{W}_o) = \\frac{\\exp\\left(\\mathbf{u}_c^\\top \\bar{\\mathbf{v}}_o\\right)}{\\sum_{i \\in \\mathcal{V}} \\exp\\left(\\mathbf{u}_i^\\top \\bar{\\mathbf{v}}_o\\right)}.\n",
    "\\end{split}\n",
    "\n",
    "Given a text sequence of length $T$, where the word at time step $t$ is denoted by $w^{(t)}$. For context window size $m$, the likelihood function of the CBOW model is the probability of generating all center words given any context words:\n",
    "\\begin{split}\n",
    "\\prod_{t=1}^{T}  P(w^{(t)} \\mid  w^{(t-m)}, \\ldots, w^{(t-1)}, w^{(t+1)}, \\ldots, w^{(t+m)}).\n",
    "\\end{split}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15.1.4.1. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the CBOW model is similar to training the skip-gram model.\n",
    "\n",
    "The maximum likelihood estimation of the CBOW model is equivalent to minimizing the following loss function:\n",
    "\\begin{split}\n",
    "-\\sum_{t=1}^T  \\textrm{log}\\, P(w^{(t)} \\mid  w^{(t-m)}, \\ldots, w^{(t-1)}, w^{(t+1)}, \\ldots, w^{(t+m)}).\n",
    "\\end{split}\n",
    "\n",
    "Note that\n",
    "\\begin{split}\n",
    "\\log\\,P(w_c \\mid \\mathcal{W}_o) = \\mathbf{u}_c^\\top \\bar{\\mathbf{v}}_o - \\log\\,\\left(\\sum_{i \\in \\mathcal{V}} \\exp\\left(\\mathbf{u}_i^\\top \\bar{\\mathbf{v}}_o\\right)\\right).\n",
    "\\end{split}\n",
    "\n",
    "Hence, its gradient with respect to the context word vector $\\mathbf{v}_{o_i}$ ($i = 1, \\ldots, 2m$) is\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial \\log\\, P(w_c \\mid \\mathcal{W}_o)}{\\partial \\mathbf{v}_{o_i}} &= \\frac{1}{2m} \\left(\\mathbf{u}_c - \\sum_{j \\in \\mathcal{V}} \\frac{\\exp(\\mathbf{u}_j^\\top \\bar{\\mathbf{v}}_o)\\mathbf{u}_j}{ \\sum_{i \\in \\mathcal{V}} \\exp(\\mathbf{u}_i^\\top \\bar{\\mathbf{v}}_o)} \\right) \\\\\n",
    "&= \\frac{1}{2m}\\left(\\mathbf{u}_c - \\sum_{j \\in \\mathcal{V}} P(w_j \\mid \\mathcal{W}_o) \\mathbf{u}_j \\right).\n",
    "\\end{aligned}\n",
    "\n",
    "Unlike the skip-gram model, the continuous bag of words model typically uses context word vectors as the word representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.2. Approximate Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient calculation of the skip-gram and CBOW models involves the sum of all conditional probabilities of words in the dictionary. When the dictionary is large, the sum of all conditional probabilities is computationally expensive.\n",
    "\n",
    "To reduce the aforementioned computational cost, we can use *negative sampling* and *hierarchical softmax* to approximate the maximum likelihood estimation of the skip-gram and CBOW models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.2.1. Negative Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Negative sampling* modifies the original objective function.\n",
    "\n",
    "Given the context window of a center word $w_c$, the fact that any (context) word $w_o$ comes from this context window is considered as an event with the probability modeled by\n",
    "\\begin{split}\n",
    "P(D=1\\mid w_c, w_o) = \\sigma(\\mathbf{u}_o^\\top \\mathbf{v}_c),\n",
    "\\end{split}\n",
    "where $\\sigma$ is the sigmoid function:\n",
    "\\begin{split}\n",
    "\\sigma(x) = \\frac{1}{1+\\exp(-x)}.\n",
    "\\end{split}\n",
    "\n",
    "Given a text sequence of lenght $T$, denote by $w^{(t)}$ the word at time step $t$ and let the context window size be $m$, the joint probability of generating all context words in the text sequence is\n",
    "\\begin{split}\n",
    "\\prod_{t=1}^{T} \\prod_{-m \\leq j \\leq m,\\ j \\neq 0} P(D=1\\mid w^{(t)}, w^{(t+j)}).\n",
    "\\end{split}\n",
    "\n",
    "This joint probability is maximized to 1 only if all the word vectors are equal to infinity. To make the objective function more meaningful, *negative sampling* adds negative examples sampled from a predefined distribution.\n",
    "\n",
    "Denote by $S$ the event that a context word $w_o$ comes from the context window of a center word $w_c$. \n",
    "\n",
    "For this event involving $w_o$, from a predefined distribution $P(w)$, we can sample $K$ *noise words* that are not from this context window. Denote by $N_k$ the event that a noise word $w_k$ ($k = 1, \\ldots, K$) does not come from the context window of $w_c$.\n",
    "\n",
    "Assume that these events involving both the positive example $S$ and the negative examples $N_1, \\ldots, N_K$ are mutually independent. Then, the negative sampling rewrites the joint probability (involving only positive examples $S$) as\n",
    "\\begin{split}\n",
    "\\prod_{t=1}^{T} \\prod_{-m \\leq j \\leq m,\\ j \\neq 0} P(D=1\\mid w^{(t)}, w^{(t+j)}) = \\prod_{t=1}^{T} \\prod_{-m \\leq j \\leq m,\\ j \\neq 0} P(w^{(t+j)} \\mid w^{(t)}),\n",
    "\\end{split}\n",
    "where the conditional probability is approximated through events $S, N_1, \\ldots, N_K$:\n",
    "\\begin{split}\n",
    "P(w^{(t+j)} \\mid w^{(t)}) =P(D=1\\mid w^{(t)}, w^{(t+j)})\\prod_{k=1,\\ w_k \\sim P(w)}^K P(D=0\\mid w^{(t)}, w_k).\n",
    "\\end{split}\n",
    "\n",
    "Denote by $i_t$ and $h_k$ the indices of a word $w^{(t)}$ at time step $t$ of a text sequence and a noise word $w_k$, respectively. The logarithmic loss with respect to the conditional probabilities of the positive example and the negative examples is\n",
    "\\begin{split}\\begin{aligned}\n",
    "-\\log P(w^{(t+j)} \\mid w^{(t)})\n",
    "=& -\\log P(D=1\\mid w^{(t)}, w^{(t+j)}) - \\sum_{k=1,\\ w_k \\sim P(w)}^K \\log P(D=0\\mid w^{(t)}, w_k)\\\\\n",
    "=&-  \\log\\, \\sigma\\left(\\mathbf{u}_{i_{t+j}}^\\top \\mathbf{v}_{i_t}\\right) - \\sum_{k=1,\\ w_k \\sim P(w)}^K \\log\\left(1-\\sigma\\left(\\mathbf{u}_{h_k}^\\top \\mathbf{v}_{i_t}\\right)\\right)\\\\\n",
    "=&-  \\log\\, \\sigma\\left(\\mathbf{u}_{i_{t+j}}^\\top \\mathbf{v}_{i_t}\\right) - \\sum_{k=1,\\ w_k \\sim P(w)}^K \\log\\sigma\\left(-\\mathbf{u}_{h_k}^\\top \\mathbf{v}_{i_t}\\right).\n",
    "\\end{aligned}\\end{split}\n",
    "\n",
    "The computational cost for gradients at each training step does not depend on the dictionary size, but linearly depends on $K$. When setting the hyperparameter $K$ to a smaller value, the computational cost for gradients at each training with negative sampling is smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.2.2. Hierarchical Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *hierarchical softmax* uses a binary where each leaf node of the tree represents a word in the dictionary $\\mathcal{V}$, as shown in the figure below.\n",
    "\n",
    "![](../imgs/ch15/hi-softmax.svg)\n",
    "\n",
    "Denote by $L(w)$ the number of nodes (including both ends) on the path from the root node to the leaf node representing word $w$ in the binary tree.\n",
    "\n",
    "Let $n(w,j)$ be the $j$-th node on this path, with its context word vector being $\\mathbf{u}_{n(w,j)}$. For instance, in the figure above, $L(w_3) = 4$ since the path from the root node to the leaf node representing $w_3$ is $n(w_3, 1)$, $n(w_3, 2)$, $n(w_3, 3)$ and $w_3$.\n",
    "\n",
    "The *hierarchical softmax* approximates the conditional probability \n",
    "\\begin{split}\n",
    "P(w_o \\mid w_c) = \\frac{\\exp(\\mathbf{u}_o^\\top \\mathbf{v}_c)}{ \\sum_{i \\in \\mathcal{V}} \\exp(\\mathbf{u}_i^\\top \\mathbf{v}_c)},\n",
    "\\end{split}\n",
    "as\n",
    "\\begin{split}\n",
    "P(w_o \\mid w_c) = \\prod_{j=1}^{L(w_o)-1} \\sigma\\left( [\\![  n(w_o, j+1) = \\textrm{leftChild}(n(w_o, j)) ]\\!] \\cdot \\mathbf{u}_{n(w_o, j)}^\\top \\mathbf{v}_c\\right),\n",
    "\\end{split}\n",
    "where $\\sigma$ is the sigmoid function and $\\textrm{leftChild}(n)$ is the left child node of $n$: if $x$ is true, then $[\\![x]\\!] = 1$; otherwise, $[\\![x]\\!] = -1$.\n",
    "\n",
    "In the figure above, if we want to calculate the conditional probaility of generating word $w_3$ given word $w_c$, we need to calculate the dot products between the word vector $\\mathbf{v}_c$ of $w_c$ and non-leaf node vectors on the path (the bold line segments in the figure above) from the root to $w_3$, which is traversed left, right, and then left:\n",
    "\\begin{split}\n",
    "P(w_3 \\mid w_c) = \\sigma(\\mathbf{u}_{n(w_3, 1)}^\\top \\mathbf{v}_c) \\cdot \\sigma(-\\mathbf{u}_{n(w_3, 2)}^\\top \\mathbf{v}_c) \\cdot \\sigma(\\mathbf{u}_{n(w_3, 3)}^\\top \\mathbf{v}_c).\n",
    "\\end{split}\n",
    "\n",
    "Since $\\sigma(x) + \\sigma(-x) = 1$, it holds that the conditional probabilities of generating all the words in dictionary $\\mathcal{V}$ based on any word $w_c$ sum up to one:\n",
    "\\begin{split}\n",
    "\\sum_{w \\in \\mathcal{V}} P(w \\mid w_c) = 1.\n",
    "\\end{split}\n",
    "\n",
    "Since $L(w_o) - 1$ is the order of $\\mathcal{O}(\\log_2 |\\mathcal{V}|)$ due to the binary tree structure, when the dictionary size $|\\mathcal{V}|$ is large, the computational cost for each training step of the hierarchical softmax is significantly reduced compared with that without approximate training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.3. The Dataset for Pretraining Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import torch\n",
    "\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.3.1. Reading the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Penn Tree Bank (PTB) is a widely used dataset for language modeling. This corpus is sampled from Wall Street Journal articles. In the original format, each line of the text file represents a sentence of words that are separated by spaces. We will treat each word as a token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2l.DATA_HUB['ptb'] = (d2l.DATA_URL + 'ptb.zip',\n",
    "                       '319d85e578af0cdc590547f26231e4e31cdf1e42')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ptb():\n",
    "    '''Load the PTB dataset into a list of text lines'''\n",
    "    data_dir = d2l.download_extract('ptb')\n",
    "    # Read the training set\n",
    "    with open(os.path.join(data_dir, 'ptb.train.txt')) as f:\n",
    "        raw_text = f.read()\n",
    "\n",
    "    return [line.split() for line in raw_text.split('\\n')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# sentences: 42069\n"
     ]
    }
   ],
   "source": [
    "sentences = read_ptb()\n",
    "print(f'# sentences: {len(sentences)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to build a vocabulary for the corpus after reading the dataset. Any word that appears less than 10 times is replaced by the `<unk>` token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size: 6719\n"
     ]
    }
   ],
   "source": [
    "vocab = d2l.Vocab(sentences, min_freq=10)\n",
    "print(f'vocabulary size: {len(vocab)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.3.2. Subsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text data typically have high-frequency words such as “the”, “a”, and “in”: they may even occur billions of times in very large corpora. However, these words often co-occur with many different words in context windows, providing little useful signals.\n",
    "\n",
    "When training word embedding models, high-frequency words can be *subsampled*. Each indexed word $w_i$ in the dataset will be discarded with probability\n",
    "\\begin{split}\n",
    "P(w_i) = \\max\\left(1 - \\sqrt{\\frac{t}{f(w_i)}}, 0\\right),\n",
    "\\end{split}\n",
    "where $f(w_i)$ is the ratio of the number of words $w_i$ to the total number of words in the dataset, and the constant $t$ is a hyperparameter.\n",
    "\n",
    "When the relative frequency $f(w_i)>t$, the word $w_i$ can be discarded. The higher the relative frequency of the word, the greater the probability of being discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsample(sentences, vocab):\n",
    "    '''Subsample high-frequency words'''\n",
    "    # exclude unkown tokens '<unk>'\n",
    "    sentences = [\n",
    "        [token for token in line if vocab[token] != vocab.unk]\n",
    "        for line in sentences\n",
    "    ]\n",
    "    counter = collections.Counter(\n",
    "        [token for line in sentences for token in line]\n",
    "    )\n",
    "\n",
    "    num_tokens = sum(counter.values())\n",
    "\n",
    "    # return True if `token` is kept during subsampling\n",
    "    def keep(token):\n",
    "        t = 1e-4 # threshold for subsampling\n",
    "        f_w = counter[token] / num_tokens\n",
    "        prob = math.sqrt(t / f_w)\n",
    "        return random.uniform(0, 1) < prob\n",
    "    \n",
    "    return (\n",
    "        [[token for token in line if keep(token)] for line in sentences],\n",
    "        counter\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsampled, counter = subsample(sentences, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the histogram of the number of tokens per sentence before and after subsampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"262.190625pt\" height=\"185.484929pt\" viewBox=\"0 0 262.190625 185.484929\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2024-01-03T11:55:35.174131</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.7.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 185.484929 \n",
       "L 262.190625 185.484929 \n",
       "L 262.190625 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 59.690625 147.928679 \n",
       "L 254.990625 147.928679 \n",
       "L 254.990625 9.328679 \n",
       "L 59.690625 9.328679 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 68.567898 147.928679 \n",
       "L 75.814651 147.928679 \n",
       "L 75.814651 126.211667 \n",
       "L 68.567898 126.211667 \n",
       "z\n",
       "\" clip-path=\"url(#p8cecdc083d)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 86.684781 147.928679 \n",
       "L 93.931534 147.928679 \n",
       "L 93.931534 87.718057 \n",
       "L 86.684781 87.718057 \n",
       "z\n",
       "\" clip-path=\"url(#p8cecdc083d)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 104.801664 147.928679 \n",
       "L 112.048417 147.928679 \n",
       "L 112.048417 78.368513 \n",
       "L 104.801664 78.368513 \n",
       "z\n",
       "\" clip-path=\"url(#p8cecdc083d)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 122.918547 147.928679 \n",
       "L 130.1653 147.928679 \n",
       "L 130.1653 98.688845 \n",
       "L 122.918547 98.688845 \n",
       "z\n",
       "\" clip-path=\"url(#p8cecdc083d)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_7\">\n",
       "    <path d=\"M 141.03543 147.928679 \n",
       "L 148.282183 147.928679 \n",
       "L 148.282183 127.400215 \n",
       "L 141.03543 127.400215 \n",
       "z\n",
       "\" clip-path=\"url(#p8cecdc083d)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_8\">\n",
       "    <path d=\"M 159.152313 147.928679 \n",
       "L 166.399067 147.928679 \n",
       "L 166.399067 140.956231 \n",
       "L 159.152313 140.956231 \n",
       "z\n",
       "\" clip-path=\"url(#p8cecdc083d)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_9\">\n",
       "    <path d=\"M 177.269196 147.928679 \n",
       "L 184.51595 147.928679 \n",
       "L 184.51595 146.351252 \n",
       "L 177.269196 146.351252 \n",
       "z\n",
       "\" clip-path=\"url(#p8cecdc083d)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_10\">\n",
       "    <path d=\"M 195.38608 147.928679 \n",
       "L 202.632833 147.928679 \n",
       "L 202.632833 147.528845 \n",
       "L 195.38608 147.528845 \n",
       "z\n",
       "\" clip-path=\"url(#p8cecdc083d)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_11\">\n",
       "    <path d=\"M 213.502963 147.928679 \n",
       "L 220.749716 147.928679 \n",
       "L 220.749716 147.79175 \n",
       "L 213.502963 147.79175 \n",
       "z\n",
       "\" clip-path=\"url(#p8cecdc083d)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_12\">\n",
       "    <path d=\"M 231.619846 147.928679 \n",
       "L 238.866599 147.928679 \n",
       "L 238.866599 147.851999 \n",
       "L 231.619846 147.851999 \n",
       "z\n",
       "\" clip-path=\"url(#p8cecdc083d)\" style=\"fill: #1f77b4\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_13\">\n",
       "    <path d=\"M 75.814651 147.928679 \n",
       "L 83.061404 147.928679 \n",
       "L 83.061404 15.928679 \n",
       "L 75.814651 15.928679 \n",
       "z\n",
       "\" clip-path=\"url(#p8cecdc083d)\" style=\"fill: url(#h379c09ba72)\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_14\">\n",
       "    <path d=\"M 93.931534 147.928679 \n",
       "L 101.178287 147.928679 \n",
       "L 101.178287 59.828264 \n",
       "L 93.931534 59.828264 \n",
       "z\n",
       "\" clip-path=\"url(#p8cecdc083d)\" style=\"fill: url(#h379c09ba72)\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_15\">\n",
       "    <path d=\"M 112.048417 147.928679 \n",
       "L 119.29517 147.928679 \n",
       "L 119.29517 138.217642 \n",
       "L 112.048417 138.217642 \n",
       "z\n",
       "\" clip-path=\"url(#p8cecdc083d)\" style=\"fill: url(#h379c09ba72)\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_16\">\n",
       "    <path d=\"M 130.1653 147.928679 \n",
       "L 137.412054 147.928679 \n",
       "L 137.412054 147.359053 \n",
       "L 130.1653 147.359053 \n",
       "z\n",
       "\" clip-path=\"url(#p8cecdc083d)\" style=\"fill: url(#h379c09ba72)\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_17\">\n",
       "    <path d=\"M 148.282183 147.928679 \n",
       "L 155.528937 147.928679 \n",
       "L 155.528937 147.890339 \n",
       "L 148.282183 147.890339 \n",
       "z\n",
       "\" clip-path=\"url(#p8cecdc083d)\" style=\"fill: url(#h379c09ba72)\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_18\">\n",
       "    <path d=\"M 166.399067 147.928679 \n",
       "L 173.64582 147.928679 \n",
       "L 173.64582 147.928679 \n",
       "L 166.399067 147.928679 \n",
       "z\n",
       "\" clip-path=\"url(#p8cecdc083d)\" style=\"fill: url(#h379c09ba72)\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_19\">\n",
       "    <path d=\"M 184.51595 147.928679 \n",
       "L 191.762703 147.928679 \n",
       "L 191.762703 147.928679 \n",
       "L 184.51595 147.928679 \n",
       "z\n",
       "\" clip-path=\"url(#p8cecdc083d)\" style=\"fill: url(#h379c09ba72)\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_20\">\n",
       "    <path d=\"M 202.632833 147.928679 \n",
       "L 209.879586 147.928679 \n",
       "L 209.879586 147.928679 \n",
       "L 202.632833 147.928679 \n",
       "z\n",
       "\" clip-path=\"url(#p8cecdc083d)\" style=\"fill: url(#h379c09ba72)\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_21\">\n",
       "    <path d=\"M 220.749716 147.928679 \n",
       "L 227.996469 147.928679 \n",
       "L 227.996469 147.928679 \n",
       "L 220.749716 147.928679 \n",
       "z\n",
       "\" clip-path=\"url(#p8cecdc083d)\" style=\"fill: url(#h379c09ba72)\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_22\">\n",
       "    <path d=\"M 238.866599 147.928679 \n",
       "L 246.113352 147.928679 \n",
       "L 246.113352 147.928679 \n",
       "L 238.866599 147.928679 \n",
       "z\n",
       "\" clip-path=\"url(#p8cecdc083d)\" style=\"fill: url(#h379c09ba72)\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"m7b53ddae45\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m7b53ddae45\" x=\"66.756209\" y=\"147.928679\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(63.574959 162.527117) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m7b53ddae45\" x=\"110.943729\" y=\"147.928679\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 20 -->\n",
       "      <g transform=\"translate(104.581229 162.527117) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m7b53ddae45\" x=\"155.131249\" y=\"147.928679\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 40 -->\n",
       "      <g transform=\"translate(148.768749 162.527117) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m7b53ddae45\" x=\"199.318769\" y=\"147.928679\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 60 -->\n",
       "      <g transform=\"translate(192.956269 162.527117) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \n",
       "Q 1688 2584 1439 2293 \n",
       "Q 1191 2003 1191 1497 \n",
       "Q 1191 994 1439 701 \n",
       "Q 1688 409 2113 409 \n",
       "Q 2538 409 2786 701 \n",
       "Q 3034 994 3034 1497 \n",
       "Q 3034 2003 2786 2293 \n",
       "Q 2538 2584 2113 2584 \n",
       "z\n",
       "M 3366 4563 \n",
       "L 3366 3988 \n",
       "Q 3128 4100 2886 4159 \n",
       "Q 2644 4219 2406 4219 \n",
       "Q 1781 4219 1451 3797 \n",
       "Q 1122 3375 1075 2522 \n",
       "Q 1259 2794 1537 2939 \n",
       "Q 1816 3084 2150 3084 \n",
       "Q 2853 3084 3261 2657 \n",
       "Q 3669 2231 3669 1497 \n",
       "Q 3669 778 3244 343 \n",
       "Q 2819 -91 2113 -91 \n",
       "Q 1303 -91 875 529 \n",
       "Q 447 1150 447 2328 \n",
       "Q 447 3434 972 4092 \n",
       "Q 1497 4750 2381 4750 \n",
       "Q 2619 4750 2861 4703 \n",
       "Q 3103 4656 3366 4563 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-36\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m7b53ddae45\" x=\"243.506289\" y=\"147.928679\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 80 -->\n",
       "      <g transform=\"translate(237.143789 162.527117) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \n",
       "Q 1584 2216 1326 1975 \n",
       "Q 1069 1734 1069 1313 \n",
       "Q 1069 891 1326 650 \n",
       "Q 1584 409 2034 409 \n",
       "Q 2484 409 2743 651 \n",
       "Q 3003 894 3003 1313 \n",
       "Q 3003 1734 2745 1975 \n",
       "Q 2488 2216 2034 2216 \n",
       "z\n",
       "M 1403 2484 \n",
       "Q 997 2584 770 2862 \n",
       "Q 544 3141 544 3541 \n",
       "Q 544 4100 942 4425 \n",
       "Q 1341 4750 2034 4750 \n",
       "Q 2731 4750 3128 4425 \n",
       "Q 3525 4100 3525 3541 \n",
       "Q 3525 3141 3298 2862 \n",
       "Q 3072 2584 2669 2484 \n",
       "Q 3125 2378 3379 2068 \n",
       "Q 3634 1759 3634 1313 \n",
       "Q 3634 634 3220 271 \n",
       "Q 2806 -91 2034 -91 \n",
       "Q 1263 -91 848 271 \n",
       "Q 434 634 434 1313 \n",
       "Q 434 1759 690 2068 \n",
       "Q 947 2378 1403 2484 \n",
       "z\n",
       "M 1172 3481 \n",
       "Q 1172 3119 1398 2916 \n",
       "Q 1625 2713 2034 2713 \n",
       "Q 2441 2713 2670 2916 \n",
       "Q 2900 3119 2900 3481 \n",
       "Q 2900 3844 2670 4047 \n",
       "Q 2441 4250 2034 4250 \n",
       "Q 1625 4250 1398 4047 \n",
       "Q 1172 3844 1172 3481 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-38\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_6\">\n",
       "     <!-- # tokens per sentence -->\n",
       "     <g transform=\"translate(100.6125 176.205242) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-23\" d=\"M 3272 2816 \n",
       "L 2363 2816 \n",
       "L 2100 1772 \n",
       "L 3016 1772 \n",
       "L 3272 2816 \n",
       "z\n",
       "M 2803 4594 \n",
       "L 2478 3297 \n",
       "L 3391 3297 \n",
       "L 3719 4594 \n",
       "L 4219 4594 \n",
       "L 3897 3297 \n",
       "L 4872 3297 \n",
       "L 4872 2816 \n",
       "L 3775 2816 \n",
       "L 3519 1772 \n",
       "L 4513 1772 \n",
       "L 4513 1294 \n",
       "L 3397 1294 \n",
       "L 3072 0 \n",
       "L 2572 0 \n",
       "L 2894 1294 \n",
       "L 1978 1294 \n",
       "L 1656 0 \n",
       "L 1153 0 \n",
       "L 1478 1294 \n",
       "L 494 1294 \n",
       "L 494 1772 \n",
       "L 1594 1772 \n",
       "L 1856 2816 \n",
       "L 850 2816 \n",
       "L 850 3297 \n",
       "L 1978 3297 \n",
       "L 2297 4594 \n",
       "L 2803 4594 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \n",
       "L 1172 3500 \n",
       "L 2356 3500 \n",
       "L 2356 3053 \n",
       "L 1172 3053 \n",
       "L 1172 1153 \n",
       "Q 1172 725 1289 603 \n",
       "Q 1406 481 1766 481 \n",
       "L 2356 481 \n",
       "L 2356 0 \n",
       "L 1766 0 \n",
       "Q 1100 0 847 248 \n",
       "Q 594 497 594 1153 \n",
       "L 594 3053 \n",
       "L 172 3053 \n",
       "L 172 3500 \n",
       "L 594 3500 \n",
       "L 594 4494 \n",
       "L 1172 4494 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \n",
       "Q 1497 3097 1228 2736 \n",
       "Q 959 2375 959 1747 \n",
       "Q 959 1119 1226 758 \n",
       "Q 1494 397 1959 397 \n",
       "Q 2419 397 2687 759 \n",
       "Q 2956 1122 2956 1747 \n",
       "Q 2956 2369 2687 2733 \n",
       "Q 2419 3097 1959 3097 \n",
       "z\n",
       "M 1959 3584 \n",
       "Q 2709 3584 3137 3096 \n",
       "Q 3566 2609 3566 1747 \n",
       "Q 3566 888 3137 398 \n",
       "Q 2709 -91 1959 -91 \n",
       "Q 1206 -91 779 398 \n",
       "Q 353 888 353 1747 \n",
       "Q 353 2609 779 3096 \n",
       "Q 1206 3584 1959 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6b\" d=\"M 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 1991 \n",
       "L 2875 3500 \n",
       "L 3609 3500 \n",
       "L 1753 1863 \n",
       "L 3688 0 \n",
       "L 2938 0 \n",
       "L 1159 1709 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \n",
       "L 2834 2853 \n",
       "Q 2591 2978 2328 3040 \n",
       "Q 2066 3103 1784 3103 \n",
       "Q 1356 3103 1142 2972 \n",
       "Q 928 2841 928 2578 \n",
       "Q 928 2378 1081 2264 \n",
       "Q 1234 2150 1697 2047 \n",
       "L 1894 2003 \n",
       "Q 2506 1872 2764 1633 \n",
       "Q 3022 1394 3022 966 \n",
       "Q 3022 478 2636 193 \n",
       "Q 2250 -91 1575 -91 \n",
       "Q 1294 -91 989 -36 \n",
       "Q 684 19 347 128 \n",
       "L 347 722 \n",
       "Q 666 556 975 473 \n",
       "Q 1284 391 1588 391 \n",
       "Q 1994 391 2212 530 \n",
       "Q 2431 669 2431 922 \n",
       "Q 2431 1156 2273 1281 \n",
       "Q 2116 1406 1581 1522 \n",
       "L 1381 1569 \n",
       "Q 847 1681 609 1914 \n",
       "Q 372 2147 372 2553 \n",
       "Q 372 3047 722 3315 \n",
       "Q 1072 3584 1716 3584 \n",
       "Q 2034 3584 2315 3537 \n",
       "Q 2597 3491 2834 3397 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \n",
       "L 1159 -1331 \n",
       "L 581 -1331 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "z\n",
       "M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \n",
       "Q 2534 3019 2420 3045 \n",
       "Q 2306 3072 2169 3072 \n",
       "Q 1681 3072 1420 2755 \n",
       "Q 1159 2438 1159 1844 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1341 3275 1631 3429 \n",
       "Q 1922 3584 2338 3584 \n",
       "Q 2397 3584 2469 3576 \n",
       "Q 2541 3569 2628 3553 \n",
       "L 2631 2963 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \n",
       "L 3122 2828 \n",
       "Q 2878 2963 2633 3030 \n",
       "Q 2388 3097 2138 3097 \n",
       "Q 1578 3097 1268 2742 \n",
       "Q 959 2388 959 1747 \n",
       "Q 959 1106 1268 751 \n",
       "Q 1578 397 2138 397 \n",
       "Q 2388 397 2633 464 \n",
       "Q 2878 531 3122 666 \n",
       "L 3122 134 \n",
       "Q 2881 22 2623 -34 \n",
       "Q 2366 -91 2075 -91 \n",
       "Q 1284 -91 818 406 \n",
       "Q 353 903 353 1747 \n",
       "Q 353 2603 823 3093 \n",
       "Q 1294 3584 2113 3584 \n",
       "Q 2378 3584 2631 3529 \n",
       "Q 2884 3475 3122 3366 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-23\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"83.789062\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-74\" x=\"115.576172\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"154.785156\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6b\" x=\"215.966797\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"270.251953\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"331.775391\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"395.154297\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"447.253906\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"479.041016\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"542.517578\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"604.041016\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-20\" x=\"645.154297\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"676.941406\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"729.041016\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"790.564453\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-74\" x=\"853.943359\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"893.152344\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"954.675781\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"1018.054688\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"1073.035156\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <defs>\n",
       "       <path id=\"m1dd03d628a\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m1dd03d628a\" x=\"59.690625\" y=\"147.928679\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(46.328125 151.727898) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m1dd03d628a\" x=\"59.690625\" y=\"120.542787\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 5000 -->\n",
       "      <g transform=\"translate(27.240625 124.342006) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-35\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m1dd03d628a\" x=\"59.690625\" y=\"93.156895\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 10000 -->\n",
       "      <g transform=\"translate(20.878125 96.956114) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"254.492188\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m1dd03d628a\" x=\"59.690625\" y=\"65.771003\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 15000 -->\n",
       "      <g transform=\"translate(20.878125 69.570222) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"254.492188\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m1dd03d628a\" x=\"59.690625\" y=\"38.385111\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 20000 -->\n",
       "      <g transform=\"translate(20.878125 42.18433) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"254.492188\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m1dd03d628a\" x=\"59.690625\" y=\"10.999219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 25000 -->\n",
       "      <g transform=\"translate(20.878125 14.798437) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"254.492188\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_13\">\n",
       "     <!-- count -->\n",
       "     <g transform=\"translate(14.798438 92.734929) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-75\" d=\"M 544 1381 \n",
       "L 544 3500 \n",
       "L 1119 3500 \n",
       "L 1119 1403 \n",
       "Q 1119 906 1312 657 \n",
       "Q 1506 409 1894 409 \n",
       "Q 2359 409 2629 706 \n",
       "Q 2900 1003 2900 1516 \n",
       "L 2900 3500 \n",
       "L 3475 3500 \n",
       "L 3475 0 \n",
       "L 2900 0 \n",
       "L 2900 538 \n",
       "Q 2691 219 2414 64 \n",
       "Q 2138 -91 1772 -91 \n",
       "Q 1169 -91 856 284 \n",
       "Q 544 659 544 1381 \n",
       "z\n",
       "M 1991 3584 \n",
       "L 1991 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-63\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"54.980469\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-75\" x=\"116.162109\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"179.541016\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-74\" x=\"242.919922\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"patch_23\">\n",
       "    <path d=\"M 59.690625 147.928679 \n",
       "L 59.690625 9.328679 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_24\">\n",
       "    <path d=\"M 254.990625 147.928679 \n",
       "L 254.990625 9.328679 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_25\">\n",
       "    <path d=\"M 59.690625 147.928679 \n",
       "L 254.990625 147.928679 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_26\">\n",
       "    <path d=\"M 59.690625 9.328679 \n",
       "L 254.990625 9.328679 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"legend_1\">\n",
       "    <g id=\"patch_27\">\n",
       "     <path d=\"M 155.389063 46.684929 \n",
       "L 247.990625 46.684929 \n",
       "Q 249.990625 46.684929 249.990625 44.684929 \n",
       "L 249.990625 16.328679 \n",
       "Q 249.990625 14.328679 247.990625 14.328679 \n",
       "L 155.389063 14.328679 \n",
       "Q 153.389063 14.328679 153.389063 16.328679 \n",
       "L 153.389063 44.684929 \n",
       "Q 153.389063 46.684929 155.389063 46.684929 \n",
       "z\n",
       "\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n",
       "    </g>\n",
       "    <g id=\"patch_28\">\n",
       "     <path d=\"M 157.389063 25.927117 \n",
       "L 177.389063 25.927117 \n",
       "L 177.389063 18.927117 \n",
       "L 157.389063 18.927117 \n",
       "z\n",
       "\" style=\"fill: #1f77b4\"/>\n",
       "    </g>\n",
       "    <g id=\"text_14\">\n",
       "     <!-- origin -->\n",
       "     <g transform=\"translate(185.389063 25.927117) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \n",
       "L 1178 3500 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 3500 \n",
       "z\n",
       "M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 4134 \n",
       "L 603 4134 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-67\" d=\"M 2906 1791 \n",
       "Q 2906 2416 2648 2759 \n",
       "Q 2391 3103 1925 3103 \n",
       "Q 1463 3103 1205 2759 \n",
       "Q 947 2416 947 1791 \n",
       "Q 947 1169 1205 825 \n",
       "Q 1463 481 1925 481 \n",
       "Q 2391 481 2648 825 \n",
       "Q 2906 1169 2906 1791 \n",
       "z\n",
       "M 3481 434 \n",
       "Q 3481 -459 3084 -895 \n",
       "Q 2688 -1331 1869 -1331 \n",
       "Q 1566 -1331 1297 -1286 \n",
       "Q 1028 -1241 775 -1147 \n",
       "L 775 -588 \n",
       "Q 1028 -725 1275 -790 \n",
       "Q 1522 -856 1778 -856 \n",
       "Q 2344 -856 2625 -561 \n",
       "Q 2906 -266 2906 331 \n",
       "L 2906 616 \n",
       "Q 2728 306 2450 153 \n",
       "Q 2172 0 1784 0 \n",
       "Q 1141 0 747 490 \n",
       "Q 353 981 353 1791 \n",
       "Q 353 2603 747 3093 \n",
       "Q 1141 3584 1784 3584 \n",
       "Q 2172 3584 2450 3431 \n",
       "Q 2728 3278 2906 2969 \n",
       "L 2906 3500 \n",
       "L 3481 3500 \n",
       "L 3481 434 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"61.181641\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"102.294922\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-67\" x=\"130.078125\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"193.554688\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"221.337891\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"patch_29\">\n",
       "     <path d=\"M 157.389063 40.605242 \n",
       "L 177.389063 40.605242 \n",
       "L 177.389063 33.605242 \n",
       "L 157.389063 33.605242 \n",
       "z\n",
       "\" style=\"fill: url(#h379c09ba72)\"/>\n",
       "    </g>\n",
       "    <g id=\"text_15\">\n",
       "     <!-- subsampled -->\n",
       "     <g transform=\"translate(185.389063 40.605242) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-62\" d=\"M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "M 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2969 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \n",
       "Q 1497 1759 1228 1600 \n",
       "Q 959 1441 959 1056 \n",
       "Q 959 750 1161 570 \n",
       "Q 1363 391 1709 391 \n",
       "Q 2188 391 2477 730 \n",
       "Q 2766 1069 2766 1631 \n",
       "L 2766 1759 \n",
       "L 2194 1759 \n",
       "z\n",
       "M 3341 1997 \n",
       "L 3341 0 \n",
       "L 2766 0 \n",
       "L 2766 531 \n",
       "Q 2569 213 2275 61 \n",
       "Q 1981 -91 1556 -91 \n",
       "Q 1019 -91 701 211 \n",
       "Q 384 513 384 1019 \n",
       "Q 384 1609 779 1909 \n",
       "Q 1175 2209 1959 2209 \n",
       "L 2766 2209 \n",
       "L 2766 2266 \n",
       "Q 2766 2663 2505 2880 \n",
       "Q 2244 3097 1772 3097 \n",
       "Q 1472 3097 1187 3025 \n",
       "Q 903 2953 641 2809 \n",
       "L 641 3341 \n",
       "Q 956 3463 1253 3523 \n",
       "Q 1550 3584 1831 3584 \n",
       "Q 2591 3584 2966 3190 \n",
       "Q 3341 2797 3341 1997 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6d\" d=\"M 3328 2828 \n",
       "Q 3544 3216 3844 3400 \n",
       "Q 4144 3584 4550 3584 \n",
       "Q 5097 3584 5394 3201 \n",
       "Q 5691 2819 5691 2113 \n",
       "L 5691 0 \n",
       "L 5113 0 \n",
       "L 5113 2094 \n",
       "Q 5113 2597 4934 2840 \n",
       "Q 4756 3084 4391 3084 \n",
       "Q 3944 3084 3684 2787 \n",
       "Q 3425 2491 3425 1978 \n",
       "L 3425 0 \n",
       "L 2847 0 \n",
       "L 2847 2094 \n",
       "Q 2847 2600 2669 2842 \n",
       "Q 2491 3084 2119 3084 \n",
       "Q 1678 3084 1418 2786 \n",
       "Q 1159 2488 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1356 3278 1631 3431 \n",
       "Q 1906 3584 2284 3584 \n",
       "Q 2666 3584 2933 3390 \n",
       "Q 3200 3197 3328 2828 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-64\" d=\"M 2906 2969 \n",
       "L 2906 4863 \n",
       "L 3481 4863 \n",
       "L 3481 0 \n",
       "L 2906 0 \n",
       "L 2906 525 \n",
       "Q 2725 213 2448 61 \n",
       "Q 2172 -91 1784 -91 \n",
       "Q 1150 -91 751 415 \n",
       "Q 353 922 353 1747 \n",
       "Q 353 2572 751 3078 \n",
       "Q 1150 3584 1784 3584 \n",
       "Q 2172 3584 2448 3432 \n",
       "Q 2725 3281 2906 2969 \n",
       "z\n",
       "M 947 1747 \n",
       "Q 947 1113 1208 752 \n",
       "Q 1469 391 1925 391 \n",
       "Q 2381 391 2643 752 \n",
       "Q 2906 1113 2906 1747 \n",
       "Q 2906 2381 2643 2742 \n",
       "Q 2381 3103 1925 3103 \n",
       "Q 1469 3103 1208 2742 \n",
       "Q 947 2381 947 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-73\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-75\" x=\"52.099609\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-62\" x=\"115.478516\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"178.955078\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"231.054688\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6d\" x=\"292.333984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"389.746094\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"453.222656\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"481.005859\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-64\" x=\"542.529297\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p8cecdc083d\">\n",
       "   <rect x=\"59.690625\" y=\"9.328679\" width=\"195.3\" height=\"138.6\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       " <defs>\n",
       "  <pattern id=\"h379c09ba72\" patternUnits=\"userSpaceOnUse\" x=\"0\" y=\"0\" width=\"72\" height=\"72\">\n",
       "   <rect x=\"0\" y=\"0\" width=\"73\" height=\"73\" fill=\"#ff7f0e\"/>\n",
       "   <path d=\"M -36 36 \n",
       "L 36 -36 \n",
       "M -24 48 \n",
       "L 48 -24 \n",
       "M -12 60 \n",
       "L 60 -12 \n",
       "M 0 72 \n",
       "L 72 0 \n",
       "M 12 84 \n",
       "L 84 12 \n",
       "M 24 96 \n",
       "L 96 24 \n",
       "M 36 108 \n",
       "L 108 36 \n",
       "\" style=\"fill: #000000; stroke: #000000; stroke-width: 1.0; stroke-linecap: butt; stroke-linejoin: miter\"/>\n",
       "  </pattern>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 350x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "d2l.show_list_len_pair_hist(\n",
    "    ['origin', 'subsampled'],\n",
    "    '# tokens per sentence',\n",
    "    'count',\n",
    "    sentences,\n",
    "    subsampled\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_counts(token):\n",
    "    return (\n",
    "        f'# of \"{token}\": '\n",
    "        f'before={sum([l.count(token) for l in sentences])}, '\n",
    "        f'after={sum([l.count(token) for l in subsampled])}'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# of \"the\": before=50770, after=1971'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_counts('the')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For individual tokens, the sampling rate of the high-frequency words \"the\" is less than 1/20.\n",
    "\n",
    "On the other hand, low-frequency words \"join\" are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# of \"join\": before=45, after=45'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_counts('join')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After subsampling, we map tokens to their indices for the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], [6697, 3228, 710, 1773], [3922, 1922, 4743]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [vocab[line] for line in subsampled]\n",
    "corpus[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These indices correspond to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], ['years', 'join', 'board', 'director'], ['n.v.', 'dutch', 'publishing']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subsampled[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.3.3. Extracting Center Words and Context Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `get_centers_and_contexts` function extracts all the center words and their context words from `corpus`. It uniformly samples an integer between 1 and `max_window_size` at random as the context window size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_centers_and_contexts(corpus, max_window_size):\n",
    "    '''Return center words and context words in skip-gram'''\n",
    "    centers = []\n",
    "    contexts = []\n",
    "\n",
    "    for line in corpus:\n",
    "        # To form a \"center word--context word\" pair,\n",
    "        # each sentence needs to have at least 2 words\n",
    "        if len(line) < 2:\n",
    "            continue\n",
    "\n",
    "        # add line to centers\n",
    "        centers += line\n",
    "        for i in range(len(line)): # context window centered at `i`\n",
    "            window_size = random.randint(1, max_window_size)\n",
    "            indices = list(range(\n",
    "                max(0, i - window_size), # left limit\n",
    "                min(len(line), i + 1 + window_size), # right limit\n",
    "            ))\n",
    "            # exclude the center word from the context words\n",
    "            indices.remove(i)\n",
    "            # add remaining words to contexts\n",
    "            contexts.append([line[idx] for idx in indices])\n",
    "\n",
    "    return centers, contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create an artificial dataset containing two sentences of 7 and 3 words, respectively. Let the maximum context window size be 2 nad print out all the center words and their context words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset:  [[0, 1, 2, 3, 4, 5, 6], [7, 8, 9]]\n",
      "center 0 has contexts [1, 2]\n",
      "center 1 has contexts [0, 2]\n",
      "center 2 has contexts [1, 3]\n",
      "center 3 has contexts [1, 2, 4, 5]\n",
      "center 4 has contexts [3, 5]\n",
      "center 5 has contexts [4, 6]\n",
      "center 6 has contexts [4, 5]\n",
      "center 7 has contexts [8]\n",
      "center 8 has contexts [7, 9]\n",
      "center 9 has contexts [8]\n"
     ]
    }
   ],
   "source": [
    "tiny_dataset = [\n",
    "    list(range(7)), # a sentence with 7 tokens\n",
    "    list(range(7, 10)) # another sentence with 3 tokens\n",
    "]\n",
    "max_window_size = 2\n",
    "\n",
    "print('dataset: ', tiny_dataset)\n",
    "centers, contexts = get_centers_and_contexts(tiny_dataset, max_window_size)\n",
    "for center, context in zip(centers, contexts):\n",
    "    print('center', center, 'has contexts', context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is randomness in the sampling of context windows. Hence, the output of the following code may vary.\n",
    "\n",
    "When training on the PTB dataset, we set the maximum context window size to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# center-context pairs: 1500707\n"
     ]
    }
   ],
   "source": [
    "max_window_size = 5\n",
    "all_centers, all_contexts = get_centers_and_contexts(corpus, max_window_size)\n",
    "print(f'# center-context pairs: {sum([len(contexts) for contexts in all_contexts])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.3.4. Negative Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use negative sampling for approximate training.\n",
    "\n",
    "To sample noise words according to a predefined distribution, we define the `RandomGenerator` class, where the (possibly unnormalized) sampling distribution is passed via the argument `sampling_weights`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomGenerator:\n",
    "    '''Randomly draw among {1,..., n} according to n sampling weights'''\n",
    "    def __init__(self, sampling_weights):\n",
    "        # exclude 0 for convenience\n",
    "        self.population = list(range(1, len(sampling_weights) + 1))\n",
    "        self.sampling_weights = sampling_weights\n",
    "        self.candidates = []\n",
    "        self.i = 0\n",
    "\n",
    "    def draw(self):\n",
    "        if self.i == len(self.candidates):\n",
    "            # cache `k` random sampling results\n",
    "            self.candidates = random.choices(\n",
    "                self.population,\n",
    "                self.sampling_weights,\n",
    "                k=10000\n",
    "            )\n",
    "            self.i = 0\n",
    "        \n",
    "        self.i += 1\n",
    "        return self.candidates[self.i - 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can draw 10 random variables $X$ among indices 1, 2, and 3 with sampling probability\n",
    "\\begin{split}\n",
    "P(X=1)=2/9, P(X=2)=3/9, \\textrm{and } P(X=3)=4/9.\n",
    "\\end{split}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1, 3, 3, 2, 3, 3, 2, 3, 1]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampling_weights = [2, 3, 4] # 3 tokens with weights 2, 3 and 4\n",
    "generator = RandomGenerator(sampling_weights)\n",
    "[generator.draw() for _ in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a pair of center word and context word, we randomly sample `K` (5 in the experiment) noise words. In the word2vec paper, the sampling probability $P(w)$ of a noise word $w$ is set to its relatie frequency in the dictionary raised to the power of 0.75."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_negatives(all_contexts, vocab, counter, K):\n",
    "    '''Return noise words in negative sampling'''\n",
    "    # Sampling weights for words with indices 1, 2, ...\n",
    "    # (index 0 is the excluded unknown token) in the vocabulary\n",
    "    sampling_weights = [counter[vocab.to_tokens(i)]**0.75 for i in range(1, len(vocab))]\n",
    "\n",
    "    all_negatives = []\n",
    "    generator = RandomGenerator(sampling_weights)\n",
    "\n",
    "    for contexts in all_contexts:\n",
    "        negatives = []\n",
    "        while len(negatives) < len(contexts) * K:\n",
    "            neg = generator.draw()\n",
    "            # noise words cannot be context words\n",
    "            if neg not in contexts:\n",
    "                negatives.append(neg)\n",
    "\n",
    "        all_negatives.append(negatives)\n",
    "\n",
    "    return all_negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_negatives = get_negatives(all_contexts, vocab, counter, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.3.5. Loading Training Examples in Minibatches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all the center words together with their context words and sampled noise words are extracted, they will be transformed into minibatches of examples that can be iteratively loaded during training.\n",
    "\n",
    "In a minibatch, the $i$-th example includes a center word and its $n_i$ context words and $m_i$ noise words. Due to varying context window sizes, $n_i + m_i$ varies for different $i$. Thus, for each example in a minibatch, we concatenate its context words and noise words in the `contexts_negatives` variable, and padd zeros until the concatenation length reaches the maximum context window size $\\max_i n_i + m_i$ (`max_len`) in the minibatch.\n",
    "\n",
    "To exclude paddings in the calculation of the loss, we define a mask variable `masks`. There is a one-to-one correspondence between elements in `masks` and elements in `contexts_negatives`, where zeros (otherwise ones) in `masks` correspond to paddings (otherwise non-paddings) in `contexts_negatives`.\n",
    "\n",
    "To distinguish between positive and negative examples in `contexts_negatives`, we separte context words from noise words via a `labels` variable. Similar to `masks`, there is also a one-to-one correspondence between elements in `labels` and elements in `contexts_negatives`, where ones (otherwise zeros) in `labels` correspond to context words (positive examples) in `contexts_negatives`.\n",
    "\n",
    "In the `batchify` function below, its input `data` is a list with length equal to the batch size, where each element is an example consisting of the center word `center`, its context words `context`, and its noise words `negatives`. This function returns a minibatch that can be loaded for calcualtions during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data):\n",
    "    '''Return a minibatch of examples for skip-gram with negative sampling'''\n",
    "    # data: (batch size, centers, contexts, negatives)\n",
    "    max_len = max(len(c) + len(n) for _, c, n in data)\n",
    "    # minibatch of center words\n",
    "    centers = []\n",
    "    # minibatch of context words and noise words associated with the center word\n",
    "    contexts_negatives = []\n",
    "    # mask for non-padding entries\n",
    "    masks = []\n",
    "    # mask for context words and noise words\n",
    "    labels = []\n",
    "\n",
    "    for center, context, negative in data:\n",
    "        # take one example\n",
    "        cur_len = len(context) + len(negative)\n",
    "        # add center word\n",
    "        centers += [center]\n",
    "        # add context words, noise words, and padding associated with the center word\n",
    "        contexts_negatives += [context + negative + [0] * (max_len - cur_len)]\n",
    "        # add mask for padding\n",
    "        masks += [[1] * cur_len + [0] * (max_len - cur_len)]\n",
    "        # add mask for context words\n",
    "        labels += [[1] * len(context) + [0] * (max_len - len(context))]\n",
    "\n",
    "    return (\n",
    "        torch.tensor(centers).reshape((-1, 1)), # (batch size, 1) for center words\n",
    "        torch.tensor(contexts_negatives), # (batch size, max_len) for context words and noise words\n",
    "        torch.tensor(masks), # (batch size, max_len) for mask\n",
    "        torch.tensor(labels) # (batch size, max_len) for mask\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "centers = tensor([[1],\n",
      "        [4],\n",
      "        [1]])\n",
      "contexts_negatives = tensor([[2, 2, 3, 3, 3, 3, 0, 0],\n",
      "        [2, 2, 2, 3, 3, 0, 0, 0],\n",
      "        [3, 3, 3, 1, 4, 4, 2, 2]])\n",
      "masks = tensor([[1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "labels = tensor([[1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "# test a minibatch of three examples\n",
    "X1 = (1, # center word\n",
    "      [2, 2], # context words\n",
    "      [3, 3, 3, 3]) # noise words\n",
    "X2 = (4, # center word\n",
    "      [2, 2, 2], # context words\n",
    "      [3, 3]) # noise words\n",
    "X3 = (1, # center word\n",
    "      [3, 3, 3, 1], # context words\n",
    "      [4, 4, 2, 2]) # noise words\n",
    "\n",
    "examples = [X1, X2, X3]\n",
    "batch = batchify(examples)\n",
    "\n",
    "names = ['centers', 'contexts_negatives', 'masks', 'labels']\n",
    "for name, data in zip(names, batch):\n",
    "    print(name, '=', data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.3.6. Putting It All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataset class\n",
    "class PTBDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, centers, contexts, negatives):\n",
    "        # centers, contexts, and negatives are of equal length\n",
    "        assert len(centers) == len(contexts) == len(negatives)\n",
    "\n",
    "        self.centers = centers\n",
    "        self.contexts = contexts\n",
    "        self.negatives = negatives\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (\n",
    "            self.centers[index], # center word\n",
    "            self.contexts[index], # positive context words\n",
    "            self.negatives[index], # negative noise words\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.centers)\n",
    "    \n",
    "    \n",
    "def load_data_ptb(batch_size, max_window_size, num_noise_words):\n",
    "    '''Download the PTB dataset and then load it into memory'''\n",
    "    num_workers = d2l.get_dataloader_workers()\n",
    "\n",
    "    sentences = read_ptb()\n",
    "    vocab = d2l.Vocab(sentences, min_freq=10)\n",
    "\n",
    "    subsampled, counter = subsample(sentences, vocab)\n",
    "    corpus = [vocab[line] for line in subsampled]\n",
    "    all_centers, all_contexts = get_centers_and_contexts(corpus, max_window_size)\n",
    "    all_negatives = get_negatives(all_contexts, vocab, counter, num_noise_words)\n",
    "        \n",
    "    # intialize a dataset instance\n",
    "    dataset = PTBDataset(all_centers, all_contexts, all_negatives)\n",
    "    # create a dataloader\n",
    "    data_iter = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=batchify, # use batchify function to collate examples\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return data_iter, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "max_window_size = 5\n",
    "num_noise_words = 5\n",
    "\n",
    "data_iter, vocab = load_data_ptb(batch_size, max_window_size, num_noise_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the first minibatch\n",
    "for batch in data_iter:\n",
    "    for name, data in zip(names, batch):\n",
    "        print(name, 'shape:', data.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.4. Pretraining word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement the skip-gram model and then pretrain word2vec using negative sampling on the PTB dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we need to obtain the data iterator and the vocabulary of the PTB dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "max_window_size = 5\n",
    "num_noise_words = 5\n",
    "\n",
    "data_iter, vocab = load_data_ptb(batch_size, max_window_size, num_noise_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.4.1. The Skip-Gram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15.4.1.1. Embedding Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from Chapter 10 that an embedding layer maps a token's index to its feature vector.\n",
    "\n",
    "The weight of this layer is a matrix whose number of rows equals to the dictionary size (`input_dim`) and the number of columns equals to the vector dimension for each token (`output_dim`). After a word embedding model is trained, the weight of the embedding layer is what we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter embedding_weight (torch.Size([20, 4]), dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "input_dim = 20 # vocabulary size\n",
    "output_dim = 4 # vector dimension\n",
    "\n",
    "embed = nn.Embedding(num_embeddings=input_dim,\n",
    "                     embedding_dim=output_dim)\n",
    "print(f'Parameter embedding_weight ({embed.weight.shape}, '\n",
    "      f'dtype={embed.weight.dtype})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input of an embedding layer is the index of a token (word).\n",
    "\n",
    "For any token index $i$, its vector representation can be obtained from the $i$-th row of the weight matrix in the embedding layer. Since the vector dimension (`output_dim`) is set to 4, the embedding layer returns vectors with shape (2, 3, 4) for a minibatch of 2 examples, each consisting of 3 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.4735, -1.2734,  1.3280,  1.4527],\n",
       "          [ 0.1065, -0.8098, -1.5031,  0.3291],\n",
       "          [ 0.4750, -0.7327, -0.5792,  0.3721]],\n",
       " \n",
       "         [[ 1.2788, -0.3188, -1.5614,  0.7817],\n",
       "          [-1.6512, -1.8211,  1.2905, -0.8847],\n",
       "          [-1.5387,  1.2123, -1.1465, -0.9661]]], grad_fn=<EmbeddingBackward0>),\n",
       " torch.Size([2, 3, 4]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor(\n",
    "    [[1, 2, 3],\n",
    "     [4, 5, 6]],\n",
    ")\n",
    "\n",
    "embed(X), embed(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15.4.1.2. Defining the Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the forward propagation, the input of the skip-gram model includes \n",
    "* the center word indices `center` of shape (batch size, 1), and\n",
    "* the concatenated context and noise word indices `contexts_and_negatives` of shape (batch size, `max_len`).\n",
    "\n",
    "These two variables are transformed from the token indices into vectors via the embedding layer, and then their batch matrix multiplication returns an output of shape (batch size, 1, `max_len`), where each element is the dot product of a center word vector and a context or noise word vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skip_gram(center, contexts_and_negatives, embed_v, embed_u):\n",
    "    '''Compute the skip-gram objective function for one center word'''\n",
    "    v = embed_v(center) # center word vector\n",
    "    u = embed_u(contexts_and_negatives) # context and noise word vectors\n",
    "    pred = torch.bmm(v, u.permute(0, 2, 1)) # bmm: batch matrix multiplication\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 4])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skip_gram(\n",
    "    center=torch.ones((2, 1), dtype=torch.long), # batch size: 2\n",
    "    contexts_and_negatives=torch.ones((2, 4), dtype=torch.long), # 4 combined context and noise words\n",
    "    embed_v=embed, # embedding for center words\n",
    "    embed_u=embed # embedding for context and noise words\n",
    ").shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.4.2. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15.4.2.1. Binary Cross-Entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the binary cross-entropy loss to train the skip-gram model with negative sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigmoidBCELoss(nn.Module):\n",
    "    '''Binary cross-entropy loss with masking'''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, inputs, target, mask=None):\n",
    "        out = nn.functional.binary_cross_entropy_with_logits(\n",
    "            inputs, target, weight=mask, reduction='none'\n",
    "        )\n",
    "        return out.mean(dim=1)\n",
    "    \n",
    "loss = SigmoidBCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that\n",
    "* `mask` is a variable for excluding paddings in the loss calculation, and\n",
    "* `label` is a variable for distinguishing between context words and noise words.\n",
    "\n",
    "We can calculate the binary cross-entorpy loss for the given variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9352, 1.8462])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = torch.tensor(\n",
    "    [[1.1, -2.2, 3.3, -4.4]] * 2\n",
    ")\n",
    "\n",
    "mask = torch.tensor(\n",
    "    [[1, 1, 1, 1],\n",
    "     [1, 1, 0, 0]]\n",
    ")\n",
    "\n",
    "label = torch.tensor(\n",
    "    [[1., 0., 0., 0.],\n",
    "     [0., 1., 0., 0.]]\n",
    ")\n",
    "\n",
    "loss(pred, label, mask) * mask.shape[1] / mask.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the sigmoid function in the binary cross-entropy loss to calculate the above results. We can consider the two outputs as two normalized losses that are average over non-masked predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9352\n",
      "1.8462\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return -math.log(1 / (1 + math.exp(-x)))\n",
    "\n",
    "print(f'{(sigmoid(1.1) + sigmoid(2.2) + sigmoid(-3.3) + sigmoid(4.4)) / 4:.4f}')\n",
    "print(f'{(sigmoid(-1.1) + sigmoid(-2.2)) / 2:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15.4.2.2. Initializing Model Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need two embedding layers for all the words in the vocabulary: one for the center words and the other for the context words and noise words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 100\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Embedding(num_embeddings=len(vocab),\n",
    "                 embedding_dim=embed_size), # embedding for center words\n",
    "    nn.Embedding(num_embeddings=len(vocab),\n",
    "                    embedding_dim=embed_size) # embedding for context words\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15.4.2.3. Defning the Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, data_iter, lr, num_epochs, device=d2l.try_gpu()):\n",
    "    # initialize embedding parameters\n",
    "    def init_weights(module):\n",
    "        if type(module) == nn.Embedding:\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "\n",
    "    net.apply(init_weights)\n",
    "    net = net.to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    # create an animator\n",
    "    animator = d2l.Animator(xlabel='epoch', ylabel='loss',\n",
    "                            xlim=[1, num_epochs])\n",
    "    \n",
    "    # sum of normalized loss, number of normalized losses\n",
    "    metric = d2l.Accumulator(2) # loss_sum, num_examples\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        timer = d2l.Timer()\n",
    "        num_batches = len(data_iter)\n",
    "\n",
    "        for i, batch in enumerate(data_iter):\n",
    "            optimizer.zero_grad()\n",
    "            # center words, context+noise words, masks, labels\n",
    "            center, context_negative, mask, label = [\n",
    "                data.to(device) for data in batch\n",
    "            ]\n",
    "\n",
    "            # forward pass\n",
    "            pred = skip_gram(center,\n",
    "                             context_negative,\n",
    "                             embed_v=net[0],\n",
    "                             embed_u=net[1])\n",
    "            # compute loss\n",
    "            l = (loss(pred.reshape(label.shape).float(),\n",
    "                      label.float(),\n",
    "                      mask)\n",
    "                      / mask.sum(axis=1) * mask.shape[1])\n",
    "            \n",
    "            # backward pass\n",
    "            l.sum().backward()\n",
    "            # update parameters\n",
    "            optimizer.step()\n",
    "            # update metric\n",
    "            metric.add(l.sum(), l.numel())\n",
    "\n",
    "            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n",
    "                animator.add(epoch + (i + 1) / num_batches,\n",
    "                             (metric[0] / metric[1],))\n",
    "    \n",
    "    print(f'loss {metric[0] / metric[1]:.3f}, '\n",
    "          f'{metric[1] / timer.stop():.1f} tokens/sec on {str(device)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a skip-gram model using negative sampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.002\n",
    "num_epochs = 5\n",
    "\n",
    "train(net, data_iter, lr, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.4.3. Applying Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the word2vec model, we can use the cosine similarity of word vectors from the trained model to find words from the dictionary that are most semantically similar to an input word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_tokens(query_token, k, embed):\n",
    "    W = embed.weight.data\n",
    "    x = W[vocab[query_token]]\n",
    "\n",
    "    # compute the cosine similarity. Add 1e-9 for numerical stability\n",
    "    cos = torch.mv(W, x) / torch.sqrt(torch.sum(W * W, dim=1) *\n",
    "                                      torch.sum(x * x) + 1e-9)\n",
    "    topk = torch.topk(cos, k=k+1)[1].cpu().numpy().astype('int32')\n",
    "\n",
    "    for i in topk[1:]: # remove the input words\n",
    "        print(f'cosine sim={float(cos[i]):.3f}: {vocab.to_tokens(i)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_similar_tokens('chip', 3, net[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.5. Word Embedding with Global Vectors (GloVe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word-word co-occurrences within context windows may carry rich semantic information. For example, in a large corpus word “solid” is more likely to co-occur with “ice” than “steam”, but word “gas” probably co-occurs with “steam” more frequently than “ice”.\n",
    "\n",
    "The global corpus statistics of such co-occurrences can be pre-computed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.5.1. Skip-Gram with Global Corpus Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Denoting by $q_{ij}$ the conditional probability $P(w_j \\mid w_i)$ of word $w_j$ given word $w_i$ in the skip-gram model, we have\n",
    "\\begin{split}\n",
    "P(w_j \\mid w_i) = q_{ij}=\\frac{\\exp(\\mathbf{u}_j^\\top \\mathbf{v}_i)}{ \\sum_{k \\in \\mathcal{V}} \\exp(\\mathbf{u}_k^\\top \\mathbf{v}_i)},\n",
    "\\end{split}\n",
    "where for any index $i$, vectors $\\mathbf{v}_i$ and $\\mathbf{u}_i$ represent word $w_i$ as the center word and the context word, respectively, and $\\mathcal{V}={0, 1, \\ldots, |\\mathcal{V}|-1}$ is the index set of the vocabulary.\n",
    "\n",
    "Consider the word $w_i$ that may occur multiple times in the corpus. In the entire corpus, all the context words wherever $w_i$ is taken as their center word form a *multiset* $\\mathcal{C}_i$ of word indcies that *allows for multiple instances of the same element*.\n",
    "\n",
    "For any element, its number of instances is called its *multiplicity*.\n",
    "\n",
    "For example, suppose that the word $w_i$ occurs twice in the corpus and indices of the context words that take $w_i$ as their center word in the two context windows are $k, j, m, k$ and $k, l, k, j$. Therefore, the multiset $\\mathcal{C}_i$ of word indices is $\\mathcal{C}_i = \\{j, j, k, k, k, k, l, m\\}$, where the multiplicity of elements $j, k, l, m$ are 2, 4, 1, and 1, respectively.\n",
    "\n",
    "Suppose that the multiplicity of element $j$ in this multiset $\\mathcal{C}_i$ is denoted as $x_{ij}$, this is the global co-occurrence count of word $w_j$ (as the context word) and word $w_i$ (as the center word) in the same context window in the entire corpus.\n",
    "\n",
    "Using such global corpus statistics, the loss function of the skip-gram model is rewritten as\n",
    "\\begin{split}\n",
    "-\\sum_{i\\in\\mathcal{V}}\\sum_{j\\in\\mathcal{V}} x_{ij} \\log\\,q_{ij}.\n",
    "\\end{split}\n",
    "\n",
    "Suppose that the number of all the context words is denoted as $x_i$ in the context windows where $w_i$ occurs as their center word, which is equivalent to the sum of the multiplicities of all the elements in the multiset $\\mathcal{C}_i$, i.e., $x_i = | \\mathcal{C}_i |$. Let $p_{ij}$ be the conditional probability $x_{ij}/x_i$ for generating context word $w_j$ given center word $w_i$, then the loss function of the skip-gram model can be further rewritten as\n",
    "\\begin{split}\n",
    "-\\sum_{i\\in\\mathcal{V}}\\sum_{j\\in\\mathcal{V}} x_{ij} \\log\\,q_{ij} =-\\sum_{i\\in\\mathcal{V}} x_i \\sum_{j\\in\\mathcal{V}} p_{ij} \\log\\,q_{ij}.\n",
    "\\end{split}\n",
    "\n",
    "We can see that $-\\sum_{j\\in\\mathcal{V}} p_{ij} \\log\\,q_{ij}$ calculates the cross-entorpy of the conditional distribution $p_{ij}$ of global corpus statistics and the conditional distribution $q_{ij}$ of the model predictions. This loss is also weighted by $x_i$. Minimizing this loss function will allow the predicted conditional distribution to get close to the conditional distribution from the global corpus statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.5.2. The GloVe Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *global vectors for word representation (GloVe)* model makes 3 changes to the skip-gram model based on squared loss:\n",
    "1. Use variables $p'_{ij}=x_{ij}$ and $q'_{ij}=\\exp(\\mathbf{u}_j^\\top \\mathbf{v}_i)$ that are not probability distributions and take the logarithm of both, so the squared loss term is\n",
    "\\begin{split}\n",
    "\\left(\\log\\,p'_{ij} - \\log\\,q'_{ij}\\right)^2 = \\left(\\mathbf{u}_j^\\top \\mathbf{v}_i - \\log\\,x_{ij}\\right)^2.\n",
    "\\end{split}\n",
    "2. Add two scalar model parameters for each word $w_i$:\n",
    "    * the center word bias $b_i$, and\n",
    "    * the context word bias $c_i$.\n",
    "3. Replace the weight of each loss term with the weight function $h(x_{ij})$, where $h(x)$ is increasing in the interval of [0, 1].\n",
    "\n",
    "Putting all things together, training GloVe is to minimize the following loss function:\n",
    "\\begin{split}\n",
    "\\sum_{i\\in\\mathcal{V}} \\sum_{j\\in\\mathcal{V}} h(x_{ij}) \\left(\\mathbf{u}_j^\\top \\mathbf{v}_i + b_i + c_j - \\log\\,x_{ij}\\right)^2.\n",
    "\\end{split}\n",
    "\n",
    "For the weight function $h(x)$, a sugguested choice is\n",
    "\\begin{split}\n",
    "h(x) = (x/c) ^\\alpha \\textrm{ if } x < c \\textrm{ else } 1,\n",
    "\\end{split}\n",
    "where $\\alpha = 0.75$ and $c = 100$.\n",
    "\n",
    "In this case, since $h(0)=0$, the squared loss term for any $x_{ij}=0$ can be omitted for computational efficiency. When using minibatch SGD, at each iteration we randomly sample a minibatch of *non-zero* $x_{ij}$ to calculate gradients and update the model parameters. These non-zero $x_{ij}$ are precomputed global corpus statistics; thus, the model is called GloVe for *Global Vectors*.\n",
    "\n",
    "If word $w_i$ appears in the context window of word $w_i$, then *vice versa*. Therefore, $x_{ij}=x_{ji}$. Unlike word2vec that fits the asymmetric condtional probability $p_{ij}$, GloVe fits the symmetric $\\log x_{ij}$. Therefore, the center word vector and the context word vector of any word are mathematically equivalent in the GloVe model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.5.3. Interpreting GloVe from the Ratio of Co-occurrence Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $p_{ij} \\stackrel{\\textrm{def}}{=} P(w_j \\mid w_i)$ be the conditional probability of generating context word $w_j$ given $w_i$ as the center word in the corpus.\n",
    "\n",
    "The table bellow lists several co-occurrence probabilities given words \"ice\" and \"steam\" and their ratios based on statistics from a large corpus.\n",
    "\n",
    "|          | $w_k$ = \"solid\" | $w_k$ = \"gas\" | $w_k$ = \"water\" | $w_k$ = \"fashion\" |\n",
    "|:--------:|:---------------:|:-------------:|:---------------:|:-----------------:|\n",
    "| $p_1=P(w_k \\mid \\textrm{\"ice\"})$ | 0.00019 | 0.000066 | 0.003 | 0.000017         |\n",
    "| $p_2=P(w_k \\mid \\textrm{\"steam\"})$ | 0.000022 | 0.00078 | 0.0022 | 0.000018      |\n",
    "| $p_1/p_2$ | 8.9 | 0.085 | 1.36 | 0.96 |\n",
    "\n",
    "Using this table as an example, we can see that\n",
    "* For a word $w_k$ that is related to \"ice\" but unrelated to \"steam\", such as $w_k$ = \"solid\", we expect a larger ratio of co-occurrence probabilities $p_1/p_2 = 8.9$.\n",
    "* For a word $w_k$ that is lreated to \"steam\" but unrelated to \"ice\", such as $w_k$ = \"gas\", we expect a smaller ratio of co-occurrence probabilities $p_1/p_2 = 0.085$.\n",
    "* For a word $w_k$ that is related to both \"ice\" and \"steam\", such as $w_k$ = \"water\", we expect a ratio of co-occurrence probabilities $p_1/p_2 = 1.36$ that is close to 1.\n",
    "* For a word $w_k$ that is unrelated to both \"ice\" and \"steam\", such as $w_k$ = \"fashion\", we expect a ratio of co-occurrence probabilities $p_1/p_2 = 0.96$ that is close to 1.\n",
    "\n",
    "*The ratio of co-occurrence probabilities can intuitively express the relationship between words.* Thus, for the ratio of co-occurrence probabilities $p_{ij}/p_{ik}$, with $w_i$ being the center word and $w_j$ and $w_k$ being the context words, we want to fit this ratio using some function $f$:\n",
    "\\begin{split}\n",
    "f(\\mathbf{u}_j, \\mathbf{u}_k, {\\mathbf{v}}_i) \\approx \\frac{p_{ij}}{p_{ik}}.\n",
    "\\end{split}\n",
    "\n",
    "Since the ratio of co-occurrence probabilities is a scalar, we require that $f$ be a scalar function, such as $f(\\mathbf{u}_j, \\mathbf{u}_k, {\\mathbf{v}}_i) = f\\left((\\mathbf{u}_j - \\mathbf{u}_k)^\\top {\\mathbf{v}}_i\\right)$. Switching word indices $j$ and $k$, it must hold that $f(x)f(-x)=1$, so one possible solution is $f(x)=\\exp(x)$, i.e.,\n",
    "\\begin{split}\n",
    "f(\\mathbf{u}_j, \\mathbf{u}_k, {\\mathbf{v}}_i) = \\frac{\\exp\\left(\\mathbf{u}_j^\\top {\\mathbf{v}}_i\\right)}{\\exp\\left(\\mathbf{u}_k^\\top {\\mathbf{v}}_i\\right)} \\approx \\frac{p_{ij}}{p_{ik}}.\n",
    "\\end{split}\n",
    "\n",
    "If we pick $\\exp\\left(\\mathbf{u}_j^\\top {\\mathbf{v}}_i\\right) \\approx \\alpha p_{ij}$, where $\\alpha$ is a constant, then since $p_{ij}=x_{ij}/x_i$, after taking the logarithm on both sides, we get\n",
    "\\begin{split}\n",
    "\\mathbf{u}_j^\\top {\\mathbf{v}}_i \\approx \\log\\,\\alpha + \\log\\,x_{ij} - \\log\\,x_i\n",
    "\\end{split}\n",
    "\n",
    "We may use additional bias terms to fit $- \\log\\, \\alpha + \\log\\, x_i$, such as the center word bias $b_i$ and the context word bias $c_j$:\n",
    "\\begin{split}\n",
    "\\mathbf{u}_j^\\top \\mathbf{v}_i + b_i + c_j \\approx \\log\\, x_{ij}.\n",
    "\\end{split}\n",
    "\n",
    "Measuring the squared error of the above approximation with weights, the GloVe loss function is obtained as before:\n",
    "\\begin{split}\n",
    "\\sum_{i\\in\\mathcal{V}} \\sum_{j\\in\\mathcal{V}} h(x_{ij}) \\left(\\mathbf{u}_j^\\top \\mathbf{v}_i + b_i + c_j - \\log\\,x_{ij}\\right)^2.\n",
    "\\end{split}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.6. Subword Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In English, words such as “helps”, “helped”, and “helping” are *inflected forms* of the same word “help”. This internal structure of words was neither explored in word2vec nor in GloVe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.6.1. The fastText Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both the skip-gram model and the continuous bag of words model, different inflected forms of the same word are directly represented by different vectors without shared parameters.\n",
    "\n",
    "The *fastText* model, on the other hand, proposed a *subword embedding* approach, where a *subword* is a character $n$-gram. Instead of learning word-level vector representations, fastText can be considered as the *subword-level* skip-gram, where each *center word* is represented by the sum of its subword vectors.\n",
    "\n",
    "To get subwords for each center word using the word \"where\", we first add special characters \"<\" and \">\" ath the beginning and end of the word to distinguish prefixes and suffixes from other subwords. Then, we extract character $n$-grams from the word. For example, when $n=3$, we get all subwords of length 3: \"<wh\", \"whe\", \"her\", \"ere\", \"re>\", and the special subword \"<where>\".\n",
    "\n",
    "In fastText, for any word $w$, we denote by $\\mathcal{G}_w$ the union of all its subwords of length between 3 and 6 and its special subword. The vocabulary is the union of the subwords of all words. Letting $\\mathbf{z}_g$ be the vector of subword $g$ in the dicitonary, the vector $\\mathbf{v}_w$ for word $w$ as a center word in the skip-gram model is the sum of its subword vectors:\n",
    "\\begin{split}\n",
    "\\mathbf{v}_w = \\sum_{g\\in\\mathcal{G}_w} \\mathbf{z}_g.\n",
    "\\end{split}\n",
    "\n",
    "The rest of the fastText model is the same as the skip-gram model.\n",
    "\n",
    "The vocabulary in fastText is larger than that in the skip-gram model, resulting in more model parameters.\n",
    "\n",
    "The computational complexity of fastText is higher than that of the skip-gram model due to the summation over all subwords. However, thanks to shared parameters from subwords among words with similar structures, rare words and even out-of-vocabulary words may obtain better vector representations in fastText."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.6.2. Byte Pair Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fastText, all the extracted subwords have to be of the specified lengths, such as 3 to 6, thus the vocabulary size cannot be predefined.\n",
    "\n",
    "To allow for variable-length subwords in a fixed-size vocabulary, we can apply a compression algorithm called *byte pair encoding (BPE)* to extract subwords.\n",
    "\n",
    "*Byte pair encoding* performs a statistical analysis of the training dataset to discover common symbols within a word, such as consecutive characters of arbitrary length. Starting from symbols of length 1, byte pair encoding iteratively merges the most frequent pair of consecutive symbols to produce new longer symbols. Then, we can use such symbols as subwords to segment words.\n",
    "\n",
    "For example, in BPE, we first initialize the vocabulary of symbols as all the English lowercase characters, a special end-of-word symbol `'_'`, and a special unkonwn symbol `'[UNK]'`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "symbols = [\n",
    "    'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm',\n",
    "    'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z',\n",
    "    '_', '[UNK]'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will not consider symbol pairs that cross boundaries of words, so we only need a dictionary `raw_token_freqs` that maps words to their frequencies (number of occurrences) in a dataset.\n",
    "\n",
    "The special symbol `'_'` is appended to each word so that we can easily recover a word sequence (e.g., \"a taller man) from a sequence of output symbols (e.g., \"a_taller_man_\").\n",
    "\n",
    "Since we start the merging process from a vocabulary of only single characters and special symbols, space is inserted between every pair of consecutive characters within each word (keys of the dictionary `token_freqs`). In other words, space is the delimiter between symbols within a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f a s t _': 4, 'f a s t e r _': 3, 't a l l _': 5, 't a l l e r _': 4}\n"
     ]
    }
   ],
   "source": [
    "raw_token_freqs = {\n",
    "    'fast_': 4, 'faster_': 3, 'tall_': 5, 'taller_': 4,\n",
    "}\n",
    "\n",
    "token_freqs = {}\n",
    "\n",
    "for token, freq in raw_token_freqs.items():\n",
    "    token_freqs[' '.join(list(token))] = raw_token_freqs[token]\n",
    "\n",
    "print(token_freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `get_max_freq_pair` functions returns the most frequent pair of consecutive symbols with in a word, where words come from keys of the input dictionary `token_freqs`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_freq_pair(token_freqs):\n",
    "    pairs = collections.defaultdict(int)\n",
    "\n",
    "    for token, freq in token_freqs.items():\n",
    "        symbols = token.split()\n",
    "        for i in range(len(symbols) - 1):\n",
    "            # key of `pairs` is a tuple of two consecutive symbols\n",
    "            pairs[symbols[i], symbols[i+1]] += freq\n",
    "\n",
    "    return max(pairs, key=pairs.get) # key of `pairs` with max value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('t', 'a')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_max_freq_pair(token_freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a greedy approach based on frequency of consecutive symbols, the byte pair encoding uses the `merge_symbols` function to merge the most frequent pair of consecutive symbols to produce new symbols:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_symbols(max_freq_pair, token_freqs, symbols):\n",
    "    symbols.append(''.join(max_freq_pair))\n",
    "\n",
    "    new_token_freqs = {}\n",
    "    for token, freq in token_freqs.items():\n",
    "        new_token = token.replace(' '.join(max_freq_pair), # old token\n",
    "                                  ''.join(max_freq_pair)) # new token\n",
    "        new_token_freqs[new_token] = token_freqs[token]\n",
    "\n",
    "    return new_token_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('t', 'a')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'f a s t _': 4, 'f a s t e r _': 3, 'ta l l _': 5, 'ta l l e r _': 4}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_freq_pair = get_max_freq_pair(token_freqs)\n",
    "print(max_freq_pair)\n",
    "merge_symbols(max_freq_pair, token_freqs, symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we iteratively perform the byte pair encoding algorithm over the keys of the dictionary `token_freqs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merge #1: ('t', 'a')\n",
      "merge #2: ('ta', 'l')\n",
      "merge #3: ('tal', 'l')\n",
      "merge #4: ('f', 'a')\n",
      "merge #5: ('fa', 's')\n",
      "merge #6: ('fas', 't')\n",
      "merge #7: ('e', 'r')\n",
      "merge #8: ('er', '_')\n",
      "merge #9: ('tall', '_')\n",
      "merge #10: ('fast', '_')\n"
     ]
    }
   ],
   "source": [
    "num_merges = 10 # merge 10 times\n",
    "for i in range(num_merges):\n",
    "    max_freq_pair = get_max_freq_pair(token_freqs)\n",
    "    token_freqs = merge_symbols(max_freq_pair, token_freqs, symbols)\n",
    "    print(f'merge #{i+1}:', max_freq_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the 1st iteration, `'t'` and `'a'` are the most frequent pair of consecutive symbols, so they are merged into a new symbol `'ta'`.\n",
    "\n",
    "In the 2nd iteration, the BPE algorithm merges `'ta'` and `'l'` into a new symbol `'tal'`.\n",
    "\n",
    "After 10 iterations of BPE, we can see that the list `symbols` contains 10 more symbols that are iteratively merged from other symbols:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '_', '[UNK]', 'ta', 'ta', 'tal', 'tall', 'fa', 'fas', 'fast', 'er', 'er_', 'tall_', 'fast_']\n"
     ]
    }
   ],
   "source": [
    "print(symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the same dataset specified in the keys of the dictionary `raw_token_freqs`, each word in the dataset is now segmented by subwords \"fast_\", \"fast\", \"er_\", \"tall_\", and \"tall\" as a result of the BPE algorithm. For example, the word \"faster_\" is segmented as \"fast er_\" and the word \"taller_\" is segmented as \"tall er_\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fast_', 'fast er_', 'tall_', 'tall er_']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(token_freqs.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of byte pair encoding depends on the dataset begin used!\n",
    "\n",
    "We can also use the subwords learned from one dataset to segment words of another dataset. The `segment_BPE` function breaks words into the longest possible subwords from the input dictionary `symbols`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_BPE(tokens, symbols):\n",
    "    outputs = []\n",
    "\n",
    "    for token in tokens:\n",
    "        start = 0\n",
    "        end = len(token)\n",
    "        cur_output = []\n",
    "        # segment token with the longest possible subwords from symbols\n",
    "        while start < len(token) and start < end:\n",
    "            if token[start:end] in symbols:\n",
    "                cur_output.append(token[start:end])\n",
    "                start = end\n",
    "                end = len(token)\n",
    "            else:\n",
    "                end -= 1\n",
    "\n",
    "        if start < len(token):\n",
    "            cur_output.append('[UNK]')\n",
    "        outputs.append(' '.join(cur_output))\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tall e s t _', 'fa t t er']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = ['tallest_', 'fatter']\n",
    "segment_BPE(tokens, symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.7. Word Similarity and Analogy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, word vectors that are pretrained on large corpora can be applied to downstream natural language processing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.7.1. Loading Pretrained Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below lists pretrained GloVe embeddings of dimension 50, 100, and 300. The pretrained fastText embeddings are available in multiple languages. Here we only consider the English version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GloVe 50dim\n",
    "d2l.DATA_HUB['glove.6b.50d'] = (d2l.DATA_URL + 'glove.6B.50d.zip',\n",
    "                                '0b8703943ccdb6eb788e6f091b8946e82231bc4d')\n",
    "\n",
    "# GloVe 100dim\n",
    "d2l.DATA_HUB['glove.6b.100d'] = (d2l.DATA_URL + 'glove.6B.100d.zip',\n",
    "                                 'cd43bfb07e44e6f27cbcc7bc9ae3d80284fdaf5a')\n",
    "\n",
    "# GloVe 300dim\n",
    "d2l.DATA_HUB['glove.42b.300d'] = (d2l.DATA_URL + 'glove.42B.300d.zip',\n",
    "                                  'b5116e234e9eb9076672cfeabf5469f3eec904fa')\n",
    "\n",
    "# fastText English word vectors\n",
    "d2l.DATA_HUB['wiki.en'] = (d2l.DATA_URL + 'wiki.en.zip',\n",
    "                           'c1816da3821ae9f43899be655002f6c723e91b88')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `TokenEmbedding` class will load these pretrained GloVe and fastText embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding:\n",
    "    '''Token Embedding'''\n",
    "    def __init__(self, embedding_name):\n",
    "        self.idx_to_token, self.idx_to_vec = self._load_embedding(embedding_name)\n",
    "        self.unknown_idx = 0\n",
    "        self.token_to_idx = {\n",
    "            token: idx for idx, token in enumerate(self.idx_to_token)\n",
    "        }\n",
    "\n",
    "    def _load_embedding(self, embedding_name):\n",
    "        idx_to_token = ['<unk>']\n",
    "        idx_to_vec = []\n",
    "\n",
    "        data_dir = d2l.download_extract(embedding_name)\n",
    "        # GloVe website: https://nlp.stanford.edu/projects/glove/\n",
    "        # fastText website: https://fasttext.cc/\n",
    "\n",
    "        with open(os.path.join(data_dir, 'vec.txt'), 'r') as f:\n",
    "            for line in f:\n",
    "                elems = line.rstrip().split(' ')\n",
    "                token = elems[0]\n",
    "                elems = [float(elem) for elem in elems[1:]]\n",
    "\n",
    "                # skip header information, such as the top row in fastText\n",
    "                if len(elems) > 1:\n",
    "                    idx_to_token.append(token)\n",
    "                    idx_to_vec.append(elems)\n",
    "\n",
    "        idx_to_vec = [[0] * len(idx_to_vec[0])] + idx_to_vec\n",
    "        return idx_to_token, torch.tensor(idx_to_vec)\n",
    "    \n",
    "    def __getitem__(self, tokens):\n",
    "        indices = [\n",
    "            self.token_to_idx.get(token, self.unknown_idx)\n",
    "            for token in tokens\n",
    "        ]\n",
    "        vecs = self.idx_to_vec[torch.tensor(indices)]\n",
    "        return vecs\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ../data\\glove.6B.50d.zip from http://d2l-data.s3-accelerate.amazonaws.com/glove.6B.50d.zip...\n"
     ]
    }
   ],
   "source": [
    "# load pretrained GloVe 50dim\n",
    "glove_6b50d = TokenEmbedding('glove.6b.50d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400001"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output the vocabulary size\n",
    "len(glove_6b50d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This vocabulary contains 400,000 words (tokens) and a special unknown token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3367"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the index of a word\n",
    "idx = glove_6b50d.token_to_idx['beautiful']\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'beautiful'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this index should correspond to the same word\n",
    "glove_6b50d.idx_to_token[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.5462,  1.2042, -1.1288, -0.1325,  0.9553,  0.0405, -0.4786, -0.3397,\n",
       "        -0.2806,  0.7176, -0.5369, -0.0046,  0.7322,  0.1210,  0.2809, -0.0881,\n",
       "         0.5973,  0.5526,  0.0566, -0.5025, -0.6320,  1.1439, -0.3105,  0.1263,\n",
       "         1.3155, -0.5244, -1.5041,  1.1580,  0.6880, -0.8505,  2.3236, -0.4179,\n",
       "         0.4452, -0.0192,  0.2897,  0.5326, -0.0230,  0.5896, -0.7240, -0.8522,\n",
       "        -0.1776,  0.1443,  0.4066, -0.5200,  0.0908,  0.0830, -0.0220, -1.6214,\n",
       "         0.3458, -0.0109])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the vector of a word using its index\n",
    "glove_6b50d.idx_to_vec[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.7.2. Applying Pretrained Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15.7.2.1. Word Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find semantically similar words for an input word based on cosine similarities between word vectors, we can implement the `knn` ($k$-nearest neighbors) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(W, x, k):\n",
    "    # add 1e-9 for numerical stability\n",
    "    eps = 1e-9\n",
    "\n",
    "    cos = torch.mv(W, x.reshape(-1,)) / (\n",
    "        torch.sqrt(torch.sum(W * W, dim=1) + eps) *\n",
    "        torch.sqrt((x * x).sum())\n",
    "    )\n",
    "\n",
    "    _, topk = torch.topk(cos, k=k)\n",
    "\n",
    "    return topk, [cos[int(i)] for i in topk]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the pretrained word vectors from the `TokenEmbedding` instance to search for similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_tokens(query_token, k, embed):\n",
    "    topk, cos = knn(W=embed.idx_to_vec,\n",
    "                    x=embed[[query_token]],\n",
    "                    k=k+1)\n",
    "    \n",
    "    print(f'input token: {query_token}')\n",
    "    for i, c in zip(topk[1:], cos[1:]): # exclude the input word itself\n",
    "        print(f'cosine sim={float(c):.3f}: {embed.idx_to_token[int(i)]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input token: chip\n",
      "cosine sim=0.856: chips\n",
      "cosine sim=0.749: intel\n",
      "cosine sim=0.749: electronics\n",
      "cosine sim=0.731: semiconductor\n",
      "cosine sim=0.716: maker\n"
     ]
    }
   ],
   "source": [
    "query_token = 'chip'\n",
    "k = 5\n",
    "get_similar_tokens(query_token, k, glove_6b50d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input token: Chip\n",
      "cosine sim=nan: of\n",
      "cosine sim=nan: the\n",
      "cosine sim=nan: <unk>\n",
      "cosine sim=nan: ,\n",
      "cosine sim=nan: to\n"
     ]
    }
   ],
   "source": [
    "query_token = 'Chip'\n",
    "k = 5\n",
    "get_similar_tokens(query_token, k, glove_6b50d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input token: baby\n",
      "cosine sim=0.839: babies\n",
      "cosine sim=0.800: boy\n",
      "cosine sim=0.792: girl\n",
      "cosine sim=0.778: newborn\n",
      "cosine sim=0.765: pregnant\n"
     ]
    }
   ],
   "source": [
    "query_token = 'baby'\n",
    "k = 5\n",
    "get_similar_tokens(query_token, k, glove_6b50d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input token: beautiful\n",
      "cosine sim=0.921: lovely\n",
      "cosine sim=0.893: gorgeous\n",
      "cosine sim=0.830: wonderful\n",
      "cosine sim=0.825: charming\n",
      "cosine sim=0.801: beauty\n"
     ]
    }
   ],
   "source": [
    "query_token = 'beautiful'\n",
    "k = 5\n",
    "get_similar_tokens(query_token, k, glove_6b50d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15.7.2.2. Word Analogy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word analogy completion task is defined as\n",
    "* For a word analogy $a : b :: c : d$, given the first three words $a$, $b$, and $c$, find the fourth word $d$ from the vocabulary.\n",
    "* Denote the vector of word $w$ by $\\textrm{vec}(w)$. To complete the analogy, we will find the word whose vector is the most similar to the result of \n",
    "\\begin{split}\n",
    "\\textrm{vec}(c)+(\\textrm{vec}(b)-\\textrm{vec}(a)).\n",
    "\\end{split}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_analogy(token_a, token_b, token_c, embed):\n",
    "    vecs = embed[\n",
    "        [token_a, token_b, token_c]\n",
    "    ]\n",
    "\n",
    "    x = vecs[2] + (vecs[1] - vecs[0])\n",
    "\n",
    "    topk, cos = knn(\n",
    "        W=embed.idx_to_vec,\n",
    "        x=x,\n",
    "        k=1\n",
    "    )\n",
    "\n",
    "    return embed.idx_to_token[int(topk[0])] # remove unknown words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'daughter'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_a = 'man'\n",
    "token_b = 'woman'\n",
    "token_c = 'son'\n",
    "token_d = get_analogy(token_a, token_b, token_c, glove_6b50d)\n",
    "token_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'japan'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_a = 'beijing'\n",
    "token_b = 'china'\n",
    "token_c = 'tokyo'\n",
    "token_d = get_analogy(token_a, token_b, token_c, glove_6b50d)\n",
    "token_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'biggest'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_a = 'bad'\n",
    "token_b = 'worst'\n",
    "token_c = 'big'\n",
    "token_d = get_analogy(token_a, token_b, token_c, glove_6b50d)\n",
    "token_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'went'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_a = 'do'\n",
    "token_b = 'did'\n",
    "token_c = 'go'\n",
    "token_d = get_analogy(token_a, token_b, token_c, glove_6b50d)\n",
    "token_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.8. Bidirectional Encoder Representations from Transformers (BERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After pretraining on the word embedding models, the output can be thought of as a matrix where each row is a vector that represents a word of a predefined vocabulary. These word embedding models are all *context-independent*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.8.1. From Context-Independent to Context-Sensitive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both word2vec and GloVe assign the same pretrained vector to the same word regardless of the context of the word. Formally, a context-independent representation of any token $x$ is a function $f(x)$ that only takes $x$ as its input. However, in natural language processing, the same word may have different meanings in different contexts.\n",
    "* For example, the word “crane” in contexts “a crane is flying” and “a crane driver came” has completely different meanings; thus, the same word may be assigned different representations depending on contexts.\n",
    "\n",
    "The *context-sensitive* word representations are the representations of words depending on their contexts. A context-sensitive representation of any token $x$ is a function $f(x, c(x))$ depending on both $x$ and its context $c(x)$. Popular context-sensitive word representations include\n",
    "* *TagLM* (language-model-augmented sequence tagger),\n",
    "* *CoVe* (Context Vectors), and\n",
    "* *ELMo* (Embeddings from Language Models).\n",
    "\n",
    "The ELMo is a function that assigns a representation to each word from the input sequence by taking the entire sequence as input. ELMo combines all the intermediate layer representations from pretrained bidirectional LSTM as the output representation. Then the ELMo representation will be added to a downstream task's existing supervised model as additional features, such as by concatenating ELMo representation and the original representation (e.g., GloVe) of tokens in the existing model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.8.2. From Task-Specific to Task-Agnostic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ELMo has significantly improved solutions to a diverse set of NLP tasks, but each solution still hinges on a *task-specific* architecture.\n",
    "\n",
    "The *GPT (Generative Pre-Training)* model represents an effort in designing a general *task-agnostic* model for context-sensitive representations. Built on a Transformer decoder, GPT pretrains a language model that will be used to represent text sequences. When applying GPT to a downstream task, the output of the language model will be fed into an added linear output layer to predict the label of the task.\n",
    "\n",
    "However, due to the autoregressive nature of language models, GPT only looks forward (left-to-right)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.8.3. BERT: Combining the Best of Both Worlds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ELMo encodes context bidrectionally but uses task-specific architectures.\n",
    "* GPT is task-agnostic but encodes context left-to-right.\n",
    "\n",
    "Combining both of them, BERT (Bidirectional Encoder Representations from Transformers) encodes context bidirectionally and requires minimal architecture changes for a wide range of natural language processing tasks.\n",
    "\n",
    "Using a pretrained Transformer encoder, BERT is able to represent any token based on its bidirectional context.\n",
    "\n",
    "During supervised learning of downstream tasks, BERT is similar to GPT in two aspects:\n",
    "1. BERT representations will be fed into an added output layer, with minimal changes to the model architecture depending on nature of tasks, such as predicting for every token vs. predicting for the entire sequence.\n",
    "2. All the prarmeters of the pretrained Transformer encoder are fine-tuned, while the additional output layer will be trained from scratch.\n",
    "\n",
    "The figure below shows the differences among ELMo, GPT, and BERT.\n",
    "\n",
    "![](../imgs/ch15/elmo-gpt-bert.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.8.4. Input Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BERT input sequence represents both single text and text pairs.\n",
    "1. In a case of *single text*, the BERT input sequence is the concatenation of the special classification token \"`<cls>`\", tokens of a text sequence, and the special separation token \"`<sep>`\".\n",
    "2. In a case of *text pairs*, the BERT input sequence is the concatenation of \"`<cls>`\", tokens of the first text sequence, \"`<sep>`\", tokens of the second text sequence, and \"`<sep>`\".\n",
    "\n",
    "One *BERT input sequence* may include either one *text sequence* or two *text sequences*.\n",
    "\n",
    "To distinguish text pairs, the learned segment embeddings $\\mathbf{e}_A$ and $\\mathbf{e}_B$ are added to the token embeddings of the first and second text sequences, respectively. For single text inputs, only $\\mathbf{e}_A$ is used.\n",
    "\n",
    "The `get_tokens_and_segments` function takes either one sentence or two sentences as input, and then returns tokens of the BERT input sequence and their cooresponding segment IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens_and_segments(tokens_a, tokens_b=None):\n",
    "    '''Get tokens of the BERT input sequence and their segment IDs'''\n",
    "    # for single text input\n",
    "    tokens = ['<cls>'] + tokens_a + ['<sep>']\n",
    "\n",
    "    # 0 and 1 are marking segment A and B, respectively\n",
    "    # 0s for tokens_a\n",
    "    segments = [0] * (len(tokens_a) + 2)\n",
    "\n",
    "    if tokens_b is not None:\n",
    "        # for text pair input\n",
    "        tokens += tokens_b + ['<sep>']\n",
    "        # 1s for tokens_b\n",
    "        segments += [1] * (len(tokens_b) + 1)\n",
    "\n",
    "    return tokens, segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT chooses the Transformer encoder as its bidirectional architecture. Common in the Transformer encoder, positional embeddings are added at every position of the BERT input sequence.\n",
    "\n",
    "Different from the original Transfomer encoder, BERT uses *learnable* positional embeddings instead of the fixed positional embeddings. The figure below shows that the embeddings of the BERT input sequence are the sum of the token embeddings, segment embeddings, and positional embeddings.\n",
    "\n",
    "![](../imgs/ch15/bert-input.svg)\n",
    "\n",
    "The `BERTEncoder` uses the Transformer encoder as its backbone and uses segment embedings and learnable positional embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTEncoder(nn.Module):\n",
    "    '''BERT encoder'''\n",
    "    def __init__(self, vocab_size, num_hiddens, ffn_num_hiddens, num_heads, num_blks, dropout, max_len=1000, **kwargs):\n",
    "        super(BERTEncoder, self).__init__(**kwargs)\n",
    "\n",
    "        # token embedding\n",
    "        self.token_embedding = nn.Embedding(num_embeddings=vocab_size,\n",
    "                                            embedding_dim=num_hiddens)\n",
    "        # segment embedding\n",
    "        self.segment_embedding = nn.Embedding(num_embeddings=2, # segment(s) A (and B)\n",
    "                                              embedding_dim=num_hiddens)\n",
    "        \n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_blks):\n",
    "            self.blks.add_module(\n",
    "                name=f'{i}',\n",
    "                module=d2l.TransformerEncoderBlock(\n",
    "                    num_hiddens, ffn_num_hiddens, num_heads, dropout, True\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # In BERT, positional embeddings are learnable,\n",
    "        # we need to create a parameter of positional embeddings\n",
    "        # that are long enough\n",
    "        self.pos_embedding = nn.Parameter(\n",
    "            torch.randn(1, max_len, num_hiddens)\n",
    "        )\n",
    "\n",
    "    def forward(self, tokens, segments, valid_lens):\n",
    "        # Shape of `X` remains unchanged throughout the encoding process:\n",
    "        # (batch size, max sequence length, `num_hiddens`)\n",
    "        \n",
    "        # combine input token embedding and segment embedding\n",
    "        X = self.token_embedding(tokens) + self.segment_embedding(segments)\n",
    "        # add positional embedding\n",
    "        X = X + self.pos_embedding[:, :X.shape[1], :]\n",
    "\n",
    "        for blk in self.blks:\n",
    "            X = blk(X, valid_lens)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "vocab_size = 10000\n",
    "num_hiddens = 768\n",
    "ffn_num_hiddens = 1024\n",
    "num_heads = 4\n",
    "num_blks = 2\n",
    "dropout = 0.2\n",
    "\n",
    "encoder = BERTEncoder(\n",
    "    vocab_size, num_hiddens, ffn_num_hiddens, num_heads, num_blks, dropout\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define `tokens` to be 2 BERT input sequences of length 8, where each token is an index of the vocabulary. In the encoded result from `BERTEncoder`, each token is represented by a vector whose length is predefined by the hyperparameter `num_hiddens`, which is referred to as the *hidden size* (number of hidden units) of the Transformer encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 768])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input sequence of 2 examples\n",
    "tokens = torch.randint(\n",
    "    low = 0,\n",
    "    high = vocab_size,\n",
    "    size = (2, 8) # 2 examples, each example has 8 tokens\n",
    ")\n",
    "\n",
    "segments = torch.tensor([\n",
    "    [0, 0, 0, 0, 1, 1, 1, 1], # segment A\n",
    "    [0, 0, 0, 1, 1, 1, 1, 1] # segment B\n",
    "])\n",
    "\n",
    "encoded_X = encoder(tokens, segments, None)\n",
    "encoded_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of the encoded result is (batch size, number of tokens, hidden size)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.8.5 Pretraining Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forward inference of `BERTEncoder` gives the BERT representation of each token of the input text and the inserted special tokens \"`<cls>`\" and \"`<seq>`\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15.8.5.1. Masked Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from Chapter 9, a language model predicts a token using the context on its left.\n",
    "\n",
    "To encode context bidirectionally for representing each token, BERT randomly masks tokens and uses tokens from the bidirectional context to predict the masked tokens in a self-supervised fashion. This task is referred to as a *masked language model*.\n",
    "\n",
    "In this pretraining task, 15% of tokens will be selected at random as the masked tokens for prediction. To predict a masked token without cheating by using the label, we can replace it with a speical token \"`<mask>`\" in the BERT input sequence. \n",
    "\n",
    "This artificial special token \"`<mask>`\" will never appear in fine-tuning. To avoid this mismatch between pretraining and fine-tuning, if a token is masked for prediction (e.g., \"great\" is selected to be masked and predicted in \"this movie is great), in the input it will be replaced with:\n",
    "* a special token \"`<mask>`\" 80% of the time (e.g., \"this movie is `<mask>`\");\n",
    "* a random token for 10% of the time (e.g., \"this movie is apple\");\n",
    "* the unchanged label token for 10% of the time (e.g., \"this movie is great\").\n",
    "\n",
    "The `MaskLM` class is used to predict masked tokens in the masked language model task of BERT pretraining. The prediction uses a one-hidden-layer MLP (`self.mlp`). In forward inference, it takes two inputs: the encoded result of `BERTEncoder` and the token positions for prediction. The output is the prediction results at these positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskLM(nn.Module):\n",
    "    '''The masked language model task of BERT'''\n",
    "    def __init__(self, vocab_size, num_hiddens, **kwargs):\n",
    "        super(MaskLM, self).__init__(**kwargs)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.LazyLinear(num_hiddens),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(num_hiddens),\n",
    "            nn.LazyLinear(vocab_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, X, pred_positions):\n",
    "        num_pred_positions = pred_positions.shape[1]\n",
    "        pred_positions = pred_positions.reshape(-1)\n",
    "        batch_size = X.shape[0]\n",
    "        batch_idx = torch.arange(0, batch_size)\n",
    "\n",
    "        # Suppose that `batch_size` = 2, `num_pred_positions` = 3,\n",
    "        # then `batch_idx` is `torch.tensor([0, 0, 0, 1, 1, 1])`\n",
    "        batch_idx = torch.repeat_interleave(batch_idx, num_pred_positions)\n",
    "\n",
    "        masked_X = X[batch_idx, pred_positions]\n",
    "        masked_X = masked_X.reshape((batch_size, num_pred_positions, -1))\n",
    "        mlm_Y_hat = self.mlp(masked_X)\n",
    "        return mlm_Y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `encoded_X` from `BERTEncoder` represents 2 BERT input sequences of length 8, where each token is represented by a vector of length 768.\n",
    "\n",
    "We define `mlm_positions` as the 3 indices to predict either BERT input sequence of `encoded_X`. The forward inference of `MaskLM` returns the prediction results `mlm_Y_hat` at all the masked positions `mlm_positions` of `encoded_X`. For each prediction, the size of the result is equal to the vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 768])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 10000])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a masked language model instance\n",
    "mlm = MaskLM(vocab_size, num_hiddens)\n",
    "\n",
    "mlm_positions = torch.tensor([\n",
    "    [1, 5, 2], # 1st example: the 2nd, 6th, and 3rd tokens are masked\n",
    "    [6, 1, 5] # 2nd example: the 7th, 2nd, and 6th tokens are masked\n",
    "])\n",
    "\n",
    "# forward pass\n",
    "mlm_Y_hat = mlm(encoded_X, mlm_positions)\n",
    "mlm_Y_hat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the ground-truth labels `mlm_Y` of the predicted tokens `mlm_Y_hat` under masks, we can calculate the cross-entropy loss of the masked language model task in BERT pretraining:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ground truth\n",
    "mlm_Y = torch.tensor([\n",
    "    [7, 8, 9],\n",
    "    [10, 20, 30]\n",
    "])\n",
    "\n",
    "# compute the loss\n",
    "loss = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "mlm_l = loss(\n",
    "    mlm_Y_hat.reshape((-1, vocab_size)),\n",
    "    mlm_Y.reshape(-1)\n",
    ")\n",
    "mlm_l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 10000])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlm_Y_hat.reshape((-1, vocab_size)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlm_Y.reshape(-1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15.8.5.2. Next Sentence Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help understand the relationship between two text sequences, BERT considers a binary classification task, *next sentence prediction*, in its pretraining. When generating sentence pairs for pretraining, for half of the time they are indeed consecutive sentences with the label \"True\"; while for the other half of the time the second sentence is randomly sampled from the corpus with the label \"False\".\n",
    "\n",
    "The `NextSentencePred` class uses a one-hidden-layer MLP to predict whether the second sentence is the next sentence of the first in the BERT input sequence. \n",
    "\n",
    "Due to the self-attention in the Transformer encoder, the BERT representation of the special token \"`<cls>`\" encodes both the two sentences from the input. The output layer (`self.output`) of the MLP classifier takes `X` as input, where `X` is the output of the MLP hidden layer whose input is the encoded \"`<cls>`\" token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NextSentencePred(nn.Module):\n",
    "    '''The next sentence prediction task of BERT'''\n",
    "    def __init__(self, **kwargs):\n",
    "        super(NextSentencePred, self).__init__(**kwargs)\n",
    "\n",
    "        self.output = nn.LazyLinear(2)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # `X` shape: (batch size, `num_hiddens`)\n",
    "        return self.output(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PyTorch by default will not flatten the tensor\n",
    "encoded_X_flat = torch.flatten(encoded_X, start_dim=1)\n",
    "\n",
    "# input_shape for NextSentencePred: (batch size, `num_hiddens`)\n",
    "nsp = NextSentencePred()\n",
    "nsp_Y_hat = nsp(encoded_X_flat)\n",
    "nsp_Y_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 8, 768]), torch.Size([2, 6144]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_X.shape, encoded_X_flat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the cross-entropy loss of the next sentence prediction task in BERT pretraining:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsp_y = torch.tensor([0, 1])\n",
    "\n",
    "nsp_l = loss(nsp_Y_hat, nsp_y)\n",
    "nsp_l.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the labels in both the aforementioned pretraining tasks can be trivially obtained from the pretraining corpus without manual labeling effort. The original BERT has been pretrained on the concatenation of BookCorpus and English Wikipedia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.8.6. Putting It All Together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When pretraining BERT, the final loss function is a linear combination of both the loss functions for masked language modeling and next sentence prediction.\n",
    "\n",
    "The `BERTModel` class defines the BERT model for pretraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTModel(nn.Module):\n",
    "    '''The BERT model'''\n",
    "    def __init__(self, vocab_size, num_hiddens, ffn_num_hiddens, num_heads, num_blks, dropout, max_len=1000):\n",
    "        super(BERTModel, self).__init__()\n",
    "\n",
    "        # BERT encoder\n",
    "        self.encoder = BERTEncoder(\n",
    "            vocab_size, num_hiddens,ffn_num_hiddens, num_heads, num_blks, dropout, max_len\n",
    "        )\n",
    "\n",
    "        self.hidden = nn.Sequential(\n",
    "            nn.LazyLinear(num_hiddens),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        # masked language model\n",
    "        self.mlm = MaskLM(vocab_size, num_hiddens)\n",
    "        # next sentence prediction\n",
    "        self.nsp = NextSentencePred()\n",
    "\n",
    "    def forward(self, tokens, segments, valid_lens=None, pred_positions=None):\n",
    "        # encode input sequence\n",
    "        encoded_X = self.encoder(tokens, segments, valid_lens)\n",
    "\n",
    "        # mask language model forward pass\n",
    "        if pred_positions is not None:\n",
    "            mlm_Y_hat = self.mlm(encoded_X, pred_positions)\n",
    "        else:\n",
    "            mlm_Y_hat = None\n",
    "\n",
    "        # The hidden layer of the MLP classifer for next sentence prediction\n",
    "        # 0 is the index of the '<cls>' token\n",
    "        cls_out = self.hidden(encoded_X[:, 0, :])\n",
    "        nsp_Y_hat = self.nsp(cls_out)\n",
    "\n",
    "        return encoded_X, mlm_Y_hat, nsp_Y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.9. The Dataset for Pretraining BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the WikiText-2 dataset, each line represents a paragraph where space is inserted between any punctuation and its preceding token. Paragraphs with at least two sentences are retained. To split sentences, we only use the period as the delimiter for simplicity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2l.DATA_HUB['wikitext-2'] = (\n",
    "    'https://s3.amazonaws.com/research.metamind.io/wikitext/'\n",
    "    'wikitext-2-v1.zip', '3c914d17d80b1459be871a5039ac23e752a53cbe')\n",
    "\n",
    "def _read_wiki(data_dir):\n",
    "    file_name = os.path.join(data_dir, 'wiki.train.tokens')\n",
    "    with open(file_name, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Uppercase letters are converted to lowercase ones\n",
    "    paragraphs = [\n",
    "        line.strip().lower().split(' . ')\n",
    "        for line in lines if len(line.split(' . ')) >= 2\n",
    "    ]\n",
    "\n",
    "    random.shuffle(paragraphs)\n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.9.1. Defining Helper Functions for Pretraining Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define helper functions for the two pretraining tasks in BERT. These helper functions are used to transform the raw text corpus into the dataset of the ideal format to pretrain BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15.9.1.1. Generating the Next Sentence Prediction Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `_get_next_sentence` function generates a training example for the binary classification task of next sentence prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_next_sentence(sentence, next_sentence, paragraphs):\n",
    "    if random.random() < 0.5:\n",
    "        is_next = True\n",
    "    else:\n",
    "        # `paragraphs` is a list of lists of lists\n",
    "        next_sentence = random.choice(random.choice(paragraphs))\n",
    "        is_next = False\n",
    "\n",
    "    return sentence, next_sentence, is_next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `_get_nsp_data_from_paragraph` function generates training examples for next sentence prediction from the input `paragraph` by invoking the `_get_next_sentence` function.\n",
    "* The `paragraph` is a list of sentences, where each sentence is a list of tokens.\n",
    "* The `max_len` specifies the maximum length of a BERT input sequence during pretraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_nsp_data_from_paragraph(paragraph, paragraphs, vocab, max_len):\n",
    "    nsp_data_from_paragraph = []\n",
    "\n",
    "    for i in range(len(paragraph) - 1):\n",
    "        tokens_a, tokens_b, is_next = _get_next_sentence(\n",
    "            paragraph[i], paragraph[i+1], paragraphs\n",
    "        )\n",
    "\n",
    "        # Consider 1 '<cls>' token and 2 '<sep>' tokens\n",
    "        if len(tokens_a) + len(tokens_b) + 3 > max_len:\n",
    "            continue\n",
    "\n",
    "        # get tokens and segments\n",
    "        tokens, segments = d2l.get_tokens_and_segments(tokens_a, tokens_b)\n",
    "        nsp_data_from_paragraph.append(\n",
    "            (tokens, segments, is_next)\n",
    "        )\n",
    "\n",
    "    return nsp_data_from_paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15.9.1.2. Generating the Masked Language Modeling Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `_replace_mlm_tokens` function is used to generate training examples for the masked language modeling task from a BERT input sequence.\n",
    "* The `tokens` is a list of tokens representing a BERT input sequence.\n",
    "* The `candidate_pred_positions` is a list of token indices of the BERT input sequence excluding those of special tokens (special tokens are not predicted in the masked language modeling task).\n",
    "* The `num_mlm_preds` is the number of predictions (recall 15% random tokens to predict)\n",
    "\n",
    "We follow the same masking strategy as described in Section 15.8.5.1. At each prediction position, the input may be replaced by a special \"`<mask>`\" token (80% of the time), a random token (10% of the time), or remain unchanged (10% of the time).\n",
    "\n",
    "The function returns \n",
    "* the input tokens after possible replacement,\n",
    "* the token indices where predictions take place, and\n",
    "* the labels for these predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _replace_mlm_tokens(tokens, candidate_pred_positions, num_mlm_preds, vocab):\n",
    "    # For the input of a masked language model,\n",
    "    # we make a new copy of tokens and \n",
    "    # replace some of them by `<mask>` or random tokens\n",
    "    mlm_input_tokens = [token for token in tokens]\n",
    "    pred_positions_and_labels = []\n",
    "\n",
    "    # Shuffle for getting 15% random tokens for prediction in the masked language modeling task\n",
    "    random.shuffle(candidate_pred_positions)\n",
    "\n",
    "    for mlm_pred_position in candidate_pred_positions:\n",
    "        if len(pred_positions_and_labels) >= num_mlm_preds:\n",
    "            break\n",
    "\n",
    "        masked_token = None\n",
    "\n",
    "        # 80% of the time: replace the word with the `<mask>` token\n",
    "        if random.random() < 0.8:\n",
    "            masked_token = '<mask>'\n",
    "        else:\n",
    "            # 10% of the time: keep the word unchanged\n",
    "            if random.random() < 0.5:\n",
    "                masked_token = tokens[mlm_pred_position]\n",
    "            # 10% of the time: replace the word with a random word\n",
    "            else:\n",
    "                masked_token = random.choice(vocab.idx_to_token)\n",
    "\n",
    "        # update the masked token\n",
    "        mlm_input_tokens[mlm_pred_position] = masked_token\n",
    "        # append the position and the label of the masked token\n",
    "        pred_positions_and_labels.append(\n",
    "            (mlm_pred_position, tokens[mlm_pred_position])\n",
    "        )\n",
    "\n",
    "    return mlm_input_tokens, pred_positions_and_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By invoking the `_replace_mlm_tokens` function above, the `_get_mlm_data_from_tokens` function takes a BERT input sequence (`tokens`) as an input and return \n",
    "* indices of the input tokens (after possible token replacement), \n",
    "* the token indices where predictions take place, and\n",
    "* label indices for these predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_mlm_data_from_tokens(tokens, vocab):\n",
    "    candidate_pred_positions = []\n",
    "\n",
    "    # `tokens` is a list of strings\n",
    "    for i, token in enumerate(tokens):\n",
    "        # special tokens are not predicted in the masked language modeling task\n",
    "        if token in ['<cls>', '<sep>']:\n",
    "            continue\n",
    "\n",
    "        candidate_pred_positions.append(i)\n",
    "\n",
    "    # 15% of random tokens are predicted in the masked language modeling task\n",
    "    num_mlm_preds = max(1, round(len(tokens) * 0.15))\n",
    "    # generate the input and label for the masked language modeling task\n",
    "    mlm_input_tokens, pred_positions_and_labels = _replace_mlm_tokens(\n",
    "        tokens, candidate_pred_positions, num_mlm_preds, vocab\n",
    "    )\n",
    "\n",
    "    pred_positions_and_labels = sorted(pred_positions_and_labels, key=lambda x: x[0])\n",
    "    pred_positions = [v[0] for v in pred_positions_and_labels]\n",
    "    mlm_pred_labels = [v[1] for v in pred_positions_and_labels]\n",
    "\n",
    "    return vocab[mlm_input_tokens], pred_positions, vocab[mlm_pred_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.9.2. Transforming Text into the Pretraining Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `_pad_bert_inputs` functions appends the special \"`<pad>`\" tokens to the inputs.\n",
    "* The `examples` argument contains the outputs from the helper functions \n",
    "    * `_get_nsp_data_from_paragraph` and \n",
    "    * `_get_mlm_data_from_tokens`\n",
    "\n",
    "for the two pretraining tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pad_bert_inputs(examples, max_len, vocab):\n",
    "    max_num_mlm_preds = round(max_len * 0.15)\n",
    "\n",
    "    all_token_ids, all_segments, valid_lens = [],[],[]\n",
    "    all_pred_positions, all_mlm_weights, all_mlm_labels = [], [], []\n",
    "    nsp_labels = []\n",
    "\n",
    "    for (token_ids, pred_positions, mlm_pred_label_ids, segments, is_next) in examples:\n",
    "        all_token_ids.append(\n",
    "            torch.tensor(\n",
    "                token_ids + [vocab['<pad>']] * (max_len - len(token_ids)), # padding\n",
    "                dtype=torch.long\n",
    "            )\n",
    "        )\n",
    "        all_segments.append(\n",
    "            torch.tensor(\n",
    "                segments + [0] * (max_len - len(segments)), # padding\n",
    "                dtype=torch.long\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # `valid_lens` excludes count of '<pad>' tokens\n",
    "        valid_lens.append(\n",
    "            torch.tensor(\n",
    "                len(token_ids),\n",
    "                dtype=torch.float32\n",
    "            )\n",
    "        )\n",
    "        all_pred_positions.append(\n",
    "            torch.tensor(\n",
    "                pred_positions + [0] * (max_num_mlm_preds - len(pred_positions)), # padding\n",
    "                dtype=torch.long\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Predictions of padded tokens will be filtered out in the loss\n",
    "        # via multiplication of 0 weights\n",
    "        all_mlm_weights.append(\n",
    "            torch.tensor(\n",
    "                [1.0] * len(mlm_pred_label_ids) + \\\n",
    "                [0.0] * (max_num_mlm_preds - len(pred_positions)), # exclude padding\n",
    "                dtype=torch.float32\n",
    "            )\n",
    "        )\n",
    "        all_mlm_labels.append(\n",
    "            torch.tensor(\n",
    "                mlm_pred_label_ids + [0] * (max_num_mlm_preds - len(mlm_pred_label_ids)), # padding\n",
    "                dtype=torch.long\n",
    "            )\n",
    "        )\n",
    "\n",
    "        nsp_labels.append(\n",
    "            torch.tensor(\n",
    "                is_next, dtype=torch.long\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return (\n",
    "        all_token_ids, all_segments, valid_lens, all_pred_positions,\n",
    "        all_mlm_weights, all_mlm_labels, nsp_labels\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting the helper functions for generating training examples of the two pretraining tasks, and the helper function for padding inputs together, we can customize the `_WikiTextDataset` class as the WikiText-2 dataset for pretraining BERT.\n",
    "\n",
    "The original BERT model uses WordPiece embeddings whose vocabulary size is 30,000. The tokenization method of WordPiece is similar to that of BPE. Infrequent tokens that appear less than 5 times are filtered out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _WikiTextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, paragraphs, max_len):\n",
    "        # Input `paragraphs[i]` is a list of sentence strings representing a paragraph;\n",
    "        # while output `paragraphs[i]` is a list of sentences representing a paragraph,\n",
    "        # where each sentence is a list of tokens\n",
    "        paragraphs = [\n",
    "            d2l.tokenize(paragraph, token='word')\n",
    "            for paragraph in paragraphs\n",
    "        ]\n",
    "        sentences = [\n",
    "            sentence for paragraph in paragraphs for sentence in paragraph\n",
    "        ]\n",
    "\n",
    "        self.vocab = d2l.Vocab(\n",
    "            tokens=sentences,\n",
    "            min_freq=5,\n",
    "            reserved_tokens=['<pad>', '<mask>', '<cls>', '<sep>']\n",
    "        )\n",
    "\n",
    "        # Get data for the next sentence prediction task\n",
    "        examples = []\n",
    "        for paragraph in paragraphs:\n",
    "            examples.extend(\n",
    "                _get_nsp_data_from_paragraph(\n",
    "                    paragraph, paragraphs, self.vocab, max_len\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Get data for the masked language model task\n",
    "        examples = [\n",
    "            (_get_mlm_data_from_tokens(tokens, self.vocab) + (segments, is_next))\n",
    "            for tokens, segments, is_next in examples\n",
    "        ]\n",
    "\n",
    "        # Pad inputs\n",
    "        (self.all_token_ids, self.all_segments, self.valid_lens,\n",
    "         self.all_pred_positions, self.all_mlm_weights, \n",
    "         self.all_mlm_labels, self.nsp_labels) = _pad_bert_inputs(\n",
    "             examples, max_len, self.vocab\n",
    "         )\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # access the pretraining examples by index\n",
    "        return (\n",
    "            self.all_token_ids[idx], self.all_segments[idx], self.valid_lens[idx],\n",
    "            self.all_pred_positions[idx], self.all_mlm_weights[idx],\n",
    "            self.all_mlm_labels[idx], self.nsp_labels[idx]\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.all_token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `load_data_wiki` function uses the `_read_wiki` helper function and our customized `_WikiTextDataset` class to download and read the WikiText-2 dataset and generate pretraining examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_wiki(batch_size, max_len):\n",
    "    '''Load the WikiText-2 dataset'''\n",
    "    num_workers = d2l.get_dataloader_workers()\n",
    "\n",
    "    data_dir = d2l.download_extract('wikitext-2', 'wikitext-2')\n",
    "    # read paragraphs\n",
    "    paragraphs = _read_wiki(data_dir)\n",
    "    # create a dataset instance\n",
    "    train_set = _WikiTextDataset(paragraphs, max_len)\n",
    "    # create a dataloader\n",
    "    train_iter = torch.utils.data.DataLoader(\n",
    "        train_set,\n",
    "        batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return train_iter, train_set.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ../data\\wikitext-2-v1.zip from https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip...\n"
     ]
    }
   ],
   "source": [
    "batch_size = 512\n",
    "max_len = 64\n",
    "\n",
    "train_iter, vocab = load_data_wiki(batch_size, max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we set the batch size to 512 and the maximum length of a BERT input sequence to be 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (tokens_X, segments_X, valid_lens_X, \n",
    "     pred_positions_X, mlm_weights_X, mlm_Y, nsp_Y) in train_iter:\n",
    "    print(\n",
    "        f\"tokens_X shape: {tokens_X.shape}, \"\n",
    "        f\"\\nsegments_X shape: {segments_X.shape}, \"\n",
    "        f\"\\nuseful tokens number in examples: {valid_lens_X.shape}, \"\n",
    "        f\"\\ntrue predicted positions in the input sequences: {pred_positions_X.shape}, \"\n",
    "        f\"\\nthe weights of masked tokens: {mlm_weights_X.shape}, \"\n",
    "        f\"\\nMLM label: {mlm_Y.shape}, \"\n",
    "        f\"\\nNSP label: {nsp_Y.shape}\"\n",
    "    )\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each BERT input sequence, $64\\times 15% = 10$ positions are predicted for the masked language modeling task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20256"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vocabulary size\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.10. Pretraining BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the WikiText-2 dataset\n",
    "batch_size = 512\n",
    "max_len = 64\n",
    "\n",
    "train_iter, vocab = d2l.load_data_wiki(batch_size, max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.10.1. Pretraining BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The base model ($\\textrm{BERT}_{\\textrm{BASE}}$) uses 12 layers (Transformer encoder blocks) with 768 hidden units (hidden size) and 12 self-attention heads. This model has 110 million parameters.\n",
    "* The large model ($\\textrm{BERT}_{\\textrm{LARGE}}$) uses 24 layers with 1024 hidden units and 16 self-attention heads. This model has 340 million parameters.\n",
    "\n",
    "Here, we define a small BERT, using 2 layers, 128 hidden units, and 2 self-attention heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = d2l.BERTModel(\n",
    "    vocab_size=len(vocab),\n",
    "    num_hiddens=128,\n",
    "    ffn_num_hiddens=256,\n",
    "    num_heads=2,\n",
    "    num_blks=2,\n",
    "    dropout=0.2\n",
    ")\n",
    "\n",
    "devices = d2l.try_all_gpus()\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the training examples, the `_get_batch_loss_bert` function computes the loss for both the masked language modeling task and the next sentence prediction task. The final loss of BERT pretraining is the sum of both losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_batch_loss_bert(net, loss, vocab_size,\n",
    "                         tokens_X, segments_X, valid_lens_X,\n",
    "                         pred_positions_X, mlm_weights_X, mlm_Y, nsp_Y):\n",
    "    # forward pass\n",
    "    _, mlm_Y_hat, nsp_Y_hat = net(\n",
    "        tokens_X, segments_X, valid_lens_X.reshape(-1), pred_positions_X\n",
    "    )\n",
    "\n",
    "    # compute masked language modeling loss\n",
    "    mlm_l = loss(\n",
    "        mlm_Y_hat.reshape(-1, vocab_size),\n",
    "        mlm_Y.reshape(-1)\n",
    "    ) * mlm_weights_X.reshape(-1, 1)\n",
    "    mlm_l = mlm_l.sum() / (mlm_weights_X.sum() + 1e-8)\n",
    "\n",
    "    # compute next sentence prediction loss\n",
    "    nsp_l = loss(nsp_Y_hat, nsp_Y)\n",
    "    \n",
    "    # combine the two losses\n",
    "    l = mlm_l + nsp_l\n",
    "    \n",
    "    return mlm_l, nsp_l, l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the training loop, the `train_bert` function defines the procedure to pretrain BERT (`net`) on the WikiText-2 (`train_iter`) dataset.\n",
    "\n",
    "Training BERT can take very long. Intead of specifying the number of epochs for training, the input `num_steps` specifies the number of iteration steps for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bert(train_iter, net, loss, vocab_size, devices, num_steps):\n",
    "    # warm up\n",
    "    net(*next(iter(train_iter))[:4])\n",
    "    net = nn.DataParallel(net, device_ids=devices).to(devices[0])\n",
    "\n",
    "    trainer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "\n",
    "    step = 0\n",
    "    timer = d2l.Timer()\n",
    "\n",
    "    animator = d2l.Animator(xlabel='step', ylabel='loss',\n",
    "                            xlim=[1, num_steps], legend=['mlm', 'nsp'])\n",
    "    \n",
    "    # `metric` consists of\n",
    "    # sum of masked language modeling losses, sum of next sentence prediction losses,\n",
    "    # no. of sentence pairs, count\n",
    "    metric = d2l.Accumulator(4)\n",
    "\n",
    "    num_steps_reached = False\n",
    "    while step < num_steps and not num_steps_reached:\n",
    "        for tokens_X, segments_X, valid_lens_X, pred_positions_X,\\\n",
    "            mlm_weights_X, mlm_Y, nsp_Y in train_iter:\n",
    "            # send data to devices\n",
    "            tokens_X = tokens_X.to(devices[0])\n",
    "            segments_X = segments_X.to(devices[0])\n",
    "            valid_lens_X = valid_lens_X.to(devices[0])\n",
    "            pred_positions_X = pred_positions_X.to(devices[0])\n",
    "            mlm_weights_X = mlm_weights_X.to(devices[0])\n",
    "            mlm_Y = mlm_Y.to(devices[0])\n",
    "            nsp_Y = nsp_Y.to(devices[0])\n",
    "\n",
    "            # zero the gradients before forward pass\n",
    "            trainer.zero_grad()\n",
    "            timer.start()\n",
    "\n",
    "            # forward pass\n",
    "            mlm_l, nsp_l, l = _get_batch_loss_bert(\n",
    "                net, loss, vocab_size, tokens_X, segments_X, valid_lens_X,\n",
    "                pred_positions_X, mlm_weights_X, mlm_Y, nsp_Y\n",
    "            )\n",
    "            # backward pass\n",
    "            l.backward()\n",
    "            # update parameters\n",
    "            trainer.step()\n",
    "\n",
    "            timer.stop()\n",
    "            metric.add(mlm_l, nsp_l, tokens_X.shape[0], 1)\n",
    "            animator.add(step + 1,\n",
    "                         (metric[0] / metric[3], metric[1] / metric[3]))\n",
    "            \n",
    "            step += 1\n",
    "            if step == num_steps:\n",
    "                num_steps_reached = True\n",
    "                break\n",
    "\n",
    "    print(f'MLM loss {metric[0] / metric[3]:.3f}, NSP loss {metric[1] / metric[3]:.3f}')\n",
    "    print(f'{metric[2] / timer.sum():.1f} sentence pairs/sec on {str(devices)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bert(train_iter, net, loss, len(vocab), devices, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.10.2. Representing Text with BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After pretraining BERT, we can use it to represent single text, text pairs, or any token in them.\n",
    "\n",
    "The `get_bert_encoding` function returns the BERT (`net`) representation for all tokens in `tokens_a` and `tokens_b` (if not `None`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_encoding(net, tokens_a, tokens_b=None):\n",
    "    tokens, segments = d2l.get_tokens_and_segments(tokens_a, tokens_b)\n",
    "    token_ids = torch.tensor(vocab[tokens], device=devices[0]).unsqueeze(0)\n",
    "    segments = torch.tensor(segments, device=devices[0]).unsqueeze(0)\n",
    "    valid_len = torch.tensor(len(tokens), device=devices[0]).unsqueeze(0)\n",
    "\n",
    "    encoded_X, _, _ = net(token_ids, segments, valid_len)\n",
    "    return encoded_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1\n",
    "string_a = 'a crane is flying'\n",
    "tokens_a = string_a.split()\n",
    "encoded_text = get_bert_encoding(net, tokens_a)\n",
    "# Tokens: '<cls>' (class), 'a', 'crane', 'is', 'flying', '<sep>'\n",
    "encoded_text_cls = encoded_text[:, 0, :]\n",
    "encoded_text_crane = encoded_text[:, 2, :]\n",
    "\n",
    "encoded_text.shape, encoded_text_cls.shape, encoded_text_crane[0][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2\n",
    "string_a = 'a crane driver came'\n",
    "string_b = 'he just left'\n",
    "\n",
    "tokens_a = string_a.split()\n",
    "tokens_b = string_b.split()\n",
    "\n",
    "encoded_pair = get_bert_encoding(net, tokens_a, tokens_b)\n",
    "# Tokens: '<cls>' (class), 'a', 'crane', 'driver', 'came', '<sep>', 'he', 'just', 'left', '<sep>'\n",
    "encoded_pair_cls = encoded_pair[:, 0, :]\n",
    "encoded_pair_crane = encoded_pair[:, 2, :]\n",
    "\n",
    "encoded_pair.shape, encoded_pair_cls.shape, encoded_pair_crane[0][:3]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
