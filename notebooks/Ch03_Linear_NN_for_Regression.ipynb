{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3 - Linear Neural Networks for Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that we wish to estimate the prices of houses (in dollars) based on their area (in square feet) and age (in years)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1. Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a *linear regression* model, we assume that the relationship between features $\\mathbf{x}$ and target $y$ is approximately linear, i.e., that the conditional mean $E[Y \\mid X=\\mathbf{x}]$ can be expresed as a weighted sum of the features $\\mathbf{x}$. In addition, we assume that any noise causing the target value to deviate from its expected value is well behaved, following a Gaussian distribution.\n",
    "\n",
    "Typically,\n",
    "* $n$ is the number of examples in the dataset,\n",
    "* $\\mathbf{x}^{(i)}$ denotes the i-th sample and $x_j^{(i)} denotes its j-th coordinate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1.1. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the inputs consist of $d$ features, each feature is assigned an index (between 1 to $d$) and the prediction $\\hat{y}$ is,\n",
    "\n",
    "\\begin{split}\n",
    "\\hat{y} = w_1  x_1 + \\cdots + w_d  x_d + b\n",
    "\\end{split}\n",
    "or in a dot-product form:\n",
    "\n",
    "\\begin{split}\n",
    "\\hat{y} = \\mathbf{w}^\\top \\mathbf{x} + b\n",
    "\\end{split}\n",
    "where the vector $\\mathbf{x}$ corresponds to the features of a single example.\n",
    "\n",
    "The *design matrix* $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$ refers to as the features of the entire dataset of $n$ examples. $\\mathbf{X}$ contains one row for every example and one column for every feature.\n",
    "\n",
    "For a collection of features $\\mathbf{X}$, the predictions $\\hat{\\mathbf{y}} \\in \\mathbb{R}^n$ can be expressed as\n",
    "\n",
    "\\begin{split}\n",
    "{\\hat{\\mathbf{y}}} = \\mathbf{X} \\mathbf{w} + b\n",
    "\\end{split}\n",
    "\n",
    "Given features of a training dataset $\\mathbf{X}$ and corresponding (known) labels $\\mathbf{y}$, the goal of linear regression is to find the weight vector $\\mathbf{w}$ and the bias term $b$ such that given features of a new data example sampled from the same distribution as $\\mathbf{X}$, the new example's label will (in expectation) be predicted with the smallest error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1.2. Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Loss functions* quantify the distance between the *real* and *predicted* values of the target. \n",
    "\n",
    "The most common loss function for regression is the squared error. When the prediction for an example $i$ is $\\hat{y}^{(i)}$ and the corresponding true label is $y^{(i)}$, the *squared error* is\n",
    "\n",
    "\\begin{split}\n",
    "l^{(i)}(\\mathbf{w}, b) = \\frac{1}{2} \\left(\\hat{y}^{(i)} - y^{(i)}\\right)^2\n",
    "\\end{split}\n",
    "\n",
    "The loss on the entire training set of $n$ examples is,\n",
    "\n",
    "\\begin{split}\n",
    "L(\\mathbf{w}, b) =\\frac{1}{n}\\sum_{i=1}^n l^{(i)}(\\mathbf{w}, b) =\\frac{1}{n} \\sum_{i=1}^n \\frac{1}{2}\\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right)^2\n",
    "\\end{split}\n",
    "\n",
    "The objective is to see parameters $(\\mathbf{w}^*, b^*)$ tha minimize the total loss across all training examples:\n",
    "\n",
    "\\begin{split}\n",
    "\\mathbf{w}^*, b^* = \\operatorname*{argmin}_{\\mathbf{w}, b}\\  L(\\mathbf{w}, b).\n",
    "\\end{split}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1.3. Analytic Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimizing $\\|\\mathbf{y} - \\mathbf{X}\\mathbf{w}\\|^2$ is equivalent to\n",
    "\n",
    "\\begin{aligned}\n",
    "    \\partial_{\\mathbf{w}} \\|\\mathbf{y} - \\mathbf{X}\\mathbf{w}\\|^2 =\n",
    "    2 \\mathbf{X}^\\top (\\mathbf{X} \\mathbf{w} - \\mathbf{y}) = 0\n",
    "    \\textrm{ and hence }\n",
    "    \\mathbf{X}^\\top \\mathbf{y} = \\mathbf{X}^\\top \\mathbf{X} \\mathbf{w}.\n",
    "\\end{aligned}\n",
    "\n",
    "The solution is\n",
    "\n",
    "\\begin{split}\n",
    "\\mathbf{w}^* = (\\mathbf X^\\top \\mathbf X)^{-1}\\mathbf X^\\top \\mathbf{y}\n",
    "\\end{split}\n",
    "which will only be unique when the matrix $\\mathbf{X}^\\top \\mathbf{X}$ is invertible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1.4. Minibatch Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
