{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 18 - Gaussian Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any model that is linear in its parameters with a Gaussian distribution over the parameters is a **Gaussian process**. This class spans discrete models, including random walks, and autoregressive processes, as well as continuous models, including Bayesian linear regression models, polynomials, Fourier series, radial basis functions, and even neural networks with an infinite number of hidden units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18.1. Introduction to Gaussian Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the weights of neural networks, which are relatively uninterpretable, the gaussian proceses provide a mechanism for directly reasoning about the high-level properties of functions that could fit the data.\n",
    "\n",
    "![](../imgs/ch18/gp-observed-data.svg)\n",
    "\n",
    "Suppose we have a dataset shown in the figure above of regression targets (outputs) $y$ at inputs $x$. To fit the data with a Gaussian process, we start by specifying a *prior distribution* over what types of functions we may believe to be reasonable, some of which are shown in the figure below.\n",
    "\n",
    "![](../imgs/ch18/gp-sample-prior-functions.svg)\n",
    "\n",
    "We are not looking for functions that fit the dataset exactly, but rather for specifying reasonable high-level properties of the solutions, such as how quickly they vary with inputs.\n",
    "\n",
    "Once we condition on data, we can use this prior to infer a *posterior distribution* over functions that could fit the data. The figure below shows some possible posterior functions.\n",
    "\n",
    "![](../imgs/ch18/gp-sample-posterior-functions.svg)\n",
    "\n",
    "Each of these posterior functions are entirely consistent with the data, perfectly running through each observation. In order to use these posterior samples to make predictions, we can average the values of every possible sample function from the posterior, to create the blue sruve in the figure below.\n",
    "\n",
    "![](../imgs/ch18/gp-posterior-samples.svg)\n",
    "\n",
    "We do not have to take an infinite number of samples to compute this epxectation.\n",
    "\n",
    "We also want to know how confident we should be in our predictions. Intuitively, we should have more uncertainty where there is more variability in the sample posterior functions, as this tells us there are many more possible values the true function could take. This type of uncertainty is called *epistemic uncertainty*, which is the *reducible uncertainty* associated with lack of information. As we acquire more data, this type of uncertainty disappears, as there will be increasingly fewer solutions consistent with what we observe.\n",
    "\n",
    "Similar to the posterior mean, we can compute the *posterior variance* in closed form. The shaded region in the figure below shows the two-time posterior standard deviation on either side of the mean, creating a *credible interval* that has a 95% probability of containing the true value of the function for any input $x$.\n",
    "\n",
    "![](../imgs/ch18/gp-posterior-samples-95.svg)\n",
    "\n",
    "The properties of the Gaussian process that we fit the data are strongly controlled by the *covariance function*, also known as the *kernel*. The covariance function we used is called the *RBF (Radial Basis Function) kernel*, which has the form\n",
    "\\begin{split}\n",
    "k_{\\textrm{RBF}}(x,x') = \\textrm{Cov}(f(x),f(x')) = a^2 \\exp\\left(-\\frac{1}{2\\ell^2}||x-x'||^2\\right)\n",
    "\\end{split}\n",
    "\n",
    "The *hyperparameters* of this kernel are interpretable:\n",
    "* the *amplitude* $a$ controls the vertical scale over which the function is varying. Larger $a$ means larger function values.\n",
    "* the *length-scale* $\\ell$ controls the rate of variation of the function. Larger $\\ell$ means the function varies more slowly.\n",
    "\n",
    "At $||x-x'|| = \\ell$, the covariance between a pair of function values is $a^2\\exp(-0.5)$. At larger distances than $\\ell$, the values of the function values becomes nearly uncorrelated, which means that if we want to make a prediction at a point $x_*$, then function values with input $x$ such that $||x-x_*|| > \\ell$ will not have a strong effect on our predictions.\n",
    "\n",
    "The generalization performance of a Gaussian process will depend on having reasonable values for these hyperparameters. Fortunately, there is a robust and automatic way to specify these hyperparameters, using *maximum likelihood*.\n",
    "\n",
    "The essense of a Gaussian process states that any collection of function values $f(x_1),\\dots,f(x_n)$, indexed by any collection of inputs $x_1,\\dots,x_n$, has a joint multivariate Gaussian distribution. The mean vector $\\mu$ of this distribution is given by a *mean function*, which is typically taken to be a constant or zero. The covariance matrix of this distribution is given by the *kernel* evaluated at all pairs of the inputs $x$.\n",
    "\n",
    "\\begin{split}\n",
    "\\begin{bmatrix}\n",
    "f(x) \\\\f(x_1) \\\\ \\vdots \\\\ f(x_n) \\end{bmatrix}\\sim \\mathcal{N}\\left(\\mu, \\begin{bmatrix}k(x,x) & k(x, x_1) & \\dots & k(x,x_n) \\\\ k(x_1,x) & k(x_1,x_1) & \\dots & k(x_1,x_n) \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ k(x_n, x) & k(x_n, x_1) & \\dots & k(x_n,x_n) \n",
    "\\end{bmatrix}\\right)\n",
    "\\end{split}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This specifies a GP prior. We can compute the conditional distribution of $f(x)$ for any $x$ given $f(x_1),\\dots,f(x_n)$, the function values we have observed. This conditional distribution is called the *posterior*, and it is what we use to make predictions.\n",
    "\n",
    "In particular,\n",
    "\\begin{split}\n",
    "f(x) | f(x_1), \\dots, f(x_n) \\sim \\mathcal{N}(m,s^2)\n",
    "\\end{split}\n",
    "where\n",
    "\\begin{aligned}\n",
    "m &= k(x,x_{1:n}) k(x_{1:n},x_{1:n})^{-1} f(x_{1:n}) \\\\\n",
    "s^2 &= k(x,x) - k(x,x_{1:n})k(x_{1:n},x_{1:n})^{-1}k(x,x_{1:n})\n",
    "\\end{aligned}\n",
    "where $k(x,x_{1:n})$ is a $1\\times n$ vector formed by evaluating $k(x, x_i)$ for $i=1,\\dots,n$, and $k(x_{1:n},x_{1:n})$ is an $n\\times n$ matrix formed by evaluating $k(x_i,x_j)$ for $i,j=1,\\dots,n$.\n",
    "\n",
    "The value $m$ is waht we can use as a point predictor for any $x$, and $s^2$ is what we use for uncertainty: if we want to create an interval with a 95% probability that $f(x)$ is in the interval, we would use $m\\pm 2s$.\n",
    "\n",
    "Suppose we observe a single datapoint, $f(x_1)$ and we want to determine the value of $f(x)$ at some $x$. Because $f(x)$ is described by a Gaussian process, we know the joint distribution over $f(x)$ and $f(x_1)$ is a Gaussian:\n",
    "\n",
    "\\begin{split}\n",
    "\\begin{bmatrix}\n",
    "f(x) \\\\\n",
    "f(x_1) \\\\\n",
    "\\end{bmatrix}\n",
    "\\sim\n",
    "\\mathcal{N}\\left(\\mu,\n",
    "\\begin{bmatrix}\n",
    "k(x,x) & k(x, x_1) \\\\\n",
    "k(x_1,x) & k(x_1,x_1)\n",
    "\\end{bmatrix}\n",
    "\\right)\\end{split}\n",
    "\n",
    "The off-diagonal expression $k(x,x_1) = k(x_1, x)$ tells how correlated the function values will be - how strongly determined $f(x)$ will be form $f(x_1)$. If we use a large length-scale $\\ell$, relative to $||x-x_1||$, then the function values will be highly correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18.2. Gaussian Process Priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial import distance_matrix\n",
    "\n",
    "from d2l import torch as d2l\n",
    "\n",
    "d2l.set_figsize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18.2.1. Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
