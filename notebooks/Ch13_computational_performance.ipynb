{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 13 - Computational Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.1. Compilers and Interpreters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to now, we have focused on *imperative programming*, which makes uses of statements such as `print`, `+`, and `if` to change a program's state. For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "def add(a, b):\n",
    "    return a + b\n",
    "\n",
    "def fancy_func(a, b, c, d):\n",
    "    e = add(a, b)\n",
    "    f = add(c, d)\n",
    "    g = add(e, f)\n",
    "\n",
    "    return g\n",
    "\n",
    "print(fancy_func(1, 2, 3, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python is an *interpreted language*. When evaluating the above `fancy_func` function it performs the operations making up the function’s body *in sequence*, represented in the following figure.\n",
    "\n",
    "![](../imgs/ch13/computegraph.svg)\n",
    "\n",
    "Although imperative programming is convenient, it may be inefficient. The three `add` function calls in `fancy_func` need to be executed sequentially."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.1.1. Symbolic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the *symbolic programming*, the computation is usually performed only once the process has been fully defined. This approach is commonly used in *deep learning* and *automatic differentiation*, and usually involves the following steps:\n",
    "1. Define the operations to be executed.\n",
    "2. Compile the operations into an executable program.\n",
    "3. Provide the required inputs and call the compiled program for execution.\n",
    "\n",
    "In this way, we can skip the Python interpreter for the actual computation, thus removing a performance bottleneck that can become significant on multiple fast GPUs paired with a single Python thread on a CPU. It can also release memory whenever a variable is no longer needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "def add(a, b):\n",
      "    return a + b\n",
      "\n",
      "def fancy_func(a, b, c, d):\n",
      "    e = add(a, b)\n",
      "    f = add(c, d)\n",
      "    g = add(e, f)\n",
      "    return g\n",
      "print(fancy_func(1, 2, 3, 4))\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# example of using compile() to create a program on the fly\n",
    "# symbolic programming\n",
    "def add_():\n",
    "    return '''\n",
    "def add(a, b):\n",
    "    return a + b\n",
    "'''\n",
    "\n",
    "def fancy_func_():\n",
    "    return '''\n",
    "def fancy_func(a, b, c, d):\n",
    "    e = add(a, b)\n",
    "    f = add(c, d)\n",
    "    g = add(e, f)\n",
    "    return g\n",
    "'''\n",
    "\n",
    "def evoke_():\n",
    "    return add_() + fancy_func_() + 'print(fancy_func(1, 2, 3, 4))'\n",
    "\n",
    "prog = evoke_()\n",
    "print(prog)\n",
    "y = compile(prog, '', 'exec')\n",
    "exec(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Imperative programming is easier. When imperative programming is used in Python, the majority of the code is straightforward and easy to write. It is also easier to debug imperative programming code. This is because it is easier to obtain and print all relevant intermediate variable values, or use Python’s built-in debugging tools.\n",
    "* Symbolic programming is more efficient and easier to port. Symbolic programming makes it easier to optimize the code during compilation, while also having the ability to port the program into a format independent of Python. This allows the program to be run in a non-Python environment, thus avoiding any potential performance issues related to the Python interpreter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.1.2. Hybrid Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.1.3. Hybridizing the `Sequential` Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# factory for networks\n",
    "def get_net():\n",
    "    net = nn.Sequential(\n",
    "        nn.Linear(512, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 2),\n",
    "    )\n",
    "\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0150, 0.0135]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(size=(1, 512))\n",
    "net = get_net()\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a regular workflow to construct a neural network and pass data through it.\n",
    "\n",
    "By converting the model using `torch.jit.script` function, we are able to compile and optimize the computation in the MLP. The model's computation result remains unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0150, 0.0135]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = torch.jit.script(net) # convert to torchscript\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this happens, the network is optimized. This means that the execution time of the model's computation is reduced. This is because the Python interpreter is no longer needed to execute the model's computation. In addition, the optimized model can be exported to a format that is independent of Python, thus allowing the model to be run in a non-Python environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13.1.3.1. Accerlation by Hybridization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare the execution time to evaluate `net(x)` before and after hybridization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Benchmark:\n",
    "    '''For measuring running time'''\n",
    "    def __init__(self, description='Done'):\n",
    "        self.description = description\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.timer = d2l.Timer()\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, *args):\n",
    "        print(f'{self.description}: {self.timer.stop():.4f} sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without torchscript: 10.5680 sec\n",
      "With torchscript: 8.4506 sec\n"
     ]
    }
   ],
   "source": [
    "# benchmarking\n",
    "net = get_net() # no hybridization\n",
    "with Benchmark('Without torchscript'):\n",
    "    for i in range(100000):\n",
    "        net(x)\n",
    "\n",
    "net = torch.jit.script(net) # hybridization\n",
    "with Benchmark('With torchscript'):\n",
    "    for i in range(100000):\n",
    "        net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After an `nn.Sequential` instance is scripted using the `torch.jit.script` function, computing performance is improved through the use of symbolic programming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.2. Asynchronous Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For PyTorch, by default, GPU operations are asynchronous. We we call a function that uses the GPU, the operations are enqueued to the particular device, but not necessarily executed until later. This allows us to execute more computations in parallel, including operations on the CPU or other GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import numpy\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.2.1. Asynchrony via Backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that we want to generate a random matrix and multiply it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = d2l.try_gpu()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy: 3.3680 sec\n",
      "PyTorch: 0.0158 sec\n"
     ]
    }
   ],
   "source": [
    "# warmup for GPU computation\n",
    "a = torch.randn(size=(1000, 1000), device=device)\n",
    "b = torch.mm(a, a)\n",
    "\n",
    "# benchmarking with NumPy\n",
    "with d2l.Benchmark('NumPy'):\n",
    "    for _ in range(100):\n",
    "        a = numpy.random.normal(size=(1000, 1000))\n",
    "        b = numpy.dot(a, a)\n",
    "\n",
    "# benchmarking with PyTorch\n",
    "with d2l.Benchmark('PyTorch'):\n",
    "    for _ in range(100):\n",
    "        a = torch.randn(size=(1000, 1000), device=device)\n",
    "        b = torch.mm(a, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy dot product is executed on the CPU processor while PyTorch matrix multiplication is executed on GPU. By default, GPU operations are asynchronous in PyTorch. Forcing PyTorch to finish all computation prior to returning shows what happened previously: computation is being executed by the backend while the frontend returns control to Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Synchronization: 0.2012 sec\n"
     ]
    }
   ],
   "source": [
    "with d2l.Benchmark('GPU Synchronization'):\n",
    "    for _ in range(100):\n",
    "        a = torch.randn(size=(1000, 1000), device=device)\n",
    "        b = torch.mm(a, a)\n",
    "    torch.cuda.synchronize(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch has a frontend for direct interaction with the users, e.g., via Python, as well as a backend used by the system to perform the computation.\n",
    "\n",
    "Users can write PyTorch programs in various forntend languages, such as Python and C++. Regardless of the frontend programming language used, the execution of PyTorch programs occurs primarily in the backend of C++ implementations. The backend manages its own threads that continuously collect and execute queued tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 3.]], device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dependency graph\n",
    "x = torch.ones((1, 2), device=device)\n",
    "y = torch.ones((1, 2), device=device)\n",
    "z = x * y + 2\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../imgs/ch13/computegraph.svg)\n",
    "\n",
    "The figure above shows the computational graph of the code snippet. The figure below shows how frontend and backend interact for this code snippet.\n",
    "\n",
    "![](../imgs/ch13/threading.svg)\n",
    "\n",
    "When the last statement’s results need to be printed, the Python frontend thread will wait for the C++ backend thread to finish computing the result of the variable `z`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.3. Automatic Parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, a single operator will use all the computational resources on all CPUs or on a single GPU. \n",
    "\n",
    "While parallelization is typically most relevant between multiple GPUs, adding the local CPU will increase performance slightly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.3.1. Parallel Computation on GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test case:\n",
    "* the `run` function below performs 10 matrix-matrix multiplications on the device using data allocated into two variables: `x_cpu` and `x_gpu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cpu'), device(type='cuda', index=0))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device_gpu = d2l.try_gpu()\n",
    "device_cpu = torch.device('cpu')\n",
    "device_cpu, device_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(x):\n",
    "    return [x.mm(x) for _ in range(50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cpu\n",
    "x_cpu = torch.rand(size=(4000, 4000), device=device_cpu)\n",
    "# gpu\n",
    "x_gpu = torch.rand(size=(4000, 4000), device=device_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply the function to the data. To ensure that caching does not play a role in the results we warm up the devices by performing a single pass on either of them prior to measuring. `torch.cuda.synchronize()` waits for all kernels in all streams on a CUDA device to complete. It takes in a `device` argument, the device for which we need to synchronize. It uses the current device, given by `current_device()`, if the device argument is None (default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Time: 20.5477 sec\n",
      "GPU Time: 3.1646 sec\n"
     ]
    }
   ],
   "source": [
    "# warmup all devices\n",
    "run(x_cpu)\n",
    "run(x_gpu)\n",
    "torch.cuda.synchronize(device_gpu)\n",
    "\n",
    "with d2l.Benchmark('CPU Time'):\n",
    "    run(x_cpu)\n",
    "\n",
    "with d2l.Benchmark('GPU Time'):\n",
    "    run(x_gpu)\n",
    "    torch.cuda.synchronize(device_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we remove the `synchronize` statement between both tasks, the system is free to paralleize computation on both devices automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU & GPU Time: 21.6539 sec\n"
     ]
    }
   ],
   "source": [
    "with d2l.Benchmark('CPU & GPU Time'):\n",
    "    run(x_gpu)\n",
    "    run(x_cpu)\n",
    "    torch.cuda.synchronize(device_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total execution time is less than the sum of its parts, since the deep learning framework automatically schedules computation without the need for sophisticated code on our end. Note that we need to put GPU computation before CPU computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.3.2. Parallel Computation and Communication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many cases we need to move data between different devices, say between the CPU and GPU, or between different GPUs. For instance, this occurs when we want to perform distributed optimization where we need to aggregate the gradients over multiple accelerator cards. Let’s simulate this by computing on the GPU and then copying the results back to the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run on GPU: 2.3605 sec\n",
      "Copy to CPU: 1.1837 sec\n"
     ]
    }
   ],
   "source": [
    "def copy_to_cpu(x, non_blocking=False):\n",
    "    return [y.to('cpu', non_blocking=non_blocking) for y in x]\n",
    "\n",
    "with d2l.Benchmark('Run on GPU'):\n",
    "    y = run(x_gpu)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "with d2l.Benchmark('Copy to CPU'):\n",
    "    y_cpu = copy_to_cpu(y)\n",
    "    torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could already start copying parts of `y` to the CPU while the remainder of the list is still being computed. This situation occurs, e.g., when we compute the (backprop) gradient on a minibatch. The gradients of some of the parameters will be available earlier than that of others. Hence it works to our advantage to start using PCI-Express bus bandwidth while the GPU is still running.\n",
    "\n",
    "In PyTorch, several functions such as `to()` and `copy_()` admit an explicit `non_blocking` argument, which lets the caller bypass synchronization when it is unnecessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run on GPU and copy to CPU: 2.9920 sec\n"
     ]
    }
   ],
   "source": [
    "with d2l.Benchmark('Run on GPU and copy to CPU'):\n",
    "    y = run(x_gpu)\n",
    "    y_cpu = copy_to_cpu(y, non_blocking=True)\n",
    "    torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total time required for both operations is (as expected) less than the sum of their parts. Note that this task is different from parallel computation as it uses a different resource: the bus between the CPU and GPUs. \n",
    "\n",
    "In fact, we could compute on both devices and communicate, all at the same time. As noted above, there is a dependency between computation and communication: `y[i]` must be computed before it can be copied to the CPU. Fortunately, the system can copy `y[i-1]` while computing `y[i]` to reduce the total running time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.4. Training on Multiple GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.5.1. Splitting the Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given multiple GPUs, we want to partition training in a manner as to achieve good speedup while simultaneously benefitting from simple and reproducible design choices.\n",
    "\n",
    "1. We could partition the network across multiple GPUs. That is, each GPU takes as input the data flowing into a particular layer, processes data across a number of subsequent layers and then sends the data to the next GPU. This allows us to process data with larger networks when compared with what a single GPU could handle. Besides, memory footprint per GPU can be well controlled (it is a fraction of the total network footprint). However, the interface between layers (and thus GPUs) requires tight synchronization, especially if the computational workloads are not properly matched between layers. The interface between layers also requries large amounts of data transfer, such as activations and gradients. This may overwhelm the bandwidth of the GPU buses. *Not recommended unless there is excellent framework or operating system support for chaining together multiple GPUs*.\n",
    "\n",
    "2. We could split the work layerwise. For instance, rather than computing 64 channels on a single GPU we could split up the problem across 4 GPUs, each of which generates data for 16 channels. Likewise, for a fully connected layer we could split the number of output units. Using AlexNet design as an example shown in the figure below, we could split the network across 2 GPUs, each of which is responsible for computing the output of two fully connected layers. However, we need a *very large* number of synchronization or barrier operations since each layer depends on the results from all the other layers. Moreover, the amount of data that needs to be transferred is potentially even larger than when distributing layers across GPUs. *Not recommended due to its bandwidth cost and complexity*.\n",
    "\n",
    "![](../imgs/ch13/alexnet-original.svg)\n",
    "\n",
    "3. We could partition data across multiple GPUs. This way all GPUs perform the same type of work, albeit on different observations. Gradients are aggregated across GPUs after each minibatch of training data. This is the simplest approach and it can be applied in any situation. We only need to synchronize after each minibatch. That said, it is highly desirable to start exchanging gradients parameters already while others are still being computed. Moreover, larger numbers of GPUs lead to larger minibatch sizes, thus increasing training efficiency. However, adding more GPUs does not allow us to train larger models.\n",
    "\n",
    "![](../imgs/ch13/splitting.svg)\n",
    "\n",
    "A comparison of different ways of parallelization on multiple GPUs is depicted in the figure above. By and large, data parallelism is the most convenient way to proceed, provided that we have access to GPUs with sufficiently large memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.5.2. Data Parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that there are $k$ GPUs on a machine. Given the model to be trained, each GPU will maintain a complete set of model parameters independently although parameter values across the GPUs are identical and synchronized. The figure below shows the training process with data parallelism when $k=2$.\n",
    "\n",
    "![](../imgs/ch13/data-parallel.svg)\n",
    "\n",
    "In general,\n",
    "* in any iteration of training, given a random minibatch, we split the examples in the batch into $k$ portions and distribute them evenly across the GPUs.\n",
    "* each GPU calculates loss and gradient of the model parameters based on the minibatch subset it was assigned.\n",
    "* the local gradients of each of the $k$ GPUs are aggregated to obtain the current minibatch stochastic gradient.\n",
    "* the aggregate gradient is re-distributed to each GPU.\n",
    "* each GPU uses this minibatch stochastic gradient to update the complete set of model parameters that it maintains.\n",
    "\n",
    "In practice we *increase* the minibatch size $k$-fold when training on $k$ GPUs such that each GPU has the same amount of work to do as if we were training on a single GPU only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.5.3. A Toy Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use LeNet as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model parameters\n",
    "scale = 0.01\n",
    "W1 = torch.randn(size=(20, 1, 3, 3)) * scale\n",
    "b1 = torch.zeros(20)\n",
    "\n",
    "W2 = torch.randn(size=(50, 20, 5, 5)) * scale\n",
    "b2 = torch.zeros(50)\n",
    "\n",
    "W3 = torch.randn(size=(800, 128)) * scale\n",
    "b3 = torch.zeros(128)\n",
    "\n",
    "W4 = torch.randn(size=(128, 10)) * scale\n",
    "b4 = torch.zeros(10)\n",
    "\n",
    "params = [W1, b1, W2, b2, W3, b3, W4, b4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "def lenet(X, params):\n",
    "    h1_conv = F.conv2d(input=X, weight=params[0], bias=params[1])\n",
    "    h1_activation = F.relu(h1_conv)\n",
    "    h1 = F.avg_pool2d(input=h1_activation, kernel_size=(2,2), stride=(2,2))\n",
    "\n",
    "    h2_conv = F.conv2d(input=h1, weight=params[2], bias=params[3])\n",
    "    h2_activation = F.relu(h2_conv)\n",
    "    h2 = F.avg_pool2d(input=h2_activation, kernel_size=(2,2), stride=(2,2))\n",
    "    h2 = h2.reshape(h2.shape[0], -1)\n",
    "\n",
    "    h3_linear = torch.mm(h2, params[4]) + params[5]\n",
    "    h3 = F.relu(h3_linear)\n",
    "\n",
    "    y_hat = torch.mm(h3, params[6]) + params[7]\n",
    "    return y_hat\n",
    "\n",
    "# cross-entropy loss function\n",
    "loss = nn.CrossEntropyLoss(reduction='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.5.4. Data Synchronization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For efficient multi-GPU training,\n",
    "1. we need to have the ability to distribute a list of parameters to multiple devices and to attach gradients (`get_params`);\n",
    "2. we need the ability to sum parameters across multiple devices, i.e., we need an `allreduce` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(params, device):\n",
    "    new_params = [p.to(device) for p in params]\n",
    "    for p in new_params:\n",
    "        p.requires_grad_()\n",
    "    return new_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b1 weight: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "b1 grad: None\n"
     ]
    }
   ],
   "source": [
    "# copy the model parameters to a GPU\n",
    "new_params = get_params(params, d2l.try_gpu())\n",
    "\n",
    "print('b1 weight:', new_params[1])\n",
    "print('b1 grad:', new_params[1].grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we did not perform any computation yet, the gradient with regard to the bias parameter is still zero.\n",
    "\n",
    "The `allreduce` function adds up all vectors and broadcasts the result back to all devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allreduce(data):\n",
    "    for i in range(1, len(data)):\n",
    "        data[0][:] += data[i].to(data[0].device)\n",
    "\n",
    "    for i in range(1, len(data)):\n",
    "        data[i][:] = data[0].to(data[i].device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before allreduce:\n",
      " tensor([[1., 1.]], device='cuda:0') \n",
      " tensor([[2., 2.]])\n",
      "after allreduce:\n",
      " tensor([[3., 3.]], device='cuda:0') \n",
      " tensor([[3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    torch.ones((1, 2), device=d2l.try_gpu(i)) * (i + 1) for i in range(2)\n",
    "]\n",
    "print('before allreduce:\\n', data[0], '\\n', data[1])\n",
    "allreduce(data)\n",
    "print('after allreduce:\\n', data[0], '\\n', data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.5.5. Distributing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a simple utility function to distribute a minibatch evenly across multiple GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.arange(20).reshape(4, 5)\n",
    "devices = [torch.device('cuda:0'), torch.device('cpu')]\n",
    "\n",
    "split = nn.parallel.scatter(data, devices)\n",
    "print('input :', data)\n",
    "print('load into', devices)\n",
    "print('output:', split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split both data and labels\n",
    "def split_batch(X, y, devices):\n",
    "    \"\"\"Split `X` and `y` into multiple devices.\"\"\"\n",
    "    assert X.shape[0] == y.shape[0]\n",
    "    return (nn.parallel.scatter(X, devices),\n",
    "            nn.parallel.scatter(y, devices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.5.6. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement the data parallelism approach using auxiliary functions, `allreduce` and `split_and_load` to synchronize the data among multiple GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(X, y, device_params, devices, lr):\n",
    "    # split X and y across devices\n",
    "    X_shards, y_shards = split_batch(X, y, devices)\n",
    "\n",
    "    ls = [loss(lenet(X_shards, device_W), y_shards).sum()\n",
    "          for X_shard, y_shard, device_w in zip(\n",
    "                X_shards, y_shards, device_params\n",
    "          )]\n",
    "    \n",
    "    # backward pass on each device\n",
    "    for l in ls:\n",
    "        l.backward()\n",
    "    # sum all gradients from each GPU and broadcast them to all GPUs\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(device_params[0])):\n",
    "            allreduce([device_params[c][i].grad for c in range(len(devices))])\n",
    "\n",
    "    # model parameters are updated separately on each device\n",
    "            for param in device_params:\n",
    "                # use a full-size batch\n",
    "                d2l.sgd(param, lr, X.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to allocate the GPUs and copy all the model parameters to all the devices. Each batch is processed using the `train_batch` function to deal with multiple GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_gpus, batch_size, lr):\n",
    "    train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n",
    "    devices = [d2l.try_gpu(i) for i in range(num_gpus)]\n",
    "\n",
    "    # copy model paraemeters to `num_gpus` GPUs\n",
    "    device_params = [get_params(params, d) for d in devices]\n",
    "    num_epochs = 10\n",
    "    animator = d2l.Animator('epoch', 'test acc', xlim=[1, num_epochs])\n",
    "\n",
    "    timer = d2l.Timer()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        timer.start()\n",
    "\n",
    "        for X, y in train_iter:\n",
    "            # Perform multi-GPU training for a single batch\n",
    "            train_batch(X, y, device_params, devices, lr)\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "        timer.stop()\n",
    "\n",
    "        # evaluate the model on GPU 0\n",
    "        animator.add(epoch + 1,\n",
    "                     (d2l.evaluate_accuracy_gpu(lambda x: lenet(x, device_params[0]), \n",
    "                                                test_iter, \n",
    "                                                devices[0], ), )\n",
    "                    )\n",
    "        \n",
    "    print(f'test acc: {animator.Y[0][-1]:.2f}, {timer.avg():.1f} sec/epoch '\n",
    "        f'on {str(devices)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(num_gpus=1, batch_size=256, lr=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train(num_gpus=2, batch_size=256, lr=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.6. Concise Implementation for Multiple GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.6.1. A Toy Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use ResNet-18 as an example with a slight modification to the original network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet18(num_classes, in_channels=1):\n",
    "    '''A slightly modified ResNet-18 model'''\n",
    "\n",
    "    def resnet_block(in_channels, out_channels, num_residuals, first_block=False):\n",
    "        blk = []\n",
    "        for i in range(num_residuals):\n",
    "            if i == 0 and not first_block:\n",
    "                blk.append(\n",
    "                    d2l.Residual(\n",
    "                        out_channels, use_1x1conv=True, strides=2\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                blk.append(\n",
    "                    d2l.Residual(out_channels)\n",
    "                )\n",
    "\n",
    "        return nn.Sequential(*blk)\n",
    "    \n",
    "    # use a smaller convolution kernel, stride, and padding;\n",
    "    # remove the maximum pooling layer\n",
    "    net = nn.Sequential(\n",
    "        nn.Conv2d(in_channels, 64, kernel_size=3, stride=1, padding=1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.ReLU(),\n",
    "    )\n",
    "    # add all the residual blocks\n",
    "    net.add_module(\n",
    "        name='resnet_block1',\n",
    "        module=resnet_block(64, 64, 2, first_block=True)\n",
    "    )\n",
    "    net.add_module(\n",
    "        name='resnet_block2',\n",
    "        module=resnet_block(64, 128, 2)\n",
    "    )\n",
    "    net.add_module(\n",
    "        name='resnet_block3',\n",
    "        module=resnet_block(128, 256, 2)\n",
    "    )\n",
    "    net.add_module(\n",
    "        name='resnet_block4',\n",
    "        module=resnet_block(256, 512, 2)\n",
    "    )\n",
    "    # add a global average pooling layer\n",
    "    net.add_module(\n",
    "        name='global_avg_pool',\n",
    "        module=nn.AdaptiveAvgPool2d((1,1))\n",
    "    )\n",
    "    # add a fully-connected layer to output classification score\n",
    "    net.add_module(\n",
    "        name='fc',\n",
    "        module=nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.6.2. Network Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = resnet18(num_classes=10)\n",
    "# get the number of GPUs\n",
    "devices = d2l.try_all_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.6.3. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For efficient parallelism,\n",
    "* Network parameters need to be initialized across all devices.\n",
    "* While iterating over the dataset, minibatches are to be divided across all devices.\n",
    "* We compute the loss and its graident in parallel across devices.\n",
    "* Gradients are aggregated and parameters are updated accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, num_gpus, batch_size, lr):\n",
    "    # load data\n",
    "    train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)\n",
    "    # get all gpu devices\n",
    "    devices = [d2l.try_gpu(i) for i in range(num_gpus)]\n",
    "\n",
    "    # initialize weights\n",
    "    def init_weights(module):\n",
    "        if type(module) in [nn.Linear, nn.Conv2d]:\n",
    "            nn.init.normal_(module.weight, std=0.01)\n",
    "\n",
    "    # apply `init_weights` function\n",
    "    net.apply(init_weights)\n",
    "\n",
    "    # set model on multiple gpus\n",
    "    net = nn.DataParallel(net, device_ids=devices)\n",
    "    # optimizer\n",
    "    trainer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "    # loss function\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    # set a timer\n",
    "    timer = d2l.Timer()\n",
    "    num_epochs = 10\n",
    "    # animator\n",
    "    animator = d2l.Animator('epoch', 'test acc', xlim=[1, num_epochs])\n",
    "\n",
    "    # train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        net.train()\n",
    "        timer.start()\n",
    "\n",
    "        for X, y in train_iter:\n",
    "            # zero the parameter gradients\n",
    "            trainer.zero_grad()\n",
    "            # send data to a device\n",
    "            X, y = X.to(devices[0]), y.to(devices[0])\n",
    "            # compute loss\n",
    "            l = loss(net(X), y)\n",
    "            # backword pass\n",
    "            l.backward()\n",
    "            # update parameters\n",
    "            trainer.step()\n",
    "\n",
    "        timer.stop()\n",
    "        animator.add(\n",
    "            epoch + 1,\n",
    "            (d2l.evaluate_accuracy_gpu(net, test_iter),)\n",
    "        )\n",
    "\n",
    "    print(f'test acc: {animator.Y[0][-1]:.2f}, {timer.avg():.1f} sec/epoch '\n",
    "          f'on {str(devices)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a warmup, we train the network on a single GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(net, num_gpus=1, batch_size=256, lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Net we use 2 GPUs for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(net, num_gpus=2, batch_size=512, lr=0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
